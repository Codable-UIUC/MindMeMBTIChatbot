{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kidv4v3saNv2",
        "outputId": "384fdf4e-2817-4718-9834-c23fd0253bae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 15.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=73d927fb0dc8b648c1e0bbeeddfd567f04da79038b25874b15959b6e52dfade9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LfLNWcjWq_x"
      },
      "outputs": [],
      "source": [
        "import json \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDPzC_-zWYQd"
      },
      "outputs": [],
      "source": [
        "intents = {\"intents\": [\n",
        "        {\"tag\": \"greeting\",\n",
        "         \"patterns\": [\"Hi\", \"How are you\", \"Is anyone there?\", \"Hello\", \"Good day\", \"Hey\", \"What's up?\", \"Wassup\"],\n",
        "         \"responses\": [\"Hello, is everything all right?\", \"Good to see you again\", \"Hi there, how was your day?\"],\n",
        "        },\n",
        "        {\"tag\": \"goodbye\",\n",
        "         \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\"],\n",
        "         \"responses\": [\"See you later, thanks for visiting\", \"Have a nice day\", \"Bye! Come back again soon.\"]\n",
        "        },\n",
        "        {\"tag\": \"thanks\",\n",
        "         \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\"],\n",
        "         \"responses\": [\"Happy to help!\", \"Any time!\", \"My pleasure\", \"No problem\"]\n",
        "        },\n",
        "        {\"tag\": \"Agree\",\n",
        "         \"patterns\": [\"Yeah, I completely agree\", \"I know right?\"],\n",
        "         \"responses\": [\"Yeah, I completely agree\", \"I know right?\", \"I also like doing that\"]\n",
        "        },\n",
        "        {\"tag\": \"Name\",\n",
        "         \"patterns\": [\"What is your name?\", \"what are you called?\", \"Name?\" ],\n",
        "         \"responses\": [\"My name is MindME\", \"It is MindME\", \"Call me MindME\"]\n",
        "        },\n",
        "        {\"tag\": \"Maker\",\n",
        "         \"patterns\": [\"Who made you?\", \"Codable?\", \"Who created you?\"],\n",
        "         \"responses\": [\"Codable ML team created me\", \"Max Min, Yunsu Han, Hangoo Kang, Sangyun Lee from UIUC created me\"]\n",
        "        }\n",
        "   ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFXSytxHW8cl"
      },
      "outputs": [],
      "source": [
        "with open(\"intent.json\", \"w\") as outfile: \n",
        "  json.dump(intents, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_EgZ5O8WjHA"
      },
      "outputs": [],
      "source": [
        "MBTI = {\"MBTI questions\": [\n",
        "        {\"tag\": \"E vs I\",\n",
        "         \"patterns\": [\"Hi\", \"How are you\", \"Is anyone there?\", \"Hello\", \"Good day\", \"Hey\", \"\"],\n",
        "         \"responses\": [\"Hello, thanks for visiting\", \"Good to see you again\", \"Hi there, how can I help?\"],\n",
        "         \"context_set\": \"\"\n",
        "        },\n",
        "        {\"tag\": \"N vs S\",\n",
        "         \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\"],\n",
        "         \"responses\": [\"See you later, thanks for visiting\", \"Have a nice day\", \"Bye! Come back again soon.\"]\n",
        "        },\n",
        "        {\"tag\": \"F vs T\",\n",
        "         \"patterns\": [\"Thanks\", \"Thank you\", \"That's helpful\"],\n",
        "         \"responses\": [\"Happy to help!\", \"Any time!\", \"My pleasure\"]\n",
        "        },\n",
        "        {\"tag\": \"J vs P\",\n",
        "         \"patterns\": [\"What hours are you open?\", \"What are your hours?\", \"When are you open?\" ],\n",
        "         \"responses\": [\"We're open every day 9am-9pm\", \"Our hours are 9am-9pm every day\"]\n",
        "        }\n",
        "   ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj-wPZMbZxK8"
      },
      "outputs": [],
      "source": [
        "with open(\"MBTI.json\", \"w\") as outfile: \n",
        "  json.dump(MBTI, outfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9IQkyi8-ZYc"
      },
      "source": [
        "#Tokenisation and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z00FTY03Z7Tq",
        "outputId": "ac056686-7ca7-41cd-9325-16bcf75e2478"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22 intents\n",
            "6 labels ['Agree', 'Maker', 'Name', 'goodbye', 'greeting', 'thanks']\n",
            "36 unique stemmed words [\"'s\", ',', 'agr', 'anyon', 'ar', 'bye', 'cal', 'cod', 'complet', 'cre', 'day', 'good', 'goodby', 'hello', 'help', 'hey', 'hi', 'how', 'i', 'is', 'know', 'lat', 'mad', 'nam', 'right', 'see', 'thank', 'that', 'ther', 'up', 'wassup', 'what', 'who', 'yeah', 'yo', 'you']\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import json\n",
        "import random\n",
        "import tensorflow\n",
        "import tflearn\n",
        "import numpy\n",
        "import nltk\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "nltk.download('punkt')\n",
        "\n",
        "with open('intent.json') as file:\n",
        "    intent_data = json.load(file)\n",
        "\n",
        "#with open('MBTI.json') as file :\n",
        "#    mbti_Data = json.load(file)\n",
        "\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "try:\n",
        "    with open(\"data.pickle\", \"rb\") as f:\n",
        "        words, labels, training, output = pickle.load(f)\n",
        "\n",
        "except:\n",
        "    words = []\n",
        "    labels = []\n",
        "    #classes = []\n",
        "    docs_x = []\n",
        "    docs_y = []\n",
        "    #ignore_words = ['?']\n",
        "\n",
        "    for intent in intent_data['intents']:\n",
        "        for pattern in intent['patterns']:\n",
        "            wrds = nltk.word_tokenize(pattern)\n",
        "            words.extend(wrds)\n",
        "            docs_x.append(wrds)\n",
        "            docs_y.append(intent[\"tag\"])\n",
        "\n",
        "        if intent['tag'] not in labels:\n",
        "            labels.append(intent['tag'])\n",
        "\n",
        "    # stem and lower each word and remove duplicates\n",
        "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "    words = sorted(list(set(words)))\n",
        "\n",
        "    labels = sorted(labels)\n",
        "\n",
        "    print (len(docs_y), \"intents\")\n",
        "    print (len(labels), \"labels\", labels)\n",
        "    print (len(words), \"unique stemmed words\", words)\n",
        "\n",
        "\n",
        "    training = []\n",
        "    output = []\n",
        "\n",
        "    out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "    # training set, bag of words for each sentence\n",
        "    for x, doc in enumerate(docs_x):\n",
        "        bag = []\n",
        "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "\n",
        "        # create bag of words array.\n",
        "        for w in words:\n",
        "            if w in wrds:\n",
        "                bag.append(1)\n",
        "            else:\n",
        "                bag.append(0)\n",
        "\n",
        "        output_row = out_empty[:]\n",
        "        output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "        training.append(bag)\n",
        "        output.append(output_row)\n",
        "\n",
        "    training = numpy.array(training)\n",
        "    output = numpy.array(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuZr7ElSAl6n"
      },
      "outputs": [],
      "source": [
        "with open(\"data.pickle\", \"wb\") as f:\n",
        "  pickle.dump((words, labels, training, output), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8P2cTRC-WTy"
      },
      "source": [
        "# Build and Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOzQmriOA5Rm",
        "outputId": "d20b76fb-884a-4f74-89be-169685d5467f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tflearn/initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "| Adam | epoch: 286 | loss: 0.86468 - acc: 0.7571 -- iter: 22/22\n",
            "--\n",
            "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.84214\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 287 | loss: 0.84214 - acc: 0.7564 -- iter: 08/22\n",
            "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.86601\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 287 | loss: 0.86601 - acc: 0.7308 -- iter: 16/22\n",
            "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.88296\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 287 | loss: 0.88296 - acc: 0.7244 -- iter: 22/22\n",
            "--\n",
            "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.86448\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 288 | loss: 0.86448 - acc: 0.7144 -- iter: 08/22\n",
            "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.82759\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 288 | loss: 0.82759 - acc: 0.6854 -- iter: 16/22\n",
            "Training Step: 864  | total loss: \u001b[1m\u001b[32m1.05409\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 288 | loss: 1.05409 - acc: 0.6854 -- iter: 22/22\n",
            "--\n",
            "Training Step: 865  | total loss: \u001b[1m\u001b[32m1.05018\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 289 | loss: 1.05018 - acc: 0.6835 -- iter: 08/22\n",
            "Training Step: 866  | total loss: \u001b[1m\u001b[32m1.01362\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 289 | loss: 1.01362 - acc: 0.6901 -- iter: 16/22\n",
            "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.96449\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 289 | loss: 0.96449 - acc: 0.7086 -- iter: 22/22\n",
            "--\n",
            "Training Step: 868  | total loss: \u001b[1m\u001b[32m1.02393\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 290 | loss: 1.02393 - acc: 0.6711 -- iter: 08/22\n",
            "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.96669\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 290 | loss: 0.96669 - acc: 0.6707 -- iter: 16/22\n",
            "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.94505\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 290 | loss: 0.94505 - acc: 0.6911 -- iter: 22/22\n",
            "--\n",
            "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.93838\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 291 | loss: 0.93838 - acc: 0.6595 -- iter: 08/22\n",
            "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.88982\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 291 | loss: 0.88982 - acc: 0.6769 -- iter: 16/22\n",
            "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.83162\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 291 | loss: 0.83162 - acc: 0.7092 -- iter: 22/22\n",
            "--\n",
            "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.82806\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 292 | loss: 0.82806 - acc: 0.6883 -- iter: 08/22\n",
            "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.83944\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 292 | loss: 0.83944 - acc: 0.6819 -- iter: 16/22\n",
            "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.78601\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 292 | loss: 0.78601 - acc: 0.7137 -- iter: 22/22\n",
            "--\n",
            "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.80074\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 293 | loss: 0.80074 - acc: 0.7090 -- iter: 08/22\n",
            "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.81379\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 293 | loss: 0.81379 - acc: 0.6881 -- iter: 16/22\n",
            "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.76591\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 293 | loss: 0.76591 - acc: 0.7068 -- iter: 22/22\n",
            "--\n",
            "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.78228\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 294 | loss: 0.78228 - acc: 0.7028 -- iter: 08/22\n",
            "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.75530\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 294 | loss: 0.75530 - acc: 0.6992 -- iter: 16/22\n",
            "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.75758\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 294 | loss: 0.75758 - acc: 0.6918 -- iter: 22/22\n",
            "--\n",
            "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.73650\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 295 | loss: 0.73650 - acc: 0.6976 -- iter: 08/22\n",
            "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.73650\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 295 | loss: 0.73650 - acc: 0.6945 -- iter: 16/22\n",
            "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.75429\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 295 | loss: 0.75429 - acc: 0.6584 -- iter: 22/22\n",
            "--\n",
            "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.71579\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 296 | loss: 0.71579 - acc: 0.6675 -- iter: 08/22\n",
            "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.73460\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 296 | loss: 0.73460 - acc: 0.6695 -- iter: 16/22\n",
            "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.75209\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 296 | loss: 0.75209 - acc: 0.6695 -- iter: 22/22\n",
            "--\n",
            "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.72615\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 297 | loss: 0.72615 - acc: 0.6858 -- iter: 08/22\n",
            "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.72101\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 297 | loss: 0.72101 - acc: 0.6923 -- iter: 16/22\n",
            "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.73987\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 297 | loss: 0.73987 - acc: 0.6980 -- iter: 22/22\n",
            "--\n",
            "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.71478\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 298 | loss: 0.71478 - acc: 0.7116 -- iter: 08/22\n",
            "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.70627\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 298 | loss: 0.70627 - acc: 0.7237 -- iter: 16/22\n",
            "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.71663\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 298 | loss: 0.71663 - acc: 0.7139 -- iter: 22/22\n",
            "--\n",
            "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.71160\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 299 | loss: 0.71160 - acc: 0.7300 -- iter: 08/22\n",
            "Training Step: 896  | total loss: \u001b[1m\u001b[32m1.30105\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 299 | loss: 1.30105 - acc: 0.6737 -- iter: 16/22\n",
            "Training Step: 897  | total loss: \u001b[1m\u001b[32m1.25440\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 299 | loss: 1.25440 - acc: 0.6730 -- iter: 22/22\n",
            "--\n",
            "Training Step: 898  | total loss: \u001b[1m\u001b[32m1.20619\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 300 | loss: 1.20619 - acc: 0.6932 -- iter: 08/22\n",
            "Training Step: 899  | total loss: \u001b[1m\u001b[32m1.14041\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 300 | loss: 1.14041 - acc: 0.7113 -- iter: 16/22\n",
            "Training Step: 900  | total loss: \u001b[1m\u001b[32m1.10963\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 300 | loss: 1.10963 - acc: 0.7069 -- iter: 22/22\n",
            "--\n",
            "Training Step: 901  | total loss: \u001b[1m\u001b[32m1.09921\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 301 | loss: 1.09921 - acc: 0.7195 -- iter: 08/22\n",
            "Training Step: 902  | total loss: \u001b[1m\u001b[32m1.03147\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 301 | loss: 1.03147 - acc: 0.7351 -- iter: 16/22\n",
            "Training Step: 903  | total loss: \u001b[1m\u001b[32m1.00534\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 301 | loss: 1.00534 - acc: 0.7366 -- iter: 22/22\n",
            "--\n",
            "Training Step: 904  | total loss: \u001b[1m\u001b[32m1.00505\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 302 | loss: 1.00505 - acc: 0.7462 -- iter: 08/22\n",
            "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.93686\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 302 | loss: 0.93686 - acc: 0.7716 -- iter: 16/22\n",
            "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.90462\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 302 | loss: 0.90462 - acc: 0.7945 -- iter: 22/22\n",
            "--\n",
            "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.92275\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 303 | loss: 0.92275 - acc: 0.7650 -- iter: 08/22\n",
            "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.86282\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 303 | loss: 0.86282 - acc: 0.7885 -- iter: 16/22\n",
            "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.82393\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 303 | loss: 0.82393 - acc: 0.7763 -- iter: 22/22\n",
            "--\n",
            "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.82045\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 304 | loss: 0.82045 - acc: 0.7987 -- iter: 08/22\n",
            "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.81793\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 304 | loss: 0.81793 - acc: 0.7938 -- iter: 16/22\n",
            "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.78346\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 304 | loss: 0.78346 - acc: 0.7811 -- iter: 22/22\n",
            "--\n",
            "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.78797\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 305 | loss: 0.78797 - acc: 0.7697 -- iter: 08/22\n",
            "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.76228\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 305 | loss: 0.76228 - acc: 0.7677 -- iter: 16/22\n",
            "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.76439\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 305 | loss: 0.76439 - acc: 0.7909 -- iter: 22/22\n",
            "--\n",
            "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.77064\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 306 | loss: 0.77064 - acc: 0.7785 -- iter: 08/22\n",
            "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.75494\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 306 | loss: 0.75494 - acc: 0.7840 -- iter: 16/22\n",
            "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.75664\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 306 | loss: 0.75664 - acc: 0.7931 -- iter: 22/22\n",
            "--\n",
            "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.75088\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 307 | loss: 0.75088 - acc: 0.7888 -- iter: 08/22\n",
            "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.73688\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 307 | loss: 0.73688 - acc: 0.7932 -- iter: 16/22\n",
            "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.75008\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 307 | loss: 0.75008 - acc: 0.7972 -- iter: 22/22\n",
            "--\n",
            "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.76866\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 308 | loss: 0.76866 - acc: 0.8020 -- iter: 08/22\n",
            "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.72556\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 308 | loss: 0.72556 - acc: 0.8020 -- iter: 16/22\n",
            "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.73964\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 308 | loss: 0.73964 - acc: 0.8051 -- iter: 22/22\n",
            "--\n",
            "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.74711\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 309 | loss: 0.74711 - acc: 0.7913 -- iter: 08/22\n",
            "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.72594\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 309 | loss: 0.72594 - acc: 0.7997 -- iter: 16/22\n",
            "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.73061\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 309 | loss: 0.73061 - acc: 0.8072 -- iter: 22/22\n",
            "--\n",
            "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.73887\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 310 | loss: 0.73887 - acc: 0.7931 -- iter: 08/22\n",
            "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.76708\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 310 | loss: 0.76708 - acc: 0.7638 -- iter: 16/22\n",
            "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.72348\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 310 | loss: 0.72348 - acc: 0.7875 -- iter: 22/22\n",
            "--\n",
            "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.73294\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 311 | loss: 0.73294 - acc: 0.7837 -- iter: 08/22\n",
            "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.76153\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 311 | loss: 0.76153 - acc: 0.7553 -- iter: 16/22\n",
            "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.71078\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 311 | loss: 0.71078 - acc: 0.7798 -- iter: 22/22\n",
            "--\n",
            "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.71909\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 312 | loss: 0.71909 - acc: 0.7768 -- iter: 08/22\n",
            "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.73972\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 312 | loss: 0.73972 - acc: 0.7491 -- iter: 16/22\n",
            "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.69090\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 312 | loss: 0.69090 - acc: 0.7742 -- iter: 22/22\n",
            "--\n",
            "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.66994\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 313 | loss: 0.66994 - acc: 0.7801 -- iter: 08/22\n",
            "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.66525\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 313 | loss: 0.66525 - acc: 0.7771 -- iter: 16/22\n",
            "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.69072\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 313 | loss: 0.69072 - acc: 0.7619 -- iter: 22/22\n",
            "--\n",
            "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.66960\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 314 | loss: 0.66960 - acc: 0.7691 -- iter: 08/22\n",
            "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.66336\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 314 | loss: 0.66336 - acc: 0.7588 -- iter: 16/22\n",
            "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.67832\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 314 | loss: 0.67832 - acc: 0.7454 -- iter: 22/22\n",
            "--\n",
            "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.93152\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 315 | loss: 0.93152 - acc: 0.7584 -- iter: 08/22\n",
            "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.93152\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 315 | loss: 0.93152 - acc: 0.6992 -- iter: 16/22\n",
            "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.92119\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 315 | loss: 0.92119 - acc: 0.7126 -- iter: 22/22\n",
            "--\n",
            "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.85542\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 316 | loss: 0.85542 - acc: 0.7414 -- iter: 08/22\n",
            "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.87112\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 316 | loss: 0.87112 - acc: 0.7047 -- iter: 16/22\n",
            "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.86640\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 316 | loss: 0.86640 - acc: 0.7176 -- iter: 22/22\n",
            "--\n",
            "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.85676\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 317 | loss: 0.85676 - acc: 0.7292 -- iter: 08/22\n",
            "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.84798\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 317 | loss: 0.84798 - acc: 0.7187 -- iter: 16/22\n",
            "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.81850\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 317 | loss: 0.81850 - acc: 0.7219 -- iter: 22/22\n",
            "--\n",
            "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.81318\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 318 | loss: 0.81318 - acc: 0.7330 -- iter: 08/22\n",
            "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.81271\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 318 | loss: 0.81271 - acc: 0.7264 -- iter: 16/22\n",
            "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.79853\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 318 | loss: 0.79853 - acc: 0.7287 -- iter: 22/22\n",
            "--\n",
            "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.78065\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 319 | loss: 0.78065 - acc: 0.7309 -- iter: 08/22\n",
            "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.93114\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 319 | loss: 0.93114 - acc: 0.6911 -- iter: 16/22\n",
            "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.88463\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 319 | loss: 0.88463 - acc: 0.6887 -- iter: 22/22\n",
            "--\n",
            "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.90377\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 320 | loss: 0.90377 - acc: 0.6823 -- iter: 08/22\n",
            "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.86057\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 320 | loss: 0.86057 - acc: 0.7016 -- iter: 16/22\n",
            "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.82123\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 320 | loss: 0.82123 - acc: 0.6981 -- iter: 22/22\n",
            "--\n",
            "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.78486\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 321 | loss: 0.78486 - acc: 0.7116 -- iter: 08/22\n",
            "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.77344\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 321 | loss: 0.77344 - acc: 0.7154 -- iter: 16/22\n",
            "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.78442\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 321 | loss: 0.78442 - acc: 0.7064 -- iter: 22/22\n",
            "--\n",
            "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.75160\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 322 | loss: 0.75160 - acc: 0.7191 -- iter: 08/22\n",
            "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.73834\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 322 | loss: 0.73834 - acc: 0.7139 -- iter: 16/22\n",
            "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.73236\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 322 | loss: 0.73236 - acc: 0.7300 -- iter: 22/22\n",
            "--\n",
            "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.73422\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 323 | loss: 0.73422 - acc: 0.7195 -- iter: 08/22\n",
            "Training Step: 968  | total loss: \u001b[1m\u001b[32m1.70028\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 323 | loss: 1.70028 - acc: 0.6475 -- iter: 16/22\n",
            "Training Step: 969  | total loss: \u001b[1m\u001b[32m1.58071\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 323 | loss: 1.58071 - acc: 0.6828 -- iter: 22/22\n",
            "--\n",
            "Training Step: 970  | total loss: \u001b[1m\u001b[32m1.49933\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 324 | loss: 1.49933 - acc: 0.6770 -- iter: 08/22\n",
            "Training Step: 971  | total loss: \u001b[1m\u001b[32m1.42420\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 324 | loss: 1.42420 - acc: 0.6718 -- iter: 16/22\n",
            "Training Step: 972  | total loss: \u001b[1m\u001b[32m1.33211\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 324 | loss: 1.33211 - acc: 0.7046 -- iter: 22/22\n",
            "--\n",
            "Training Step: 973  | total loss: \u001b[1m\u001b[32m1.26095\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 325 | loss: 1.26095 - acc: 0.7175 -- iter: 08/22\n",
            "Training Step: 974  | total loss: \u001b[1m\u001b[32m1.19798\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 325 | loss: 1.19798 - acc: 0.6957 -- iter: 16/22\n",
            "Training Step: 975  | total loss: \u001b[1m\u001b[32m1.15791\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 325 | loss: 1.15791 - acc: 0.7137 -- iter: 22/22\n",
            "--\n",
            "Training Step: 976  | total loss: \u001b[1m\u001b[32m1.10419\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 326 | loss: 1.10419 - acc: 0.7256 -- iter: 08/22\n",
            "Training Step: 977  | total loss: \u001b[1m\u001b[32m1.06165\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 326 | loss: 1.06165 - acc: 0.7197 -- iter: 16/22\n",
            "Training Step: 978  | total loss: \u001b[1m\u001b[32m1.01622\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 326 | loss: 1.01622 - acc: 0.7103 -- iter: 22/22\n",
            "--\n",
            "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.99238\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 327 | loss: 0.99238 - acc: 0.7267 -- iter: 08/22\n",
            "Training Step: 980  | total loss: \u001b[1m\u001b[32m1.43823\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 327 | loss: 1.43823 - acc: 0.6874 -- iter: 16/22\n",
            "Training Step: 981  | total loss: \u001b[1m\u001b[32m1.34545\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 327 | loss: 1.34545 - acc: 0.7020 -- iter: 22/22\n",
            "--\n",
            "Training Step: 982  | total loss: \u001b[1m\u001b[32m1.29957\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 328 | loss: 1.29957 - acc: 0.6943 -- iter: 08/22\n",
            "Training Step: 983  | total loss: \u001b[1m\u001b[32m1.23225\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 328 | loss: 1.23225 - acc: 0.6999 -- iter: 16/22\n",
            "Training Step: 984  | total loss: \u001b[1m\u001b[32m1.16006\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 328 | loss: 1.16006 - acc: 0.7132 -- iter: 22/22\n",
            "--\n",
            "Training Step: 985  | total loss: \u001b[1m\u001b[32m1.08401\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 329 | loss: 1.08401 - acc: 0.7419 -- iter: 08/22\n",
            "Training Step: 986  | total loss: \u001b[1m\u001b[32m1.05374\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 329 | loss: 1.05374 - acc: 0.7302 -- iter: 16/22\n",
            "Training Step: 987  | total loss: \u001b[1m\u001b[32m1.02995\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 329 | loss: 1.02995 - acc: 0.7197 -- iter: 22/22\n",
            "--\n",
            "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.96681\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 330 | loss: 0.96681 - acc: 0.7477 -- iter: 08/22\n",
            "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.91396\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 330 | loss: 0.91396 - acc: 0.7563 -- iter: 16/22\n",
            "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.88661\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 330 | loss: 0.88661 - acc: 0.7431 -- iter: 22/22\n",
            "--\n",
            "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.89057\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 331 | loss: 0.89057 - acc: 0.7438 -- iter: 08/22\n",
            "Training Step: 992  | total loss: \u001b[1m\u001b[32m1.45391\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 331 | loss: 1.45391 - acc: 0.6861 -- iter: 16/22\n",
            "Training Step: 993  | total loss: \u001b[1m\u001b[32m1.39062\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 331 | loss: 1.39062 - acc: 0.6842 -- iter: 22/22\n",
            "--\n",
            "Training Step: 994  | total loss: \u001b[1m\u001b[32m1.32803\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 332 | loss: 1.32803 - acc: 0.6783 -- iter: 08/22\n",
            "Training Step: 995  | total loss: \u001b[1m\u001b[32m1.24697\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 332 | loss: 1.24697 - acc: 0.6979 -- iter: 16/22\n",
            "Training Step: 996  | total loss: \u001b[1m\u001b[32m1.20431\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 332 | loss: 1.20431 - acc: 0.6948 -- iter: 22/22\n",
            "--\n",
            "Training Step: 997  | total loss: \u001b[1m\u001b[32m1.14843\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 333 | loss: 1.14843 - acc: 0.6920 -- iter: 08/22\n",
            "Training Step: 998  | total loss: \u001b[1m\u001b[32m1.10909\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 333 | loss: 1.10909 - acc: 0.6978 -- iter: 16/22\n",
            "Training Step: 999  | total loss: \u001b[1m\u001b[32m1.06431\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 333 | loss: 1.06431 - acc: 0.7030 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1000  | total loss: \u001b[1m\u001b[32m1.54085\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 334 | loss: 1.54085 - acc: 0.6494 -- iter: 08/22\n",
            "Training Step: 1001  | total loss: \u001b[1m\u001b[32m1.45252\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 334 | loss: 1.45252 - acc: 0.6678 -- iter: 16/22\n",
            "Training Step: 1002  | total loss: \u001b[1m\u001b[32m1.37399\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 334 | loss: 1.37399 - acc: 0.6635 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1003  | total loss: \u001b[1m\u001b[32m1.31085\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 335 | loss: 1.31085 - acc: 0.6721 -- iter: 08/22\n",
            "Training Step: 1004  | total loss: \u001b[1m\u001b[32m1.24552\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 335 | loss: 1.24552 - acc: 0.6883 -- iter: 16/22\n",
            "Training Step: 1005  | total loss: \u001b[1m\u001b[32m1.18414\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 335 | loss: 1.18414 - acc: 0.7028 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1006  | total loss: \u001b[1m\u001b[32m1.15559\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 336 | loss: 1.15559 - acc: 0.6950 -- iter: 08/22\n",
            "Training Step: 1007  | total loss: \u001b[1m\u001b[32m1.09315\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 336 | loss: 1.09315 - acc: 0.7005 -- iter: 16/22\n",
            "Training Step: 1008  | total loss: \u001b[1m\u001b[32m1.04699\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 336 | loss: 1.04699 - acc: 0.7138 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1009  | total loss: \u001b[1m\u001b[32m0.98396\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 337 | loss: 0.98396 - acc: 0.7424 -- iter: 08/22\n",
            "Training Step: 1010  | total loss: \u001b[1m\u001b[32m0.96276\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 337 | loss: 0.96276 - acc: 0.7432 -- iter: 16/22\n",
            "Training Step: 1011  | total loss: \u001b[1m\u001b[32m0.94833\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 337 | loss: 0.94833 - acc: 0.7188 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1012  | total loss: \u001b[1m\u001b[32m0.89493\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 338 | loss: 0.89493 - acc: 0.7470 -- iter: 08/22\n",
            "Training Step: 1013  | total loss: \u001b[1m\u001b[32m0.88326\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 338 | loss: 0.88326 - acc: 0.7389 -- iter: 16/22\n",
            "Training Step: 1014  | total loss: \u001b[1m\u001b[32m0.87149\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 338 | loss: 0.87149 - acc: 0.7525 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1015  | total loss: \u001b[1m\u001b[32m0.83930\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 339 | loss: 0.83930 - acc: 0.7398 -- iter: 08/22\n",
            "Training Step: 1016  | total loss: \u001b[1m\u001b[32m1.14574\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 339 | loss: 1.14574 - acc: 0.6825 -- iter: 16/22\n",
            "Training Step: 1017  | total loss: \u001b[1m\u001b[32m1.11046\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 339 | loss: 1.11046 - acc: 0.6809 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1018  | total loss: \u001b[1m\u001b[32m1.07557\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 340 | loss: 1.07557 - acc: 0.6878 -- iter: 08/22\n",
            "Training Step: 1019  | total loss: \u001b[1m\u001b[32m1.02208\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 340 | loss: 1.02208 - acc: 0.6940 -- iter: 16/22\n",
            "Training Step: 1020  | total loss: \u001b[1m\u001b[32m0.99913\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 340 | loss: 0.99913 - acc: 0.6913 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1021  | total loss: \u001b[1m\u001b[32m0.96068\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 341 | loss: 0.96068 - acc: 0.7055 -- iter: 08/22\n",
            "Training Step: 1022  | total loss: \u001b[1m\u001b[32m0.91713\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 341 | loss: 0.91713 - acc: 0.7224 -- iter: 16/22\n",
            "Training Step: 1023  | total loss: \u001b[1m\u001b[32m0.91656\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 341 | loss: 0.91656 - acc: 0.7002 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1024  | total loss: \u001b[1m\u001b[32m0.88624\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 342 | loss: 0.88624 - acc: 0.7135 -- iter: 08/22\n",
            "Training Step: 1025  | total loss: \u001b[1m\u001b[32m0.84971\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 342 | loss: 0.84971 - acc: 0.7255 -- iter: 16/22\n",
            "Training Step: 1026  | total loss: \u001b[1m\u001b[32m0.84974\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 342 | loss: 0.84974 - acc: 0.7154 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1027  | total loss: \u001b[1m\u001b[32m0.83032\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 343 | loss: 0.83032 - acc: 0.7189 -- iter: 08/22\n",
            "Training Step: 1028  | total loss: \u001b[1m\u001b[32m0.79905\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 343 | loss: 0.79905 - acc: 0.7303 -- iter: 16/22\n",
            "Training Step: 1029  | total loss: \u001b[1m\u001b[32m0.80041\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 343 | loss: 0.80041 - acc: 0.7240 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1030  | total loss: \u001b[1m\u001b[32m0.82159\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 344 | loss: 0.82159 - acc: 0.7141 -- iter: 08/22\n",
            "Training Step: 1031  | total loss: \u001b[1m\u001b[32m0.76635\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 344 | loss: 0.76635 - acc: 0.7302 -- iter: 16/22\n",
            "Training Step: 1032  | total loss: \u001b[1m\u001b[32m0.77085\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 344 | loss: 0.77085 - acc: 0.7238 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1033  | total loss: \u001b[1m\u001b[32m0.77464\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 345 | loss: 0.77464 - acc: 0.7181 -- iter: 08/22\n",
            "Training Step: 1034  | total loss: \u001b[1m\u001b[32m0.76330\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 345 | loss: 0.76330 - acc: 0.7213 -- iter: 16/22\n",
            "Training Step: 1035  | total loss: \u001b[1m\u001b[32m0.74887\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 345 | loss: 0.74887 - acc: 0.7242 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1036  | total loss: \u001b[1m\u001b[32m0.75451\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 346 | loss: 0.75451 - acc: 0.7184 -- iter: 08/22\n",
            "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.74633\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 346 | loss: 0.74633 - acc: 0.7299 -- iter: 16/22\n",
            "Training Step: 1038  | total loss: \u001b[1m\u001b[32m0.73153\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 346 | loss: 0.73153 - acc: 0.7444 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.73618\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 347 | loss: 0.73618 - acc: 0.7200 -- iter: 08/22\n",
            "Training Step: 1040  | total loss: \u001b[1m\u001b[32m0.72941\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 347 | loss: 0.72941 - acc: 0.7313 -- iter: 16/22\n",
            "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.71827\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 347 | loss: 0.71827 - acc: 0.7415 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1042  | total loss: \u001b[1m\u001b[32m0.71139\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 348 | loss: 0.71139 - acc: 0.7424 -- iter: 08/22\n",
            "Training Step: 1043  | total loss: \u001b[1m\u001b[32m0.71643\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 348 | loss: 0.71643 - acc: 0.7431 -- iter: 16/22\n",
            "Training Step: 1044  | total loss: \u001b[1m\u001b[32m0.70618\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 348 | loss: 0.70618 - acc: 0.7688 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1045  | total loss: \u001b[1m\u001b[32m0.67987\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 349 | loss: 0.67987 - acc: 0.7919 -- iter: 08/22\n",
            "Training Step: 1046  | total loss: \u001b[1m\u001b[32m0.68700\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 349 | loss: 0.68700 - acc: 0.7877 -- iter: 16/22\n",
            "Training Step: 1047  | total loss: \u001b[1m\u001b[32m0.69687\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 349 | loss: 0.69687 - acc: 0.7840 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1048  | total loss: \u001b[1m\u001b[32m0.67112\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 350 | loss: 0.67112 - acc: 0.8056 -- iter: 08/22\n",
            "Training Step: 1049  | total loss: \u001b[1m\u001b[32m0.65028\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 350 | loss: 0.65028 - acc: 0.8083 -- iter: 16/22\n",
            "Training Step: 1050  | total loss: \u001b[1m\u001b[32m0.63408\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 350 | loss: 0.63408 - acc: 0.8275 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1051  | total loss: \u001b[1m\u001b[32m0.67357\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 351 | loss: 0.67357 - acc: 0.8073 -- iter: 08/22\n",
            "Training Step: 1052  | total loss: \u001b[1m\u001b[32m0.65207\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 351 | loss: 0.65207 - acc: 0.8099 -- iter: 16/22\n",
            "Training Step: 1053  | total loss: \u001b[1m\u001b[32m0.64593\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 351 | loss: 0.64593 - acc: 0.8289 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1054  | total loss: \u001b[1m\u001b[32m0.64512\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 352 | loss: 0.64512 - acc: 0.8210 -- iter: 08/22\n",
            "Training Step: 1055  | total loss: \u001b[1m\u001b[32m0.65825\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 352 | loss: 0.65825 - acc: 0.8139 -- iter: 16/22\n",
            "Training Step: 1056  | total loss: \u001b[1m\u001b[32m0.65116\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 352 | loss: 0.65116 - acc: 0.8325 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1057  | total loss: \u001b[1m\u001b[32m0.67881\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 353 | loss: 0.67881 - acc: 0.8493 -- iter: 08/22\n",
            "Training Step: 1058  | total loss: \u001b[1m\u001b[32m0.67621\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 353 | loss: 0.67621 - acc: 0.8518 -- iter: 16/22\n",
            "Training Step: 1059  | total loss: \u001b[1m\u001b[32m0.65895\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 353 | loss: 0.65895 - acc: 0.8416 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1060  | total loss: \u001b[1m\u001b[32m0.68562\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 354 | loss: 0.68562 - acc: 0.8575 -- iter: 08/22\n",
            "Training Step: 1061  | total loss: \u001b[1m\u001b[32m0.67918\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 354 | loss: 0.67918 - acc: 0.8384 -- iter: 16/22\n",
            "Training Step: 1062  | total loss: \u001b[1m\u001b[32m0.68791\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 354 | loss: 0.68791 - acc: 0.8296 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1063  | total loss: \u001b[1m\u001b[32m0.68055\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 355 | loss: 0.68055 - acc: 0.8466 -- iter: 08/22\n",
            "Training Step: 1064  | total loss: \u001b[1m\u001b[32m0.67445\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 355 | loss: 0.67445 - acc: 0.8286 -- iter: 16/22\n",
            "Training Step: 1065  | total loss: \u001b[1m\u001b[32m0.65079\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 355 | loss: 0.65079 - acc: 0.8291 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1066  | total loss: \u001b[1m\u001b[32m0.64572\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 356 | loss: 0.64572 - acc: 0.8337 -- iter: 08/22\n",
            "Training Step: 1067  | total loss: \u001b[1m\u001b[32m0.67251\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 356 | loss: 0.67251 - acc: 0.8253 -- iter: 16/22\n",
            "Training Step: 1068  | total loss: \u001b[1m\u001b[32m0.64891\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 356 | loss: 0.64891 - acc: 0.8261 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1069  | total loss: \u001b[1m\u001b[32m0.64700\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 357 | loss: 0.64700 - acc: 0.8435 -- iter: 08/22\n",
            "Training Step: 1070  | total loss: \u001b[1m\u001b[32m0.64275\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 357 | loss: 0.64275 - acc: 0.8466 -- iter: 16/22\n",
            "Training Step: 1071  | total loss: \u001b[1m\u001b[32m0.65458\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 357 | loss: 0.65458 - acc: 0.8245 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1072  | total loss: \u001b[1m\u001b[32m0.65186\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 358 | loss: 0.65186 - acc: 0.8420 -- iter: 08/22\n",
            "Training Step: 1073  | total loss: \u001b[1m\u001b[32m0.68006\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 358 | loss: 0.68006 - acc: 0.8578 -- iter: 16/22\n",
            "Training Step: 1074  | total loss: \u001b[1m\u001b[32m0.66305\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 358 | loss: 0.66305 - acc: 0.8470 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1075  | total loss: \u001b[1m\u001b[32m0.65902\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 359 | loss: 0.65902 - acc: 0.8373 -- iter: 08/22\n",
            "Training Step: 1076  | total loss: \u001b[1m\u001b[32m0.68634\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 359 | loss: 0.68634 - acc: 0.8369 -- iter: 16/22\n",
            "Training Step: 1077  | total loss: \u001b[1m\u001b[32m0.69824\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 359 | loss: 0.69824 - acc: 0.8199 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1078  | total loss: \u001b[1m\u001b[32m0.69321\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 360 | loss: 0.69321 - acc: 0.8129 -- iter: 08/22\n",
            "Training Step: 1079  | total loss: \u001b[1m\u001b[32m0.68155\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 360 | loss: 0.68155 - acc: 0.8191 -- iter: 16/22\n",
            "Training Step: 1080  | total loss: \u001b[1m\u001b[32m0.69376\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 360 | loss: 0.69376 - acc: 0.8039 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1081  | total loss: \u001b[1m\u001b[32m0.68613\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 361 | loss: 0.68613 - acc: 0.7902 -- iter: 08/22\n",
            "Training Step: 1082  | total loss: \u001b[1m\u001b[32m0.69561\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 361 | loss: 0.69561 - acc: 0.7861 -- iter: 16/22\n",
            "Training Step: 1083  | total loss: \u001b[1m\u001b[32m0.68405\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 361 | loss: 0.68405 - acc: 0.8075 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1084  | total loss: \u001b[1m\u001b[32m0.67721\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 362 | loss: 0.67721 - acc: 0.7934 -- iter: 08/22\n",
            "Training Step: 1085  | total loss: \u001b[1m\u001b[32m0.68343\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 362 | loss: 0.68343 - acc: 0.8141 -- iter: 16/22\n",
            "Training Step: 1086  | total loss: \u001b[1m\u001b[32m0.70937\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 362 | loss: 0.70937 - acc: 0.7827 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1087  | total loss: \u001b[1m\u001b[32m0.67061\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 363 | loss: 0.67061 - acc: 0.8044 -- iter: 08/22\n",
            "Training Step: 1088  | total loss: \u001b[1m\u001b[32m0.67729\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 363 | loss: 0.67729 - acc: 0.8240 -- iter: 16/22\n",
            "Training Step: 1089  | total loss: \u001b[1m\u001b[32m0.66736\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 363 | loss: 0.66736 - acc: 0.8416 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1090  | total loss: \u001b[1m\u001b[32m0.66405\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 364 | loss: 0.66405 - acc: 0.8449 -- iter: 08/22\n",
            "Training Step: 1091  | total loss: \u001b[1m\u001b[32m0.67231\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 364 | loss: 0.67231 - acc: 0.8229 -- iter: 16/22\n",
            "Training Step: 1092  | total loss: \u001b[1m\u001b[32m0.66267\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 364 | loss: 0.66267 - acc: 0.8406 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1093  | total loss: \u001b[1m\u001b[32m0.67626\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 365 | loss: 0.67626 - acc: 0.8232 -- iter: 08/22\n",
            "Training Step: 1094  | total loss: \u001b[1m\u001b[32m0.65585\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 365 | loss: 0.65585 - acc: 0.8284 -- iter: 16/22\n",
            "Training Step: 1095  | total loss: \u001b[1m\u001b[32m0.66410\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 365 | loss: 0.66410 - acc: 0.8331 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1096  | total loss: \u001b[1m\u001b[32m0.67736\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 366 | loss: 0.67736 - acc: 0.8164 -- iter: 08/22\n",
            "Training Step: 1097  | total loss: \u001b[1m\u001b[32m0.65559\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 366 | loss: 0.65559 - acc: 0.8348 -- iter: 16/22\n",
            "Training Step: 1098  | total loss: \u001b[1m\u001b[32m0.66740\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 366 | loss: 0.66740 - acc: 0.8138 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1099  | total loss: \u001b[1m\u001b[32m0.66933\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 367 | loss: 0.66933 - acc: 0.8199 -- iter: 08/22\n",
            "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.64827\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 367 | loss: 0.64827 - acc: 0.8379 -- iter: 16/22\n",
            "Training Step: 1101  | total loss: \u001b[1m\u001b[32m0.64835\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 367 | loss: 0.64835 - acc: 0.8208 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1102  | total loss: \u001b[1m\u001b[32m0.67036\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 368 | loss: 0.67036 - acc: 0.8137 -- iter: 08/22\n",
            "Training Step: 1103  | total loss: \u001b[1m\u001b[32m0.64786\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 368 | loss: 0.64786 - acc: 0.8199 -- iter: 16/22\n",
            "Training Step: 1104  | total loss: \u001b[1m\u001b[32m0.64784\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 368 | loss: 0.64784 - acc: 0.8045 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1105  | total loss: \u001b[1m\u001b[32m0.65927\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 369 | loss: 0.65927 - acc: 0.7908 -- iter: 08/22\n",
            "Training Step: 1106  | total loss: \u001b[1m\u001b[32m0.64271\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 369 | loss: 0.64271 - acc: 0.7992 -- iter: 16/22\n",
            "Training Step: 1107  | total loss: \u001b[1m\u001b[32m0.65146\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 369 | loss: 0.65146 - acc: 0.7943 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1108  | total loss: \u001b[1m\u001b[32m0.66242\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 370 | loss: 0.66242 - acc: 0.7815 -- iter: 08/22\n",
            "Training Step: 1109  | total loss: \u001b[1m\u001b[32m0.65993\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 370 | loss: 0.65993 - acc: 0.7867 -- iter: 16/22\n",
            "Training Step: 1110  | total loss: \u001b[1m\u001b[32m0.63838\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 370 | loss: 0.63838 - acc: 0.7830 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1111  | total loss: \u001b[1m\u001b[32m0.66142\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 371 | loss: 0.66142 - acc: 0.7672 -- iter: 08/22\n",
            "Training Step: 1112  | total loss: \u001b[1m\u001b[32m0.93924\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 371 | loss: 0.93924 - acc: 0.7405 -- iter: 16/22\n",
            "Training Step: 1113  | total loss: \u001b[1m\u001b[32m0.90088\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 371 | loss: 0.90088 - acc: 0.7498 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1114  | total loss: \u001b[1m\u001b[32m0.88334\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 372 | loss: 0.88334 - acc: 0.7373 -- iter: 08/22\n",
            "Training Step: 1115  | total loss: \u001b[1m\u001b[32m0.85965\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 372 | loss: 0.85965 - acc: 0.7386 -- iter: 16/22\n",
            "Training Step: 1116  | total loss: \u001b[1m\u001b[32m0.82920\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 372 | loss: 0.82920 - acc: 0.7480 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1117  | total loss: \u001b[1m\u001b[32m0.83969\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 373 | loss: 0.83969 - acc: 0.7566 -- iter: 08/22\n",
            "Training Step: 1118  | total loss: \u001b[1m\u001b[32m0.79969\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 373 | loss: 0.79969 - acc: 0.7559 -- iter: 16/22\n",
            "Training Step: 1119  | total loss: \u001b[1m\u001b[32m0.78439\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 373 | loss: 0.78439 - acc: 0.7428 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1120  | total loss: \u001b[1m\u001b[32m0.79921\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 374 | loss: 0.79921 - acc: 0.7519 -- iter: 08/22\n",
            "Training Step: 1121  | total loss: \u001b[1m\u001b[32m0.79590\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 374 | loss: 0.79590 - acc: 0.7434 -- iter: 16/22\n",
            "Training Step: 1122  | total loss: \u001b[1m\u001b[32m0.75371\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 374 | loss: 0.75371 - acc: 0.7440 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1123  | total loss: \u001b[1m\u001b[32m0.76190\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 375 | loss: 0.76190 - acc: 0.7446 -- iter: 08/22\n",
            "Training Step: 1124  | total loss: \u001b[1m\u001b[32m0.76222\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 375 | loss: 0.76222 - acc: 0.7368 -- iter: 16/22\n",
            "Training Step: 1125  | total loss: \u001b[1m\u001b[32m0.74498\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 375 | loss: 0.74498 - acc: 0.7298 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1126  | total loss: \u001b[1m\u001b[32m0.74447\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 376 | loss: 0.74447 - acc: 0.7443 -- iter: 08/22\n",
            "Training Step: 1127  | total loss: \u001b[1m\u001b[32m0.72993\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 376 | loss: 0.72993 - acc: 0.7324 -- iter: 16/22\n",
            "Training Step: 1128  | total loss: \u001b[1m\u001b[32m0.71582\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 376 | loss: 0.71582 - acc: 0.7258 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1129  | total loss: \u001b[1m\u001b[32m0.73424\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 377 | loss: 0.73424 - acc: 0.7366 -- iter: 08/22\n",
            "Training Step: 1130  | total loss: \u001b[1m\u001b[32m0.71312\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 377 | loss: 0.71312 - acc: 0.7254 -- iter: 16/22\n",
            "Training Step: 1131  | total loss: \u001b[1m\u001b[32m0.69978\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 377 | loss: 0.69978 - acc: 0.7279 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1132  | total loss: \u001b[1m\u001b[32m0.71956\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 378 | loss: 0.71956 - acc: 0.7384 -- iter: 08/22\n",
            "Training Step: 1133  | total loss: \u001b[1m\u001b[32m0.68975\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 378 | loss: 0.68975 - acc: 0.7479 -- iter: 16/22\n",
            "Training Step: 1134  | total loss: \u001b[1m\u001b[32m0.70517\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 378 | loss: 0.70517 - acc: 0.7481 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1135  | total loss: \u001b[1m\u001b[32m0.69596\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 379 | loss: 0.69596 - acc: 0.7358 -- iter: 08/22\n",
            "Training Step: 1136  | total loss: \u001b[1m\u001b[32m0.66839\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 379 | loss: 0.66839 - acc: 0.7456 -- iter: 16/22\n",
            "Training Step: 1137  | total loss: \u001b[1m\u001b[32m0.67708\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 379 | loss: 0.67708 - acc: 0.7377 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1138  | total loss: \u001b[1m\u001b[32m0.68310\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 380 | loss: 0.68310 - acc: 0.7389 -- iter: 08/22\n",
            "Training Step: 1139  | total loss: \u001b[1m\u001b[32m0.66127\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 380 | loss: 0.66127 - acc: 0.7400 -- iter: 16/22\n",
            "Training Step: 1140  | total loss: \u001b[1m\u001b[32m0.67053\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 380 | loss: 0.67053 - acc: 0.7327 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1141  | total loss: \u001b[1m\u001b[32m0.65870\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 381 | loss: 0.65870 - acc: 0.7427 -- iter: 08/22\n",
            "Training Step: 1142  | total loss: \u001b[1m\u001b[32m0.66896\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 381 | loss: 0.66896 - acc: 0.7435 -- iter: 16/22\n",
            "Training Step: 1143  | total loss: \u001b[1m\u001b[32m0.66093\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 381 | loss: 0.66093 - acc: 0.7316 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1144  | total loss: \u001b[1m\u001b[32m0.64994\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 382 | loss: 0.64994 - acc: 0.7418 -- iter: 08/22\n",
            "Training Step: 1145  | total loss: \u001b[1m\u001b[32m0.67525\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 382 | loss: 0.67525 - acc: 0.7176 -- iter: 16/22\n",
            "Training Step: 1146  | total loss: \u001b[1m\u001b[32m0.65939\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 382 | loss: 0.65939 - acc: 0.7209 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1147  | total loss: \u001b[1m\u001b[32m0.65008\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 383 | loss: 0.65008 - acc: 0.7363 -- iter: 08/22\n",
            "Training Step: 1148  | total loss: \u001b[1m\u001b[32m0.88120\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 383 | loss: 0.88120 - acc: 0.6793 -- iter: 16/22\n",
            "Training Step: 1149  | total loss: \u001b[1m\u001b[32m0.83187\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 383 | loss: 0.83187 - acc: 0.6947 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1150  | total loss: \u001b[1m\u001b[32m0.80617\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 384 | loss: 0.80617 - acc: 0.7127 -- iter: 08/22\n",
            "Training Step: 1151  | total loss: \u001b[1m\u001b[32m0.81483\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 384 | loss: 0.81483 - acc: 0.6915 -- iter: 16/22\n",
            "Training Step: 1152  | total loss: \u001b[1m\u001b[32m0.77192\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 384 | loss: 0.77192 - acc: 0.7057 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1153  | total loss: \u001b[1m\u001b[32m0.74966\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 385 | loss: 0.74966 - acc: 0.7351 -- iter: 08/22\n",
            "Training Step: 1154  | total loss: \u001b[1m\u001b[32m0.73844\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 385 | loss: 0.73844 - acc: 0.7241 -- iter: 16/22\n",
            "Training Step: 1155  | total loss: \u001b[1m\u001b[32m0.73544\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 385 | loss: 0.73544 - acc: 0.7540 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1156  | total loss: \u001b[1m\u001b[32m0.71680\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 386 | loss: 0.71680 - acc: 0.7540 -- iter: 08/22\n",
            "Training Step: 1157  | total loss: \u001b[1m\u001b[32m0.68568\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 386 | loss: 0.68568 - acc: 0.7786 -- iter: 16/22\n",
            "Training Step: 1158  | total loss: \u001b[1m\u001b[32m0.67781\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 386 | loss: 0.67781 - acc: 0.7882 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1159  | total loss: \u001b[1m\u001b[32m0.69471\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 387 | loss: 0.69471 - acc: 0.7719 -- iter: 08/22\n",
            "Training Step: 1160  | total loss: \u001b[1m\u001b[32m0.66566\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 387 | loss: 0.66566 - acc: 0.7947 -- iter: 16/22\n",
            "Training Step: 1161  | total loss: \u001b[1m\u001b[32m0.65765\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 387 | loss: 0.65765 - acc: 0.7819 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1162  | total loss: \u001b[1m\u001b[32m0.65033\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 388 | loss: 0.65033 - acc: 0.8037 -- iter: 08/22\n",
            "Training Step: 1163  | total loss: \u001b[1m\u001b[32m0.65849\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 388 | loss: 0.65849 - acc: 0.7984 -- iter: 16/22\n",
            "Training Step: 1164  | total loss: \u001b[1m\u001b[32m0.65107\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 388 | loss: 0.65107 - acc: 0.8019 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1165  | total loss: \u001b[1m\u001b[32m0.64486\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 389 | loss: 0.64486 - acc: 0.8050 -- iter: 08/22\n",
            "Training Step: 1166  | total loss: \u001b[1m\u001b[32m0.63771\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 389 | loss: 0.63771 - acc: 0.8120 -- iter: 16/22\n",
            "Training Step: 1167  | total loss: \u001b[1m\u001b[32m0.64761\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 389 | loss: 0.64761 - acc: 0.8058 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1168  | total loss: \u001b[1m\u001b[32m0.64168\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 390 | loss: 0.64168 - acc: 0.8086 -- iter: 08/22\n",
            "Training Step: 1169  | total loss: \u001b[1m\u001b[32m0.62421\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 390 | loss: 0.62421 - acc: 0.8110 -- iter: 16/22\n",
            "Training Step: 1170  | total loss: \u001b[1m\u001b[32m0.64357\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 390 | loss: 0.64357 - acc: 0.8174 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1171  | total loss: \u001b[1m\u001b[32m0.63719\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 391 | loss: 0.63719 - acc: 0.8107 -- iter: 08/22\n",
            "Training Step: 1172  | total loss: \u001b[1m\u001b[32m0.94389\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 391 | loss: 0.94389 - acc: 0.7963 -- iter: 16/22\n",
            "Training Step: 1173  | total loss: \u001b[1m\u001b[32m0.89239\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 391 | loss: 0.89239 - acc: 0.8167 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1174  | total loss: \u001b[1m\u001b[32m0.86966\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 392 | loss: 0.86966 - acc: 0.8225 -- iter: 08/22\n",
            "Training Step: 1175  | total loss: \u001b[1m\u001b[32m0.85848\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 392 | loss: 0.85848 - acc: 0.7777 -- iter: 16/22\n",
            "Training Step: 1176  | total loss: \u001b[1m\u001b[32m0.81560\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 392 | loss: 0.81560 - acc: 0.8000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1177  | total loss: \u001b[1m\u001b[32m0.77834\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 393 | loss: 0.77834 - acc: 0.8033 -- iter: 08/22\n",
            "Training Step: 1178  | total loss: \u001b[1m\u001b[32m0.79695\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 393 | loss: 0.79695 - acc: 0.7855 -- iter: 16/22\n",
            "Training Step: 1179  | total loss: \u001b[1m\u001b[32m0.76183\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 393 | loss: 0.76183 - acc: 0.7819 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1180  | total loss: \u001b[1m\u001b[32m0.72994\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 394 | loss: 0.72994 - acc: 0.7871 -- iter: 08/22\n",
            "Training Step: 1181  | total loss: \u001b[1m\u001b[32m0.71153\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 394 | loss: 0.71153 - acc: 0.7750 -- iter: 16/22\n",
            "Training Step: 1182  | total loss: \u001b[1m\u001b[32m0.70085\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 394 | loss: 0.70085 - acc: 0.7850 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1183  | total loss: \u001b[1m\u001b[32m0.70325\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 395 | loss: 0.70325 - acc: 0.7690 -- iter: 08/22\n",
            "Training Step: 1184  | total loss: \u001b[1m\u001b[32m1.40198\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 395 | loss: 1.40198 - acc: 0.7255 -- iter: 16/22\n",
            "Training Step: 1185  | total loss: \u001b[1m\u001b[32m1.33508\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 395 | loss: 1.33508 - acc: 0.7362 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1186  | total loss: \u001b[1m\u001b[32m1.28731\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 396 | loss: 1.28731 - acc: 0.7251 -- iter: 08/22\n",
            "Training Step: 1187  | total loss: \u001b[1m\u001b[32m1.19175\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 396 | loss: 1.19175 - acc: 0.7276 -- iter: 16/22\n",
            "Training Step: 1188  | total loss: \u001b[1m\u001b[32m1.14585\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 396 | loss: 1.14585 - acc: 0.7382 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1189  | total loss: \u001b[1m\u001b[32m1.08984\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 397 | loss: 1.08984 - acc: 0.7310 -- iter: 08/22\n",
            "Training Step: 1190  | total loss: \u001b[1m\u001b[32m1.04922\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 397 | loss: 1.04922 - acc: 0.7329 -- iter: 16/22\n",
            "Training Step: 1191  | total loss: \u001b[1m\u001b[32m1.00604\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 397 | loss: 1.00604 - acc: 0.7346 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1192  | total loss: \u001b[1m\u001b[32m0.96402\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 398 | loss: 0.96402 - acc: 0.7278 -- iter: 08/22\n",
            "Training Step: 1193  | total loss: \u001b[1m\u001b[32m0.92624\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 398 | loss: 0.92624 - acc: 0.7217 -- iter: 16/22\n",
            "Training Step: 1194  | total loss: \u001b[1m\u001b[32m0.90091\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 398 | loss: 0.90091 - acc: 0.7245 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1195  | total loss: \u001b[1m\u001b[32m0.87353\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 399 | loss: 0.87353 - acc: 0.7271 -- iter: 08/22\n",
            "Training Step: 1196  | total loss: \u001b[1m\u001b[32m1.21189\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 399 | loss: 1.21189 - acc: 0.6710 -- iter: 16/22\n",
            "Training Step: 1197  | total loss: \u001b[1m\u001b[32m1.16515\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 399 | loss: 1.16515 - acc: 0.6706 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1198  | total loss: \u001b[1m\u001b[32m1.08320\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 400 | loss: 1.08320 - acc: 0.7035 -- iter: 08/22\n",
            "Training Step: 1199  | total loss: \u001b[1m\u001b[32m1.05855\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 400 | loss: 1.05855 - acc: 0.6832 -- iter: 16/22\n",
            "Training Step: 1200  | total loss: \u001b[1m\u001b[32m1.02710\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 400 | loss: 1.02710 - acc: 0.6815 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1201  | total loss: \u001b[1m\u001b[32m0.97807\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 401 | loss: 0.97807 - acc: 0.6967 -- iter: 08/22\n",
            "Training Step: 1202  | total loss: \u001b[1m\u001b[32m0.95315\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 401 | loss: 0.95315 - acc: 0.6895 -- iter: 16/22\n",
            "Training Step: 1203  | total loss: \u001b[1m\u001b[32m0.91884\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 401 | loss: 0.91884 - acc: 0.6956 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1204  | total loss: \u001b[1m\u001b[32m0.88058\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 402 | loss: 0.88058 - acc: 0.7094 -- iter: 08/22\n",
            "Training Step: 1205  | total loss: \u001b[1m\u001b[32m0.84609\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 402 | loss: 0.84609 - acc: 0.7384 -- iter: 16/22\n",
            "Training Step: 1206  | total loss: \u001b[1m\u001b[32m0.81039\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 402 | loss: 0.81039 - acc: 0.7396 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1207  | total loss: \u001b[1m\u001b[32m0.81437\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 403 | loss: 0.81437 - acc: 0.7156 -- iter: 08/22\n",
            "Training Step: 1208  | total loss: \u001b[1m\u001b[32m0.78638\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 403 | loss: 0.78638 - acc: 0.7441 -- iter: 16/22\n",
            "Training Step: 1209  | total loss: \u001b[1m\u001b[32m0.80357\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 403 | loss: 0.80357 - acc: 0.7197 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1210  | total loss: \u001b[1m\u001b[32m0.78123\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 404 | loss: 0.78123 - acc: 0.7352 -- iter: 08/22\n",
            "Training Step: 1211  | total loss: \u001b[1m\u001b[32m0.74703\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 404 | loss: 0.74703 - acc: 0.7367 -- iter: 16/22\n",
            "Training Step: 1212  | total loss: \u001b[1m\u001b[32m0.76794\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 404 | loss: 0.76794 - acc: 0.7130 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1213  | total loss: \u001b[1m\u001b[32m0.74850\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 405 | loss: 0.74850 - acc: 0.7250 -- iter: 08/22\n",
            "Training Step: 1214  | total loss: \u001b[1m\u001b[32m0.72820\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 405 | loss: 0.72820 - acc: 0.7275 -- iter: 16/22\n",
            "Training Step: 1215  | total loss: \u001b[1m\u001b[32m0.73119\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 405 | loss: 0.73119 - acc: 0.7173 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1216  | total loss: \u001b[1m\u001b[32m0.71535\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 406 | loss: 0.71535 - acc: 0.7289 -- iter: 08/22\n",
            "Training Step: 1217  | total loss: \u001b[1m\u001b[32m0.73854\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 406 | loss: 0.73854 - acc: 0.7227 -- iter: 16/22\n",
            "Training Step: 1218  | total loss: \u001b[1m\u001b[32m0.74491\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 406 | loss: 0.74491 - acc: 0.7129 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1219  | total loss: \u001b[1m\u001b[32m0.69212\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 407 | loss: 0.69212 - acc: 0.7291 -- iter: 08/22\n",
            "Training Step: 1220  | total loss: \u001b[1m\u001b[32m0.71735\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 407 | loss: 0.71735 - acc: 0.7229 -- iter: 16/22\n",
            "Training Step: 1221  | total loss: \u001b[1m\u001b[32m0.71908\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 407 | loss: 0.71908 - acc: 0.7172 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1222  | total loss: \u001b[1m\u001b[32m0.69571\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 408 | loss: 0.69571 - acc: 0.7205 -- iter: 08/22\n",
            "Training Step: 1223  | total loss: \u001b[1m\u001b[32m0.69488\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 408 | loss: 0.69488 - acc: 0.7235 -- iter: 16/22\n",
            "Training Step: 1224  | total loss: \u001b[1m\u001b[32m0.69871\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 408 | loss: 0.69871 - acc: 0.7178 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1225  | total loss: \u001b[1m\u001b[32m0.68514\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 409 | loss: 0.68514 - acc: 0.7127 -- iter: 08/22\n",
            "Training Step: 1226  | total loss: \u001b[1m\u001b[32m0.66718\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 409 | loss: 0.66718 - acc: 0.7164 -- iter: 16/22\n",
            "Training Step: 1227  | total loss: \u001b[1m\u001b[32m0.67953\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 409 | loss: 0.67953 - acc: 0.7198 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1228  | total loss: \u001b[1m\u001b[32m0.66776\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 410 | loss: 0.66776 - acc: 0.7145 -- iter: 08/22\n",
            "Training Step: 1229  | total loss: \u001b[1m\u001b[32m0.67289\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 410 | loss: 0.67289 - acc: 0.7263 -- iter: 16/22\n",
            "Training Step: 1230  | total loss: \u001b[1m\u001b[32m0.67742\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 410 | loss: 0.67742 - acc: 0.7037 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1231  | total loss: \u001b[1m\u001b[32m0.65531\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 411 | loss: 0.65531 - acc: 0.7208 -- iter: 08/22\n",
            "Training Step: 1232  | total loss: \u001b[1m\u001b[32m0.66149\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 411 | loss: 0.66149 - acc: 0.7321 -- iter: 16/22\n",
            "Training Step: 1233  | total loss: \u001b[1m\u001b[32m0.63748\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 411 | loss: 0.63748 - acc: 0.7255 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1234  | total loss: \u001b[1m\u001b[32m0.64170\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 412 | loss: 0.64170 - acc: 0.7405 -- iter: 08/22\n",
            "Training Step: 1235  | total loss: \u001b[1m\u001b[32m0.64878\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 412 | loss: 0.64878 - acc: 0.7289 -- iter: 16/22\n",
            "Training Step: 1236  | total loss: \u001b[1m\u001b[32m0.62596\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 412 | loss: 0.62596 - acc: 0.7227 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1237  | total loss: \u001b[1m\u001b[32m0.63909\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 413 | loss: 0.63909 - acc: 0.7004 -- iter: 08/22\n",
            "Training Step: 1238  | total loss: \u001b[1m\u001b[32m0.63465\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 413 | loss: 0.63465 - acc: 0.6929 -- iter: 16/22\n",
            "Training Step: 1239  | total loss: \u001b[1m\u001b[32m0.62534\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 413 | loss: 0.62534 - acc: 0.7236 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1240  | total loss: \u001b[1m\u001b[32m1.03488\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 414 | loss: 1.03488 - acc: 0.6512 -- iter: 08/22\n",
            "Training Step: 1241  | total loss: \u001b[1m\u001b[32m1.02040\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 414 | loss: 1.02040 - acc: 0.6361 -- iter: 16/22\n",
            "Training Step: 1242  | total loss: \u001b[1m\u001b[32m0.95238\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 414 | loss: 0.95238 - acc: 0.6725 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1243  | total loss: \u001b[1m\u001b[32m0.92299\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 415 | loss: 0.92299 - acc: 0.6510 -- iter: 08/22\n",
            "Training Step: 1244  | total loss: \u001b[1m\u001b[32m0.92299\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 415 | loss: 0.92299 - acc: 0.6510 -- iter: 16/22\n",
            "Training Step: 1245  | total loss: \u001b[1m\u001b[32m0.85429\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 415 | loss: 0.85429 - acc: 0.6859 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1246  | total loss: \u001b[1m\u001b[32m0.86433\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 416 | loss: 0.86433 - acc: 0.6673 -- iter: 08/22\n",
            "Training Step: 1247  | total loss: \u001b[1m\u001b[32m0.83516\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 416 | loss: 0.83516 - acc: 0.6756 -- iter: 16/22\n",
            "Training Step: 1248  | total loss: \u001b[1m\u001b[32m0.77524\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 416 | loss: 0.77524 - acc: 0.7080 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.75219\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 417 | loss: 0.75219 - acc: 0.7205 -- iter: 08/22\n",
            "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.72382\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 417 | loss: 0.72382 - acc: 0.7360 -- iter: 16/22\n",
            "Training Step: 1251  | total loss: \u001b[1m\u001b[32m0.73415\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 417 | loss: 0.73415 - acc: 0.7124 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1252  | total loss: \u001b[1m\u001b[32m1.17481\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 418 | loss: 1.17481 - acc: 0.6745 -- iter: 08/22\n",
            "Training Step: 1253  | total loss: \u001b[1m\u001b[32m1.11715\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 418 | loss: 1.11715 - acc: 0.6737 -- iter: 16/22\n",
            "Training Step: 1254  | total loss: \u001b[1m\u001b[32m1.05248\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 418 | loss: 1.05248 - acc: 0.6938 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1255  | total loss: \u001b[1m\u001b[32m1.02611\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 419 | loss: 1.02611 - acc: 0.6869 -- iter: 08/22\n",
            "Training Step: 1256  | total loss: \u001b[1m\u001b[32m2.05522\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 419 | loss: 2.05522 - acc: 0.6183 -- iter: 16/22\n",
            "Training Step: 1257  | total loss: \u001b[1m\u001b[32m1.87385\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 419 | loss: 1.87385 - acc: 0.6398 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1258  | total loss: \u001b[1m\u001b[32m1.75438\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 420 | loss: 1.75438 - acc: 0.6383 -- iter: 08/22\n",
            "Training Step: 1259  | total loss: \u001b[1m\u001b[32m1.66437\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 420 | loss: 1.66437 - acc: 0.6495 -- iter: 16/22\n",
            "Training Step: 1260  | total loss: \u001b[1m\u001b[32m1.80946\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 420 | loss: 1.80946 - acc: 0.6345 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1261  | total loss: \u001b[1m\u001b[32m1.67025\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 421 | loss: 1.67025 - acc: 0.6711 -- iter: 08/22\n",
            "Training Step: 1262  | total loss: \u001b[1m\u001b[32m1.58480\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 421 | loss: 1.58480 - acc: 0.6540 -- iter: 16/22\n",
            "Training Step: 1263  | total loss: \u001b[1m\u001b[32m1.48607\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 421 | loss: 1.48607 - acc: 0.6636 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1264  | total loss: \u001b[1m\u001b[32m1.37945\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 422 | loss: 1.37945 - acc: 0.6972 -- iter: 08/22\n",
            "Training Step: 1265  | total loss: \u001b[1m\u001b[32m1.31862\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 422 | loss: 1.31862 - acc: 0.6941 -- iter: 16/22\n",
            "Training Step: 1266  | total loss: \u001b[1m\u001b[32m1.25866\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 422 | loss: 1.25866 - acc: 0.6872 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1267  | total loss: \u001b[1m\u001b[32m1.17632\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 423 | loss: 1.17632 - acc: 0.7060 -- iter: 08/22\n",
            "Training Step: 1268  | total loss: \u001b[1m\u001b[32m1.43393\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 423 | loss: 1.43393 - acc: 0.6521 -- iter: 16/22\n",
            "Training Step: 1269  | total loss: \u001b[1m\u001b[32m1.33352\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 423 | loss: 1.33352 - acc: 0.6702 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1270  | total loss: \u001b[1m\u001b[32m1.25604\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 424 | loss: 1.25604 - acc: 0.6907 -- iter: 08/22\n",
            "Training Step: 1271  | total loss: \u001b[1m\u001b[32m1.21586\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 424 | loss: 1.21586 - acc: 0.6716 -- iter: 16/22\n",
            "Training Step: 1272  | total loss: \u001b[1m\u001b[32m1.13690\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 424 | loss: 1.13690 - acc: 0.6878 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1273  | total loss: \u001b[1m\u001b[32m1.09325\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 425 | loss: 1.09325 - acc: 0.7023 -- iter: 08/22\n",
            "Training Step: 1274  | total loss: \u001b[1m\u001b[32m1.04660\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 425 | loss: 1.04660 - acc: 0.6946 -- iter: 16/22\n",
            "Training Step: 1275  | total loss: \u001b[1m\u001b[32m1.00016\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 425 | loss: 1.00016 - acc: 0.7001 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1276  | total loss: \u001b[1m\u001b[32m0.97014\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 426 | loss: 0.97014 - acc: 0.7135 -- iter: 08/22\n",
            "Training Step: 1277  | total loss: \u001b[1m\u001b[32m0.91989\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 426 | loss: 0.91989 - acc: 0.7255 -- iter: 16/22\n",
            "Training Step: 1278  | total loss: \u001b[1m\u001b[32m0.90923\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 426 | loss: 0.90923 - acc: 0.7154 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1279  | total loss: \u001b[1m\u001b[32m0.87511\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 427 | loss: 0.87511 - acc: 0.7189 -- iter: 08/22\n",
            "Training Step: 1280  | total loss: \u001b[1m\u001b[32m0.83433\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 427 | loss: 0.83433 - acc: 0.7303 -- iter: 16/22\n",
            "Training Step: 1281  | total loss: \u001b[1m\u001b[32m0.80664\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 427 | loss: 0.80664 - acc: 0.7406 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1282  | total loss: \u001b[1m\u001b[32m0.78461\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 428 | loss: 0.78461 - acc: 0.7416 -- iter: 08/22\n",
            "Training Step: 1283  | total loss: \u001b[1m\u001b[32m0.77867\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 428 | loss: 0.77867 - acc: 0.7299 -- iter: 16/22\n",
            "Training Step: 1284  | total loss: \u001b[1m\u001b[32m0.75642\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 428 | loss: 0.75642 - acc: 0.7402 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1285  | total loss: \u001b[1m\u001b[32m0.76942\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 429 | loss: 0.76942 - acc: 0.7162 -- iter: 08/22\n",
            "Training Step: 1286  | total loss: \u001b[1m\u001b[32m0.75009\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 429 | loss: 0.75009 - acc: 0.7196 -- iter: 16/22\n",
            "Training Step: 1287  | total loss: \u001b[1m\u001b[32m0.72364\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 429 | loss: 0.72364 - acc: 0.7351 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1288  | total loss: \u001b[1m\u001b[32m0.73968\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 430 | loss: 0.73968 - acc: 0.7116 -- iter: 08/22\n",
            "Training Step: 1289  | total loss: \u001b[1m\u001b[32m0.69365\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 430 | loss: 0.69365 - acc: 0.7238 -- iter: 16/22\n",
            "Training Step: 1290  | total loss: \u001b[1m\u001b[32m0.71749\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 430 | loss: 0.71749 - acc: 0.7139 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1291  | total loss: \u001b[1m\u001b[32m0.70372\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 431 | loss: 0.70372 - acc: 0.7175 -- iter: 08/22\n",
            "Training Step: 1292  | total loss: \u001b[1m\u001b[32m1.25685\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 431 | loss: 1.25685 - acc: 0.6958 -- iter: 16/22\n",
            "Training Step: 1293  | total loss: \u001b[1m\u001b[32m1.17526\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 431 | loss: 1.17526 - acc: 0.6929 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1294  | total loss: \u001b[1m\u001b[32m1.14963\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 432 | loss: 1.14963 - acc: 0.6736 -- iter: 08/22\n",
            "Training Step: 1295  | total loss: \u001b[1m\u001b[32m1.08190\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 432 | loss: 1.08190 - acc: 0.7062 -- iter: 16/22\n",
            "Training Step: 1296  | total loss: \u001b[1m\u001b[32m1.01790\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 432 | loss: 1.01790 - acc: 0.7023 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1297  | total loss: \u001b[1m\u001b[32m0.96997\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 433 | loss: 0.96997 - acc: 0.7154 -- iter: 08/22\n",
            "Training Step: 1298  | total loss: \u001b[1m\u001b[32m0.93426\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 433 | loss: 0.93426 - acc: 0.7188 -- iter: 16/22\n",
            "Training Step: 1299  | total loss: \u001b[1m\u001b[32m0.91145\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 433 | loss: 0.91145 - acc: 0.7094 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1300  | total loss: \u001b[1m\u001b[32m0.87408\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 434 | loss: 0.87408 - acc: 0.7218 -- iter: 08/22\n",
            "Training Step: 1301  | total loss: \u001b[1m\u001b[32m0.86115\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 434 | loss: 0.86115 - acc: 0.6997 -- iter: 16/22\n",
            "Training Step: 1302  | total loss: \u001b[1m\u001b[32m0.83667\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 434 | loss: 0.83667 - acc: 0.7047 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1303  | total loss: \u001b[1m\u001b[32m0.80773\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 435 | loss: 0.80773 - acc: 0.7217 -- iter: 08/22\n",
            "Training Step: 1304  | total loss: \u001b[1m\u001b[32m0.80133\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 435 | loss: 0.80133 - acc: 0.6995 -- iter: 16/22\n",
            "Training Step: 1305  | total loss: \u001b[1m\u001b[32m0.77801\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 435 | loss: 0.77801 - acc: 0.6963 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1306  | total loss: \u001b[1m\u001b[32m0.74799\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 436 | loss: 0.74799 - acc: 0.7141 -- iter: 08/22\n",
            "Training Step: 1307  | total loss: \u001b[1m\u001b[32m0.75474\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 436 | loss: 0.75474 - acc: 0.7052 -- iter: 16/22\n",
            "Training Step: 1308  | total loss: \u001b[1m\u001b[32m0.73596\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 436 | loss: 0.73596 - acc: 0.7014 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1309  | total loss: \u001b[1m\u001b[32m0.71664\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 437 | loss: 0.71664 - acc: 0.7146 -- iter: 08/22\n",
            "Training Step: 1310  | total loss: \u001b[1m\u001b[32m0.69332\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 437 | loss: 0.69332 - acc: 0.7181 -- iter: 16/22\n",
            "Training Step: 1311  | total loss: \u001b[1m\u001b[32m0.70652\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 437 | loss: 0.70652 - acc: 0.7213 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1312  | total loss: \u001b[1m\u001b[32m0.69000\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 438 | loss: 0.69000 - acc: 0.7325 -- iter: 08/22\n",
            "Training Step: 1313  | total loss: \u001b[1m\u001b[32m0.69128\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 438 | loss: 0.69128 - acc: 0.7592 -- iter: 16/22\n",
            "Training Step: 1314  | total loss: \u001b[1m\u001b[32m0.66624\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 438 | loss: 0.66624 - acc: 0.7708 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1315  | total loss: \u001b[1m\u001b[32m0.67402\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 439 | loss: 0.67402 - acc: 0.7437 -- iter: 08/22\n",
            "Training Step: 1316  | total loss: \u001b[1m\u001b[32m0.67664\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 439 | loss: 0.67664 - acc: 0.7694 -- iter: 16/22\n",
            "Training Step: 1317  | total loss: \u001b[1m\u001b[32m0.66451\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 439 | loss: 0.66451 - acc: 0.7758 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1318  | total loss: \u001b[1m\u001b[32m0.69095\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 440 | loss: 0.69095 - acc: 0.7482 -- iter: 08/22\n",
            "Training Step: 1319  | total loss: \u001b[1m\u001b[32m0.65797\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 440 | loss: 0.65797 - acc: 0.7734 -- iter: 16/22\n",
            "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.64750\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 440 | loss: 0.64750 - acc: 0.7794 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1321  | total loss: \u001b[1m\u001b[32m0.62085\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 441 | loss: 0.62085 - acc: 0.8014 -- iter: 08/22\n",
            "Training Step: 1322  | total loss: \u001b[1m\u001b[32m0.61579\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 441 | loss: 0.61579 - acc: 0.7963 -- iter: 16/22\n",
            "Training Step: 1323  | total loss: \u001b[1m\u001b[32m0.63881\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 441 | loss: 0.63881 - acc: 0.7667 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1324  | total loss: \u001b[1m\u001b[32m0.61275\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 442 | loss: 0.61275 - acc: 0.7900 -- iter: 08/22\n",
            "Training Step: 1325  | total loss: \u001b[1m\u001b[32m0.61550\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 442 | loss: 0.61550 - acc: 0.7777 -- iter: 16/22\n",
            "Training Step: 1326  | total loss: \u001b[1m\u001b[32m0.60996\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 442 | loss: 0.60996 - acc: 0.7749 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1327  | total loss: \u001b[1m\u001b[32m0.61451\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 443 | loss: 0.61451 - acc: 0.7724 -- iter: 08/22\n",
            "Training Step: 1328  | total loss: \u001b[1m\u001b[32m0.61692\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 443 | loss: 0.61692 - acc: 0.7618 -- iter: 16/22\n",
            "Training Step: 1329  | total loss: \u001b[1m\u001b[32m0.61358\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 443 | loss: 0.61358 - acc: 0.7690 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1330  | total loss: \u001b[1m\u001b[32m0.61845\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 444 | loss: 0.61845 - acc: 0.7671 -- iter: 08/22\n",
            "Training Step: 1331  | total loss: \u001b[1m\u001b[32m0.61575\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 444 | loss: 0.61575 - acc: 0.7529 -- iter: 16/22\n",
            "Training Step: 1332  | total loss: \u001b[1m\u001b[32m0.61222\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 444 | loss: 0.61222 - acc: 0.7609 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1333  | total loss: \u001b[1m\u001b[32m0.59061\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 445 | loss: 0.59061 - acc: 0.7682 -- iter: 08/22\n",
            "Training Step: 1334  | total loss: \u001b[1m\u001b[32m0.57564\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 445 | loss: 0.57564 - acc: 0.7788 -- iter: 16/22\n",
            "Training Step: 1335  | total loss: \u001b[1m\u001b[32m0.61282\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 445 | loss: 0.61282 - acc: 0.7510 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1336  | total loss: \u001b[1m\u001b[32m0.59102\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 446 | loss: 0.59102 - acc: 0.7592 -- iter: 08/22\n",
            "Training Step: 1337  | total loss: \u001b[1m\u001b[32m0.59196\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 446 | loss: 0.59196 - acc: 0.7499 -- iter: 16/22\n",
            "Training Step: 1338  | total loss: \u001b[1m\u001b[32m0.60210\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 446 | loss: 0.60210 - acc: 0.7500 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1339  | total loss: \u001b[1m\u001b[32m0.59562\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 447 | loss: 0.59562 - acc: 0.7500 -- iter: 08/22\n",
            "Training Step: 1340  | total loss: \u001b[1m\u001b[32m0.59584\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 447 | loss: 0.59584 - acc: 0.7416 -- iter: 16/22\n",
            "Training Step: 1341  | total loss: \u001b[1m\u001b[32m0.62460\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 447 | loss: 0.62460 - acc: 0.7341 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1342  | total loss: \u001b[1m\u001b[32m0.60947\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 448 | loss: 0.60947 - acc: 0.7232 -- iter: 08/22\n",
            "Training Step: 1343  | total loss: \u001b[1m\u001b[32m0.60256\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 448 | loss: 0.60256 - acc: 0.7384 -- iter: 16/22\n",
            "Training Step: 1344  | total loss: \u001b[1m\u001b[32m0.63038\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 448 | loss: 0.63038 - acc: 0.7312 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1345  | total loss: \u001b[1m\u001b[32m0.64103\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 449 | loss: 0.64103 - acc: 0.7081 -- iter: 08/22\n",
            "Training Step: 1346  | total loss: \u001b[1m\u001b[32m0.63984\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 449 | loss: 0.63984 - acc: 0.6998 -- iter: 16/22\n",
            "Training Step: 1347  | total loss: \u001b[1m\u001b[32m0.62475\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 449 | loss: 0.62475 - acc: 0.7298 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1348  | total loss: \u001b[1m\u001b[32m0.63579\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 450 | loss: 0.63579 - acc: 0.7068 -- iter: 08/22\n",
            "Training Step: 1349  | total loss: \u001b[1m\u001b[32m0.63355\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 450 | loss: 0.63355 - acc: 0.7028 -- iter: 16/22\n",
            "Training Step: 1350  | total loss: \u001b[1m\u001b[32m0.62810\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 450 | loss: 0.62810 - acc: 0.6950 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1351  | total loss: \u001b[1m\u001b[32m0.62792\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 451 | loss: 0.62792 - acc: 0.7130 -- iter: 08/22\n",
            "Training Step: 1352  | total loss: \u001b[1m\u001b[32m0.62630\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 451 | loss: 0.62630 - acc: 0.7084 -- iter: 16/22\n",
            "Training Step: 1353  | total loss: \u001b[1m\u001b[32m0.60120\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 451 | loss: 0.60120 - acc: 0.7209 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1354  | total loss: \u001b[1m\u001b[32m0.60669\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 452 | loss: 0.60669 - acc: 0.7238 -- iter: 08/22\n",
            "Training Step: 1355  | total loss: \u001b[1m\u001b[32m0.61826\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 452 | loss: 0.61826 - acc: 0.7139 -- iter: 16/22\n",
            "Training Step: 1356  | total loss: \u001b[1m\u001b[32m0.59384\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 452 | loss: 0.59384 - acc: 0.7259 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1357  | total loss: \u001b[1m\u001b[32m0.59793\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 453 | loss: 0.59793 - acc: 0.7033 -- iter: 08/22\n",
            "Training Step: 1358  | total loss: \u001b[1m\u001b[32m0.61354\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 453 | loss: 0.61354 - acc: 0.7079 -- iter: 16/22\n",
            "Training Step: 1359  | total loss: \u001b[1m\u001b[32m0.59476\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 453 | loss: 0.59476 - acc: 0.7247 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1360  | total loss: \u001b[1m\u001b[32m0.59841\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 454 | loss: 0.59841 - acc: 0.7022 -- iter: 08/22\n",
            "Training Step: 1361  | total loss: \u001b[1m\u001b[32m0.60668\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 454 | loss: 0.60668 - acc: 0.7153 -- iter: 16/22\n",
            "Training Step: 1362  | total loss: \u001b[1m\u001b[32m0.60556\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 454 | loss: 0.60556 - acc: 0.7188 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1363  | total loss: \u001b[1m\u001b[32m0.59937\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 455 | loss: 0.59937 - acc: 0.7094 -- iter: 08/22\n",
            "Training Step: 1364  | total loss: \u001b[1m\u001b[32m1.26416\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 455 | loss: 1.26416 - acc: 0.6385 -- iter: 16/22\n",
            "Training Step: 1365  | total loss: \u001b[1m\u001b[32m1.17761\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 455 | loss: 1.17761 - acc: 0.6579 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1366  | total loss: \u001b[1m\u001b[32m1.11125\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 456 | loss: 1.11125 - acc: 0.6796 -- iter: 08/22\n",
            "Training Step: 1367  | total loss: \u001b[1m\u001b[32m1.08361\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 456 | loss: 1.08361 - acc: 0.6617 -- iter: 16/22\n",
            "Training Step: 1368  | total loss: \u001b[1m\u001b[32m1.01518\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 456 | loss: 1.01518 - acc: 0.6788 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1369  | total loss: \u001b[1m\u001b[32m0.96817\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 457 | loss: 0.96817 - acc: 0.6776 -- iter: 08/22\n",
            "Training Step: 1370  | total loss: \u001b[1m\u001b[32m0.94894\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 457 | loss: 0.94894 - acc: 0.6849 -- iter: 16/22\n",
            "Training Step: 1371  | total loss: \u001b[1m\u001b[32m0.90049\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 457 | loss: 0.90049 - acc: 0.6914 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1372  | total loss: \u001b[1m\u001b[32m0.86488\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 458 | loss: 0.86488 - acc: 0.6889 -- iter: 08/22\n",
            "Training Step: 1373  | total loss: \u001b[1m\u001b[32m0.86750\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 458 | loss: 0.86750 - acc: 0.6700 -- iter: 16/22\n",
            "Training Step: 1374  | total loss: \u001b[1m\u001b[32m0.82387\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 458 | loss: 0.82387 - acc: 0.6905 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1375  | total loss: \u001b[1m\u001b[32m0.79636\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 459 | loss: 0.79636 - acc: 0.6965 -- iter: 08/22\n",
            "Training Step: 1376  | total loss: \u001b[1m\u001b[32m0.80572\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 459 | loss: 0.80572 - acc: 0.6768 -- iter: 16/22\n",
            "Training Step: 1377  | total loss: \u001b[1m\u001b[32m0.78269\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 459 | loss: 0.78269 - acc: 0.6925 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1378  | total loss: \u001b[1m\u001b[32m0.74984\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 460 | loss: 0.74984 - acc: 0.6857 -- iter: 08/22\n",
            "Training Step: 1379  | total loss: \u001b[1m\u001b[32m0.75086\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 460 | loss: 0.75086 - acc: 0.6922 -- iter: 16/22\n",
            "Training Step: 1380  | total loss: \u001b[1m\u001b[32m0.73328\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 460 | loss: 0.73328 - acc: 0.7063 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1381  | total loss: \u001b[1m\u001b[32m0.71064\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 461 | loss: 0.71064 - acc: 0.7190 -- iter: 08/22\n",
            "Training Step: 1382  | total loss: \u001b[1m\u001b[32m0.71822\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 461 | loss: 0.71822 - acc: 0.7096 -- iter: 16/22\n",
            "Training Step: 1383  | total loss: \u001b[1m\u001b[32m0.69402\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 461 | loss: 0.69402 - acc: 0.7136 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1384  | total loss: \u001b[1m\u001b[32m0.67517\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 462 | loss: 0.67517 - acc: 0.7256 -- iter: 08/22\n",
            "Training Step: 1385  | total loss: \u001b[1m\u001b[32m0.67401\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 462 | loss: 0.67401 - acc: 0.7364 -- iter: 16/22\n",
            "Training Step: 1386  | total loss: \u001b[1m\u001b[32m0.64918\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 462 | loss: 0.64918 - acc: 0.7377 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1387  | total loss: \u001b[1m\u001b[32m0.65580\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 463 | loss: 0.65580 - acc: 0.7390 -- iter: 08/22\n",
            "Training Step: 1388  | total loss: \u001b[1m\u001b[32m0.65640\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 463 | loss: 0.65640 - acc: 0.7484 -- iter: 16/22\n",
            "Training Step: 1389  | total loss: \u001b[1m\u001b[32m0.64805\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 463 | loss: 0.64805 - acc: 0.7569 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1390  | total loss: \u001b[1m\u001b[32m0.64140\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 464 | loss: 0.64140 - acc: 0.7562 -- iter: 08/22\n",
            "Training Step: 1391  | total loss: \u001b[1m\u001b[32m0.64140\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 464 | loss: 0.64140 - acc: 0.7431 -- iter: 16/22\n",
            "Training Step: 1392  | total loss: \u001b[1m\u001b[32m0.63432\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 464 | loss: 0.63432 - acc: 0.7521 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1393  | total loss: \u001b[1m\u001b[32m0.62658\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 465 | loss: 0.62658 - acc: 0.7602 -- iter: 08/22\n",
            "Training Step: 1394  | total loss: \u001b[1m\u001b[32m0.62796\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 465 | loss: 0.62796 - acc: 0.7592 -- iter: 16/22\n",
            "Training Step: 1395  | total loss: \u001b[1m\u001b[32m0.62239\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 465 | loss: 0.62239 - acc: 0.7458 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1396  | total loss: \u001b[1m\u001b[32m0.61554\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 466 | loss: 0.61554 - acc: 0.7545 -- iter: 08/22\n",
            "Training Step: 1397  | total loss: \u001b[1m\u001b[32m0.61045\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 466 | loss: 0.61045 - acc: 0.7291 -- iter: 16/22\n",
            "Training Step: 1398  | total loss: \u001b[1m\u001b[32m0.59186\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 466 | loss: 0.59186 - acc: 0.7437 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1399  | total loss: \u001b[1m\u001b[32m0.61034\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 467 | loss: 0.61034 - acc: 0.7443 -- iter: 08/22\n",
            "Training Step: 1400  | total loss: \u001b[1m\u001b[32m0.60565\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 467 | loss: 0.60565 - acc: 0.7199 -- iter: 16/22\n",
            "Training Step: 1401  | total loss: \u001b[1m\u001b[32m0.58341\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 467 | loss: 0.58341 - acc: 0.7312 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1402  | total loss: \u001b[1m\u001b[32m0.59230\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 468 | loss: 0.59230 - acc: 0.7331 -- iter: 08/22\n",
            "Training Step: 1403  | total loss: \u001b[1m\u001b[32m0.59909\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 468 | loss: 0.59909 - acc: 0.7348 -- iter: 16/22\n",
            "Training Step: 1404  | total loss: \u001b[1m\u001b[32m0.57740\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 468 | loss: 0.57740 - acc: 0.7446 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1405  | total loss: \u001b[1m\u001b[32m0.54145\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 469 | loss: 0.54145 - acc: 0.7702 -- iter: 08/22\n",
            "Training Step: 1406  | total loss: \u001b[1m\u001b[32m0.56324\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 469 | loss: 0.56324 - acc: 0.7557 -- iter: 16/22\n",
            "Training Step: 1407  | total loss: \u001b[1m\u001b[32m0.57622\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 469 | loss: 0.57622 - acc: 0.7551 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1408  | total loss: \u001b[1m\u001b[32m0.54031\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 470 | loss: 0.54031 - acc: 0.7796 -- iter: 08/22\n",
            "Training Step: 1409  | total loss: \u001b[1m\u001b[32m0.53823\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 470 | loss: 0.53823 - acc: 0.7683 -- iter: 16/22\n",
            "Training Step: 1410  | total loss: \u001b[1m\u001b[32m0.54291\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 470 | loss: 0.54291 - acc: 0.7665 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1411  | total loss: \u001b[1m\u001b[32m0.55231\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 471 | loss: 0.55231 - acc: 0.7773 -- iter: 08/22\n",
            "Training Step: 1412  | total loss: \u001b[1m\u001b[32m0.54888\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 471 | loss: 0.54888 - acc: 0.7829 -- iter: 16/22\n",
            "Training Step: 1413  | total loss: \u001b[1m\u001b[32m0.55646\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 471 | loss: 0.55646 - acc: 0.7880 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1414  | total loss: \u001b[1m\u001b[32m0.55447\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 472 | loss: 0.55447 - acc: 0.7967 -- iter: 08/22\n",
            "Training Step: 1415  | total loss: \u001b[1m\u001b[32m0.55936\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 472 | loss: 0.55936 - acc: 0.7795 -- iter: 16/22\n",
            "Training Step: 1416  | total loss: \u001b[1m\u001b[32m0.56551\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 472 | loss: 0.56551 - acc: 0.8015 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1417  | total loss: \u001b[1m\u001b[32m0.53171\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 473 | loss: 0.53171 - acc: 0.8047 -- iter: 08/22\n",
            "Training Step: 1418  | total loss: \u001b[1m\u001b[32m0.55583\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 473 | loss: 0.55583 - acc: 0.7993 -- iter: 16/22\n",
            "Training Step: 1419  | total loss: \u001b[1m\u001b[32m0.56618\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 473 | loss: 0.56618 - acc: 0.7818 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1420  | total loss: \u001b[1m\u001b[32m0.53226\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 474 | loss: 0.53226 - acc: 0.7870 -- iter: 08/22\n",
            "Training Step: 1421  | total loss: \u001b[1m\u001b[32m0.50027\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 474 | loss: 0.50027 - acc: 0.7916 -- iter: 16/22\n",
            "Training Step: 1422  | total loss: \u001b[1m\u001b[32m0.51982\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 474 | loss: 0.51982 - acc: 0.8000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1423  | total loss: \u001b[1m\u001b[32m0.54217\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 475 | loss: 0.54217 - acc: 0.7700 -- iter: 08/22\n",
            "Training Step: 1424  | total loss: \u001b[1m\u001b[32m0.50915\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 475 | loss: 0.50915 - acc: 0.7763 -- iter: 16/22\n",
            "Training Step: 1425  | total loss: \u001b[1m\u001b[32m0.52990\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 475 | loss: 0.52990 - acc: 0.7487 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1426  | total loss: \u001b[1m\u001b[32m0.53466\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 476 | loss: 0.53466 - acc: 0.7488 -- iter: 08/22\n",
            "Training Step: 1427  | total loss: \u001b[1m\u001b[32m0.52910\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 476 | loss: 0.52910 - acc: 0.7614 -- iter: 16/22\n",
            "Training Step: 1428  | total loss: \u001b[1m\u001b[32m0.54771\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 476 | loss: 0.54771 - acc: 0.7353 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1429  | total loss: \u001b[1m\u001b[32m0.51318\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 477 | loss: 0.51318 - acc: 0.7617 -- iter: 08/22\n",
            "Training Step: 1430  | total loss: \u001b[1m\u001b[32m0.53125\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 477 | loss: 0.53125 - acc: 0.7356 -- iter: 16/22\n",
            "Training Step: 1431  | total loss: \u001b[1m\u001b[32m0.55256\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 477 | loss: 0.55256 - acc: 0.7370 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1432  | total loss: \u001b[1m\u001b[32m0.51744\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 478 | loss: 0.51744 - acc: 0.7633 -- iter: 08/22\n",
            "Training Step: 1433  | total loss: \u001b[1m\u001b[32m0.53249\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 478 | loss: 0.53249 - acc: 0.7537 -- iter: 16/22\n",
            "Training Step: 1434  | total loss: \u001b[1m\u001b[32m0.52287\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 478 | loss: 0.52287 - acc: 0.7533 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1435  | total loss: \u001b[1m\u001b[32m0.53546\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 479 | loss: 0.53546 - acc: 0.7530 -- iter: 08/22\n",
            "Training Step: 1436  | total loss: \u001b[1m\u001b[32m0.54851\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 479 | loss: 0.54851 - acc: 0.7610 -- iter: 16/22\n",
            "Training Step: 1437  | total loss: \u001b[1m\u001b[32m0.55499\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 479 | loss: 0.55499 - acc: 0.7682 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1438  | total loss: \u001b[1m\u001b[32m0.54686\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 480 | loss: 0.54686 - acc: 0.7539 -- iter: 08/22\n",
            "Training Step: 1439  | total loss: \u001b[1m\u001b[32m0.55697\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 480 | loss: 0.55697 - acc: 0.7785 -- iter: 16/22\n",
            "Training Step: 1440  | total loss: \u001b[1m\u001b[32m0.56237\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 480 | loss: 0.56237 - acc: 0.8007 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1441  | total loss: \u001b[1m\u001b[32m0.55755\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 481 | loss: 0.55755 - acc: 0.8039 -- iter: 08/22\n",
            "Training Step: 1442  | total loss: \u001b[1m\u001b[32m0.56663\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 481 | loss: 0.56663 - acc: 0.7985 -- iter: 16/22\n",
            "Training Step: 1443  | total loss: \u001b[1m\u001b[32m0.56432\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 481 | loss: 0.56432 - acc: 0.7937 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1444  | total loss: \u001b[1m\u001b[32m0.55899\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 482 | loss: 0.55899 - acc: 0.7976 -- iter: 08/22\n",
            "Training Step: 1445  | total loss: \u001b[1m\u001b[32m0.55768\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 482 | loss: 0.55768 - acc: 0.7846 -- iter: 16/22\n",
            "Training Step: 1446  | total loss: \u001b[1m\u001b[32m0.55663\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 482 | loss: 0.55663 - acc: 0.7936 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1447  | total loss: \u001b[1m\u001b[32m0.56251\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 483 | loss: 0.56251 - acc: 0.8017 -- iter: 08/22\n",
            "Training Step: 1448  | total loss: \u001b[1m\u001b[32m0.56073\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 483 | loss: 0.56073 - acc: 0.7882 -- iter: 16/22\n",
            "Training Step: 1449  | total loss: \u001b[1m\u001b[32m0.56969\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 483 | loss: 0.56969 - acc: 0.7927 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1450  | total loss: \u001b[1m\u001b[32m0.56280\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 484 | loss: 0.56280 - acc: 0.7885 -- iter: 08/22\n",
            "Training Step: 1451  | total loss: \u001b[1m\u001b[32m0.56445\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 484 | loss: 0.56445 - acc: 0.7846 -- iter: 16/22\n",
            "Training Step: 1452  | total loss: \u001b[1m\u001b[32m0.57277\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 484 | loss: 0.57277 - acc: 0.7895 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1453  | total loss: \u001b[1m\u001b[32m0.56659\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 485 | loss: 0.56659 - acc: 0.7939 -- iter: 08/22\n",
            "Training Step: 1454  | total loss: \u001b[1m\u001b[32m0.55983\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 485 | loss: 0.55983 - acc: 0.8020 -- iter: 16/22\n",
            "Training Step: 1455  | total loss: \u001b[1m\u001b[32m0.57195\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 485 | loss: 0.57195 - acc: 0.7843 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1456  | total loss: \u001b[1m\u001b[32m0.56553\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 486 | loss: 0.56553 - acc: 0.7892 -- iter: 08/22\n",
            "Training Step: 1457  | total loss: \u001b[1m\u001b[32m0.57677\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 486 | loss: 0.57677 - acc: 0.7603 -- iter: 16/22\n",
            "Training Step: 1458  | total loss: \u001b[1m\u001b[32m0.58124\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 486 | loss: 0.58124 - acc: 0.7717 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1459  | total loss: \u001b[1m\u001b[32m0.56590\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 487 | loss: 0.56590 - acc: 0.7696 -- iter: 08/22\n",
            "Training Step: 1460  | total loss: \u001b[1m\u001b[32m0.57691\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 487 | loss: 0.57691 - acc: 0.7426 -- iter: 16/22\n",
            "Training Step: 1461  | total loss: \u001b[1m\u001b[32m0.59734\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 487 | loss: 0.59734 - acc: 0.7350 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1462  | total loss: \u001b[1m\u001b[32m0.57982\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 488 | loss: 0.57982 - acc: 0.7365 -- iter: 08/22\n",
            "Training Step: 1463  | total loss: \u001b[1m\u001b[32m0.57632\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 488 | loss: 0.57632 - acc: 0.7504 -- iter: 16/22\n",
            "Training Step: 1464  | total loss: \u001b[1m\u001b[32m0.59653\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 488 | loss: 0.59653 - acc: 0.7420 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1465  | total loss: \u001b[1m\u001b[32m0.58485\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 489 | loss: 0.58485 - acc: 0.7678 -- iter: 08/22\n",
            "Training Step: 1466  | total loss: \u001b[1m\u001b[32m0.58005\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 489 | loss: 0.58005 - acc: 0.7785 -- iter: 16/22\n",
            "Training Step: 1467  | total loss: \u001b[1m\u001b[32m0.58716\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 489 | loss: 0.58716 - acc: 0.7507 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1468  | total loss: \u001b[1m\u001b[32m0.57619\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 490 | loss: 0.57619 - acc: 0.7756 -- iter: 08/22\n",
            "Training Step: 1469  | total loss: \u001b[1m\u001b[32m0.57376\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 490 | loss: 0.57376 - acc: 0.7647 -- iter: 16/22\n",
            "Training Step: 1470  | total loss: \u001b[1m\u001b[32m0.58229\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 490 | loss: 0.58229 - acc: 0.7632 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1471  | total loss: \u001b[1m\u001b[32m0.57097\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 491 | loss: 0.57097 - acc: 0.7744 -- iter: 08/22\n",
            "Training Step: 1472  | total loss: \u001b[1m\u001b[32m0.56901\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 491 | loss: 0.56901 - acc: 0.7636 -- iter: 16/22\n",
            "Training Step: 1473  | total loss: \u001b[1m\u001b[32m0.56437\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 491 | loss: 0.56437 - acc: 0.7873 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1474  | total loss: \u001b[1m\u001b[32m0.58091\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 492 | loss: 0.58091 - acc: 0.7835 -- iter: 08/22\n",
            "Training Step: 1475  | total loss: \u001b[1m\u001b[32m0.56443\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 492 | loss: 0.56443 - acc: 0.7802 -- iter: 16/22\n",
            "Training Step: 1476  | total loss: \u001b[1m\u001b[32m0.56010\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 492 | loss: 0.56010 - acc: 0.8022 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1477  | total loss: \u001b[1m\u001b[32m0.56366\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 493 | loss: 0.56366 - acc: 0.8220 -- iter: 08/22\n",
            "Training Step: 1478  | total loss: \u001b[1m\u001b[32m0.52811\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 493 | loss: 0.52811 - acc: 0.8273 -- iter: 16/22\n",
            "Training Step: 1479  | total loss: \u001b[1m\u001b[32m0.56314\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 493 | loss: 0.56314 - acc: 0.7945 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1480  | total loss: \u001b[1m\u001b[32m0.56617\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 494 | loss: 0.56617 - acc: 0.8151 -- iter: 08/22\n",
            "Training Step: 1481  | total loss: \u001b[1m\u001b[32m0.59725\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 494 | loss: 0.59725 - acc: 0.7669 -- iter: 16/22\n",
            "Training Step: 1482  | total loss: \u001b[1m\u001b[32m0.59867\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 494 | loss: 0.59867 - acc: 0.7777 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1483  | total loss: \u001b[1m\u001b[32m0.56478\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 495 | loss: 0.56478 - acc: 0.7999 -- iter: 08/22\n",
            "Training Step: 1484  | total loss: \u001b[1m\u001b[32m0.59576\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 495 | loss: 0.59576 - acc: 0.7533 -- iter: 16/22\n",
            "Training Step: 1485  | total loss: \u001b[1m\u001b[32m0.58378\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 495 | loss: 0.58378 - acc: 0.7613 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1486  | total loss: \u001b[1m\u001b[32m0.56689\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 496 | loss: 0.56689 - acc: 0.7727 -- iter: 08/22\n",
            "Training Step: 1487  | total loss: \u001b[1m\u001b[32m0.58538\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 496 | loss: 0.58538 - acc: 0.7579 -- iter: 16/22\n",
            "Training Step: 1488  | total loss: \u001b[1m\u001b[32m0.57418\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 496 | loss: 0.57418 - acc: 0.7654 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1489  | total loss: \u001b[1m\u001b[32m0.59483\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 497 | loss: 0.59483 - acc: 0.7556 -- iter: 08/22\n",
            "Training Step: 1490  | total loss: \u001b[1m\u001b[32m0.61393\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 497 | loss: 0.61393 - acc: 0.7425 -- iter: 16/22\n",
            "Training Step: 1491  | total loss: \u001b[1m\u001b[32m0.56734\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 497 | loss: 0.56734 - acc: 0.7683 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1492  | total loss: \u001b[1m\u001b[32m0.58847\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 498 | loss: 0.58847 - acc: 0.7414 -- iter: 08/22\n",
            "Training Step: 1493  | total loss: \u001b[1m\u001b[32m0.59607\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 498 | loss: 0.59607 - acc: 0.7506 -- iter: 16/22\n",
            "Training Step: 1494  | total loss: \u001b[1m\u001b[32m0.57603\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 498 | loss: 0.57603 - acc: 0.7506 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1495  | total loss: \u001b[1m\u001b[32m0.58035\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 499 | loss: 0.58035 - acc: 0.7380 -- iter: 08/22\n",
            "Training Step: 1496  | total loss: \u001b[1m\u001b[32m0.58855\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 499 | loss: 0.58855 - acc: 0.7475 -- iter: 16/22\n",
            "Training Step: 1497  | total loss: \u001b[1m\u001b[32m0.56508\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 499 | loss: 0.56508 - acc: 0.7561 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1498  | total loss: \u001b[1m\u001b[32m0.57619\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 500 | loss: 0.57619 - acc: 0.7555 -- iter: 08/22\n",
            "Training Step: 1499  | total loss: \u001b[1m\u001b[32m0.57518\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 500 | loss: 0.57518 - acc: 0.7550 -- iter: 16/22\n",
            "Training Step: 1500  | total loss: \u001b[1m\u001b[32m0.55294\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 500 | loss: 0.55294 - acc: 0.7628 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1501  | total loss: \u001b[1m\u001b[32m0.57852\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 501 | loss: 0.57852 - acc: 0.7532 -- iter: 08/22\n",
            "Training Step: 1502  | total loss: \u001b[1m\u001b[32m0.57771\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 501 | loss: 0.57771 - acc: 0.7529 -- iter: 16/22\n",
            "Training Step: 1503  | total loss: \u001b[1m\u001b[32m0.55263\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 501 | loss: 0.55263 - acc: 0.7651 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1504  | total loss: \u001b[1m\u001b[32m0.57797\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 502 | loss: 0.57797 - acc: 0.7552 -- iter: 08/22\n",
            "Training Step: 1505  | total loss: \u001b[1m\u001b[32m0.58584\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 502 | loss: 0.58584 - acc: 0.7630 -- iter: 16/22\n",
            "Training Step: 1506  | total loss: \u001b[1m\u001b[32m0.59220\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 502 | loss: 0.59220 - acc: 0.7367 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1507  | total loss: \u001b[1m\u001b[32m0.56870\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 503 | loss: 0.56870 - acc: 0.7631 -- iter: 08/22\n",
            "Training Step: 1508  | total loss: \u001b[1m\u001b[32m0.57715\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 503 | loss: 0.57715 - acc: 0.7701 -- iter: 16/22\n",
            "Training Step: 1509  | total loss: \u001b[1m\u001b[32m0.57331\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 503 | loss: 0.57331 - acc: 0.7597 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1510  | total loss: \u001b[1m\u001b[32m0.58853\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 504 | loss: 0.58853 - acc: 0.7588 -- iter: 08/22\n",
            "Training Step: 1511  | total loss: \u001b[1m\u001b[32m0.56604\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 504 | loss: 0.56604 - acc: 0.7704 -- iter: 16/22\n",
            "Training Step: 1512  | total loss: \u001b[1m\u001b[32m0.56315\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 504 | loss: 0.56315 - acc: 0.7600 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1513  | total loss: \u001b[1m\u001b[32m0.60864\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 505 | loss: 0.60864 - acc: 0.7174 -- iter: 08/22\n",
            "Training Step: 1514  | total loss: \u001b[1m\u001b[32m0.58624\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 505 | loss: 0.58624 - acc: 0.7331 -- iter: 16/22\n",
            "Training Step: 1515  | total loss: \u001b[1m\u001b[32m0.56178\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 505 | loss: 0.56178 - acc: 0.7598 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1516  | total loss: \u001b[1m\u001b[32m0.60698\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 506 | loss: 0.60698 - acc: 0.7338 -- iter: 08/22\n",
            "Training Step: 1517  | total loss: \u001b[1m\u001b[32m0.59712\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 506 | loss: 0.59712 - acc: 0.7604 -- iter: 16/22\n",
            "Training Step: 1518  | total loss: \u001b[1m\u001b[32m0.59350\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 506 | loss: 0.59350 - acc: 0.7594 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1519  | total loss: \u001b[1m\u001b[32m0.58822\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 507 | loss: 0.58822 - acc: 0.7710 -- iter: 08/22\n",
            "Training Step: 1520  | total loss: \u001b[1m\u001b[32m0.58011\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 507 | loss: 0.58011 - acc: 0.7939 -- iter: 16/22\n",
            "Training Step: 1521  | total loss: \u001b[1m\u001b[32m0.59210\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 507 | loss: 0.59210 - acc: 0.7811 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1522  | total loss: \u001b[1m\u001b[32m0.57416\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 508 | loss: 0.57416 - acc: 0.7905 -- iter: 08/22\n",
            "Training Step: 1523  | total loss: \u001b[1m\u001b[32m0.57097\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 508 | loss: 0.57097 - acc: 0.8115 -- iter: 16/22\n",
            "Training Step: 1524  | total loss: \u001b[1m\u001b[32m1.14159\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 508 | loss: 1.14159 - acc: 0.7470 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1525  | total loss: \u001b[1m\u001b[32m1.07833\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 509 | loss: 1.07833 - acc: 0.7723 -- iter: 08/22\n",
            "Training Step: 1526  | total loss: \u001b[1m\u001b[32m1.02312\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 509 | loss: 1.02312 - acc: 0.7701 -- iter: 16/22\n",
            "Training Step: 1527  | total loss: \u001b[1m\u001b[32m0.97788\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 509 | loss: 0.97788 - acc: 0.7806 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1528  | total loss: \u001b[1m\u001b[32m0.93080\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 510 | loss: 0.93080 - acc: 0.8025 -- iter: 08/22\n",
            "Training Step: 1529  | total loss: \u001b[1m\u001b[32m0.86978\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 510 | loss: 0.86978 - acc: 0.8223 -- iter: 16/22\n",
            "Training Step: 1530  | total loss: \u001b[1m\u001b[32m0.85954\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 510 | loss: 0.85954 - acc: 0.8025 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1531  | total loss: \u001b[1m\u001b[32m0.82072\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 511 | loss: 0.82072 - acc: 0.8223 -- iter: 08/22\n",
            "Training Step: 1532  | total loss: \u001b[1m\u001b[32m0.77093\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 511 | loss: 0.77093 - acc: 0.8400 -- iter: 16/22\n",
            "Training Step: 1533  | total loss: \u001b[1m\u001b[32m0.78162\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 511 | loss: 0.78162 - acc: 0.8060 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1534  | total loss: \u001b[1m\u001b[32m0.74323\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 512 | loss: 0.74323 - acc: 0.8254 -- iter: 08/22\n",
            "Training Step: 1535  | total loss: \u001b[1m\u001b[32m0.71145\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 512 | loss: 0.71145 - acc: 0.8304 -- iter: 16/22\n",
            "Training Step: 1536  | total loss: \u001b[1m\u001b[32m0.72787\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 512 | loss: 0.72787 - acc: 0.7974 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1537  | total loss: \u001b[1m\u001b[32m0.71508\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 513 | loss: 0.71508 - acc: 0.8010 -- iter: 08/22\n",
            "Training Step: 1538  | total loss: \u001b[1m\u001b[32m0.71563\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 513 | loss: 0.71563 - acc: 0.8084 -- iter: 16/22\n",
            "Training Step: 1539  | total loss: \u001b[1m\u001b[32m0.67492\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 513 | loss: 0.67492 - acc: 0.8025 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.66726\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 514 | loss: 0.66726 - acc: 0.8056 -- iter: 08/22\n",
            "Training Step: 1541  | total loss: \u001b[1m\u001b[32m0.67898\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 514 | loss: 0.67898 - acc: 0.8084 -- iter: 16/22\n",
            "Training Step: 1542  | total loss: \u001b[1m\u001b[32m0.65093\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 514 | loss: 0.65093 - acc: 0.8025 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1543  | total loss: \u001b[1m\u001b[32m0.63464\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 515 | loss: 0.63464 - acc: 0.8098 -- iter: 08/22\n",
            "Training Step: 1544  | total loss: \u001b[1m\u001b[32m0.64954\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 515 | loss: 0.64954 - acc: 0.8121 -- iter: 16/22\n",
            "Training Step: 1545  | total loss: \u001b[1m\u001b[32m0.66333\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 515 | loss: 0.66333 - acc: 0.7976 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1546  | total loss: \u001b[1m\u001b[32m0.64894\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 516 | loss: 0.64894 - acc: 0.8053 -- iter: 08/22\n",
            "Training Step: 1547  | total loss: \u001b[1m\u001b[32m0.62009\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 516 | loss: 0.62009 - acc: 0.8248 -- iter: 16/22\n",
            "Training Step: 1548  | total loss: \u001b[1m\u001b[32m0.63666\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 516 | loss: 0.63666 - acc: 0.8090 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1549  | total loss: \u001b[1m\u001b[32m0.62151\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 517 | loss: 0.62151 - acc: 0.8114 -- iter: 08/22\n",
            "Training Step: 1550  | total loss: \u001b[1m\u001b[32m0.62126\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 517 | loss: 0.62126 - acc: 0.8053 -- iter: 16/22\n",
            "Training Step: 1551  | total loss: \u001b[1m\u001b[32m0.60732\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 517 | loss: 0.60732 - acc: 0.8248 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1552  | total loss: \u001b[1m\u001b[32m0.59489\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 518 | loss: 0.59489 - acc: 0.8256 -- iter: 08/22\n",
            "Training Step: 1553  | total loss: \u001b[1m\u001b[32m0.58545\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 518 | loss: 0.58545 - acc: 0.8097 -- iter: 16/22\n",
            "Training Step: 1554  | total loss: \u001b[1m\u001b[32m0.57380\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 518 | loss: 0.57380 - acc: 0.8287 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1555  | total loss: \u001b[1m\u001b[32m0.57797\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 519 | loss: 0.57797 - acc: 0.8334 -- iter: 08/22\n",
            "Training Step: 1556  | total loss: \u001b[1m\u001b[32m0.56982\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 519 | loss: 0.56982 - acc: 0.8167 -- iter: 16/22\n",
            "Training Step: 1557  | total loss: \u001b[1m\u001b[32m0.58769\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 519 | loss: 0.58769 - acc: 0.8350 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1558  | total loss: \u001b[1m\u001b[32m0.57767\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 520 | loss: 0.57767 - acc: 0.8390 -- iter: 08/22\n",
            "Training Step: 1559  | total loss: \u001b[1m\u001b[32m0.56026\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 520 | loss: 0.56026 - acc: 0.8301 -- iter: 16/22\n",
            "Training Step: 1560  | total loss: \u001b[1m\u001b[32m0.57898\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 520 | loss: 0.57898 - acc: 0.8471 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1561  | total loss: \u001b[1m\u001b[32m0.55471\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 521 | loss: 0.55471 - acc: 0.8624 -- iter: 08/22\n",
            "Training Step: 1562  | total loss: \u001b[1m\u001b[32m0.54783\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 521 | loss: 0.54783 - acc: 0.8512 -- iter: 16/22\n",
            "Training Step: 1563  | total loss: \u001b[1m\u001b[32m0.56392\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 521 | loss: 0.56392 - acc: 0.8535 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1564  | total loss: \u001b[1m\u001b[32m1.52272\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 522 | loss: 1.52272 - acc: 0.7849 -- iter: 08/22\n",
            "Training Step: 1565  | total loss: \u001b[1m\u001b[32m1.42677\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 522 | loss: 1.42677 - acc: 0.7897 -- iter: 16/22\n",
            "Training Step: 1566  | total loss: \u001b[1m\u001b[32m1.32774\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 522 | loss: 1.32774 - acc: 0.7857 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1567  | total loss: \u001b[1m\u001b[32m1.25375\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 523 | loss: 1.25375 - acc: 0.8072 -- iter: 08/22\n",
            "Training Step: 1568  | total loss: \u001b[1m\u001b[32m1.18445\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 523 | loss: 1.18445 - acc: 0.8098 -- iter: 16/22\n",
            "Training Step: 1569  | total loss: \u001b[1m\u001b[32m1.10221\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 523 | loss: 1.10221 - acc: 0.8288 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1570  | total loss: \u001b[1m\u001b[32m1.05002\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 524 | loss: 1.05002 - acc: 0.8084 -- iter: 08/22\n",
            "Training Step: 1571  | total loss: \u001b[1m\u001b[32m1.00447\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 524 | loss: 1.00447 - acc: 0.8276 -- iter: 16/22\n",
            "Training Step: 1572  | total loss: \u001b[1m\u001b[32m0.94036\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 524 | loss: 0.94036 - acc: 0.8282 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1573  | total loss: \u001b[1m\u001b[32m0.88814\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 525 | loss: 0.88814 - acc: 0.8120 -- iter: 08/22\n",
            "Training Step: 1574  | total loss: \u001b[1m\u001b[32m0.84449\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 525 | loss: 0.84449 - acc: 0.8308 -- iter: 16/22\n",
            "Training Step: 1575  | total loss: \u001b[1m\u001b[32m0.82804\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 525 | loss: 0.82804 - acc: 0.8227 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1576  | total loss: \u001b[1m\u001b[32m0.78697\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 526 | loss: 0.78697 - acc: 0.8071 -- iter: 08/22\n",
            "Training Step: 1577  | total loss: \u001b[1m\u001b[32m0.72941\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 526 | loss: 0.72941 - acc: 0.8264 -- iter: 16/22\n",
            "Training Step: 1578  | total loss: \u001b[1m\u001b[32m0.71654\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 526 | loss: 0.71654 - acc: 0.8313 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1579  | total loss: \u001b[1m\u001b[32m0.71318\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 527 | loss: 0.71318 - acc: 0.8106 -- iter: 08/22\n",
            "Training Step: 1580  | total loss: \u001b[1m\u001b[32m0.66282\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 527 | loss: 0.66282 - acc: 0.8296 -- iter: 16/22\n",
            "Training Step: 1581  | total loss: \u001b[1m\u001b[32m0.65739\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 527 | loss: 0.65739 - acc: 0.8300 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1582  | total loss: \u001b[1m\u001b[32m0.64040\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 528 | loss: 0.64040 - acc: 0.8345 -- iter: 08/22\n",
            "Training Step: 1583  | total loss: \u001b[1m\u001b[32m0.62563\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 528 | loss: 0.62563 - acc: 0.8260 -- iter: 16/22\n",
            "Training Step: 1584  | total loss: \u001b[1m\u001b[32m0.62383\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 528 | loss: 0.62383 - acc: 0.8267 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1585  | total loss: \u001b[1m\u001b[32m0.63012\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 529 | loss: 0.63012 - acc: 0.8107 -- iter: 08/22\n",
            "Training Step: 1586  | total loss: \u001b[1m\u001b[32m0.63189\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 529 | loss: 0.63189 - acc: 0.8172 -- iter: 16/22\n",
            "Training Step: 1587  | total loss: \u001b[1m\u001b[32m0.59565\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 529 | loss: 0.59565 - acc: 0.8354 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1588  | total loss: \u001b[1m\u001b[32m0.60455\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 530 | loss: 0.60455 - acc: 0.8186 -- iter: 08/22\n",
            "Training Step: 1589  | total loss: \u001b[1m\u001b[32m0.60114\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 530 | loss: 0.60114 - acc: 0.8200 -- iter: 16/22\n",
            "Training Step: 1590  | total loss: \u001b[1m\u001b[32m0.59544\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 530 | loss: 0.59544 - acc: 0.8255 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1591  | total loss: \u001b[1m\u001b[32m0.58143\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 531 | loss: 0.58143 - acc: 0.8305 -- iter: 08/22\n",
            "Training Step: 1592  | total loss: \u001b[1m\u001b[32m0.84576\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 531 | loss: 0.84576 - acc: 0.7641 -- iter: 16/22\n",
            "Training Step: 1593  | total loss: \u001b[1m\u001b[32m0.81429\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 531 | loss: 0.81429 - acc: 0.7710 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1594  | total loss: \u001b[1m\u001b[32m0.76509\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 532 | loss: 0.76509 - acc: 0.7939 -- iter: 08/22\n",
            "Training Step: 1595  | total loss: \u001b[1m\u001b[32m0.75912\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 532 | loss: 0.75912 - acc: 0.8020 -- iter: 16/22\n",
            "Training Step: 1596  | total loss: \u001b[1m\u001b[32m0.73603\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 532 | loss: 0.73603 - acc: 0.8218 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1597  | total loss: \u001b[1m\u001b[32m0.69827\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 533 | loss: 0.69827 - acc: 0.8396 -- iter: 08/22\n",
            "Training Step: 1598  | total loss: \u001b[1m\u001b[32m0.68200\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 533 | loss: 0.68200 - acc: 0.8432 -- iter: 16/22\n",
            "Training Step: 1599  | total loss: \u001b[1m\u001b[32m0.67595\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 533 | loss: 0.67595 - acc: 0.8339 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1600  | total loss: \u001b[1m\u001b[32m0.64404\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 534 | loss: 0.64404 - acc: 0.8505 -- iter: 08/22\n",
            "Training Step: 1601  | total loss: \u001b[1m\u001b[32m0.61532\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 534 | loss: 0.61532 - acc: 0.8488 -- iter: 16/22\n",
            "Training Step: 1602  | total loss: \u001b[1m\u001b[32m0.61978\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 534 | loss: 0.61978 - acc: 0.8514 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1603  | total loss: \u001b[1m\u001b[32m0.60736\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 535 | loss: 0.60736 - acc: 0.8537 -- iter: 08/22\n",
            "Training Step: 1604  | total loss: \u001b[1m\u001b[32m0.58218\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 535 | loss: 0.58218 - acc: 0.8517 -- iter: 16/22\n",
            "Training Step: 1605  | total loss: \u001b[1m\u001b[32m0.58052\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 535 | loss: 0.58052 - acc: 0.8499 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1606  | total loss: \u001b[1m\u001b[32m0.58603\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 536 | loss: 0.58603 - acc: 0.8274 -- iter: 08/22\n",
            "Training Step: 1607  | total loss: \u001b[1m\u001b[32m0.56344\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 536 | loss: 0.56344 - acc: 0.8446 -- iter: 16/22\n",
            "Training Step: 1608  | total loss: \u001b[1m\u001b[32m0.56359\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 536 | loss: 0.56359 - acc: 0.8435 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1609  | total loss: \u001b[1m\u001b[32m0.58420\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 537 | loss: 0.58420 - acc: 0.8425 -- iter: 08/22\n",
            "Training Step: 1610  | total loss: \u001b[1m\u001b[32m0.58115\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 537 | loss: 0.58115 - acc: 0.8457 -- iter: 16/22\n",
            "Training Step: 1611  | total loss: \u001b[1m\u001b[32m0.55152\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 537 | loss: 0.55152 - acc: 0.8487 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1612  | total loss: \u001b[1m\u001b[32m0.57317\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 538 | loss: 0.57317 - acc: 0.8471 -- iter: 08/22\n",
            "Training Step: 1613  | total loss: \u001b[1m\u001b[32m0.59260\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 538 | loss: 0.59260 - acc: 0.8458 -- iter: 16/22\n",
            "Training Step: 1614  | total loss: \u001b[1m\u001b[32m0.57223\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 538 | loss: 0.57223 - acc: 0.8487 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1615  | total loss: \u001b[1m\u001b[32m0.55964\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 539 | loss: 0.55964 - acc: 0.8513 -- iter: 08/22\n",
            "Training Step: 1616  | total loss: \u001b[1m\u001b[32m0.58015\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 539 | loss: 0.58015 - acc: 0.8495 -- iter: 16/22\n",
            "Training Step: 1617  | total loss: \u001b[1m\u001b[32m0.56861\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 539 | loss: 0.56861 - acc: 0.8479 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1618  | total loss: \u001b[1m\u001b[32m0.55402\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 540 | loss: 0.55402 - acc: 0.8506 -- iter: 08/22\n",
            "Training Step: 1619  | total loss: \u001b[1m\u001b[32m0.56201\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 540 | loss: 0.56201 - acc: 0.8530 -- iter: 16/22\n",
            "Training Step: 1620  | total loss: \u001b[1m\u001b[32m0.79714\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 540 | loss: 0.79714 - acc: 0.8011 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1621  | total loss: \u001b[1m\u001b[32m0.77919\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 541 | loss: 0.77919 - acc: 0.8210 -- iter: 08/22\n",
            "Training Step: 1622  | total loss: \u001b[1m\u001b[32m0.76700\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 541 | loss: 0.76700 - acc: 0.8264 -- iter: 16/22\n",
            "Training Step: 1623  | total loss: \u001b[1m\u001b[32m0.71849\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 541 | loss: 0.71849 - acc: 0.8312 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1624  | total loss: \u001b[1m\u001b[32m0.99017\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 542 | loss: 0.99017 - acc: 0.7481 -- iter: 08/22\n",
            "Training Step: 1625  | total loss: \u001b[1m\u001b[32m0.95281\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 542 | loss: 0.95281 - acc: 0.7733 -- iter: 16/22\n",
            "Training Step: 1626  | total loss: \u001b[1m\u001b[32m0.90654\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 542 | loss: 0.90654 - acc: 0.7835 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1627  | total loss: \u001b[1m\u001b[32m0.86109\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 543 | loss: 0.86109 - acc: 0.8051 -- iter: 08/22\n",
            "Training Step: 1628  | total loss: \u001b[1m\u001b[32m1.19221\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 543 | loss: 1.19221 - acc: 0.7821 -- iter: 16/22\n",
            "Training Step: 1629  | total loss: \u001b[1m\u001b[32m1.10921\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 543 | loss: 1.10921 - acc: 0.7821 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1630  | total loss: \u001b[1m\u001b[32m1.05869\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 544 | loss: 1.05869 - acc: 0.7914 -- iter: 08/22\n",
            "Training Step: 1631  | total loss: \u001b[1m\u001b[32m1.00603\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 544 | loss: 1.00603 - acc: 0.8123 -- iter: 16/22\n",
            "Training Step: 1632  | total loss: \u001b[1m\u001b[32m1.62185\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 544 | loss: 1.62185 - acc: 0.7311 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1633  | total loss: \u001b[1m\u001b[32m1.51588\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 545 | loss: 1.51588 - acc: 0.7580 -- iter: 08/22\n",
            "Training Step: 1634  | total loss: \u001b[1m\u001b[32m1.40440\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 545 | loss: 1.40440 - acc: 0.7697 -- iter: 16/22\n",
            "Training Step: 1635  | total loss: \u001b[1m\u001b[32m1.32350\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 545 | loss: 1.32350 - acc: 0.7677 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1636  | total loss: \u001b[1m\u001b[32m1.98023\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 546 | loss: 1.98023 - acc: 0.7243 -- iter: 08/22\n",
            "Training Step: 1637  | total loss: \u001b[1m\u001b[32m1.84179\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 546 | loss: 1.84179 - acc: 0.7518 -- iter: 16/22\n",
            "Training Step: 1638  | total loss: \u001b[1m\u001b[32m1.70639\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 546 | loss: 1.70639 - acc: 0.7391 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1639  | total loss: \u001b[1m\u001b[32m1.58491\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 547 | loss: 1.58491 - acc: 0.7402 -- iter: 08/22\n",
            "Training Step: 1640  | total loss: \u001b[1m\u001b[32m1.97253\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 547 | loss: 1.97253 - acc: 0.6662 -- iter: 16/22\n",
            "Training Step: 1641  | total loss: \u001b[1m\u001b[32m1.84198\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 547 | loss: 1.84198 - acc: 0.6663 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1642  | total loss: \u001b[1m\u001b[32m1.70239\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 548 | loss: 1.70239 - acc: 0.6871 -- iter: 08/22\n",
            "Training Step: 1643  | total loss: \u001b[1m\u001b[32m1.58193\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 548 | loss: 1.58193 - acc: 0.6934 -- iter: 16/22\n",
            "Training Step: 1644  | total loss: \u001b[1m\u001b[32m2.01587\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 548 | loss: 2.01587 - acc: 0.6407 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1645  | total loss: \u001b[1m\u001b[32m1.87455\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 549 | loss: 1.87455 - acc: 0.6433 -- iter: 08/22\n",
            "Training Step: 1646  | total loss: \u001b[1m\u001b[32m1.70944\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 549 | loss: 1.70944 - acc: 0.6790 -- iter: 16/22\n",
            "Training Step: 1647  | total loss: \u001b[1m\u001b[32m1.61683\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 549 | loss: 1.61683 - acc: 0.6736 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1648  | total loss: \u001b[1m\u001b[32m1.90353\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 550 | loss: 1.90353 - acc: 0.6229 -- iter: 08/22\n",
            "Training Step: 1649  | total loss: \u001b[1m\u001b[32m1.76708\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 550 | loss: 1.76708 - acc: 0.6606 -- iter: 16/22\n",
            "Training Step: 1650  | total loss: \u001b[1m\u001b[32m1.65045\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 550 | loss: 1.65045 - acc: 0.6571 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1651  | total loss: \u001b[1m\u001b[32m1.53197\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 551 | loss: 1.53197 - acc: 0.6663 -- iter: 08/22\n",
            "Training Step: 1652  | total loss: \u001b[1m\u001b[32m2.09457\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 551 | loss: 2.09457 - acc: 0.5997 -- iter: 16/22\n",
            "Training Step: 1653  | total loss: \u001b[1m\u001b[32m1.94837\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 551 | loss: 1.94837 - acc: 0.6064 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1654  | total loss: \u001b[1m\u001b[32m1.79877\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 552 | loss: 1.79877 - acc: 0.6083 -- iter: 08/22\n",
            "Training Step: 1655  | total loss: \u001b[1m\u001b[32m1.67504\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 552 | loss: 1.67504 - acc: 0.6349 -- iter: 16/22\n",
            "Training Step: 1656  | total loss: \u001b[1m\u001b[32m1.77821\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 552 | loss: 1.77821 - acc: 0.5881 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1657  | total loss: \u001b[1m\u001b[32m1.65628\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 553 | loss: 1.65628 - acc: 0.5793 -- iter: 08/22\n",
            "Training Step: 1658  | total loss: \u001b[1m\u001b[32m1.54217\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 553 | loss: 1.54217 - acc: 0.6089 -- iter: 16/22\n",
            "Training Step: 1659  | total loss: \u001b[1m\u001b[32m1.44471\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 553 | loss: 1.44471 - acc: 0.6480 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1660  | total loss: \u001b[1m\u001b[32m1.35607\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 554 | loss: 1.35607 - acc: 0.6332 -- iter: 08/22\n",
            "Training Step: 1661  | total loss: \u001b[1m\u001b[32m1.27469\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 554 | loss: 1.27469 - acc: 0.6532 -- iter: 16/22\n",
            "Training Step: 1662  | total loss: \u001b[1m\u001b[32m1.20479\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 554 | loss: 1.20479 - acc: 0.6754 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1663  | total loss: \u001b[1m\u001b[32m1.13648\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 555 | loss: 1.13648 - acc: 0.6828 -- iter: 08/22\n",
            "Training Step: 1664  | total loss: \u001b[1m\u001b[32m1.65353\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 555 | loss: 1.65353 - acc: 0.6479 -- iter: 16/22\n",
            "Training Step: 1665  | total loss: \u001b[1m\u001b[32m1.54548\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 555 | loss: 1.54548 - acc: 0.6664 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1666  | total loss: \u001b[1m\u001b[32m1.45737\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 556 | loss: 1.45737 - acc: 0.6623 -- iter: 08/22\n",
            "Training Step: 1667  | total loss: \u001b[1m\u001b[32m1.35276\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 556 | loss: 1.35276 - acc: 0.6836 -- iter: 16/22\n",
            "Training Step: 1668  | total loss: \u001b[1m\u001b[32m1.27458\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 556 | loss: 1.27458 - acc: 0.6985 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1669  | total loss: \u001b[1m\u001b[32m1.18666\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 557 | loss: 1.18666 - acc: 0.7287 -- iter: 08/22\n",
            "Training Step: 1670  | total loss: \u001b[1m\u001b[32m1.11996\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 557 | loss: 1.11996 - acc: 0.7183 -- iter: 16/22\n",
            "Training Step: 1671  | total loss: \u001b[1m\u001b[32m1.07671\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 557 | loss: 1.07671 - acc: 0.7215 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1672  | total loss: \u001b[1m\u001b[32m1.70098\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 558 | loss: 1.70098 - acc: 0.6660 -- iter: 08/22\n",
            "Training Step: 1673  | total loss: \u001b[1m\u001b[32m1.59814\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 558 | loss: 1.59814 - acc: 0.6827 -- iter: 16/22\n",
            "Training Step: 1674  | total loss: \u001b[1m\u001b[32m1.50375\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 558 | loss: 1.50375 - acc: 0.6770 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1675  | total loss: \u001b[1m\u001b[32m1.38759\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 559 | loss: 1.38759 - acc: 0.6968 -- iter: 08/22\n",
            "Training Step: 1676  | total loss: \u001b[1m\u001b[32m1.31622\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 559 | loss: 1.31622 - acc: 0.7104 -- iter: 16/22\n",
            "Training Step: 1677  | total loss: \u001b[1m\u001b[32m1.21335\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 559 | loss: 1.21335 - acc: 0.7394 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1678  | total loss: \u001b[1m\u001b[32m1.13350\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 560 | loss: 1.13350 - acc: 0.7529 -- iter: 08/22\n",
            "Training Step: 1679  | total loss: \u001b[1m\u001b[32m1.10724\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 560 | loss: 1.10724 - acc: 0.7151 -- iter: 16/22\n",
            "Training Step: 1680  | total loss: \u001b[1m\u001b[32m1.02503\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 560 | loss: 1.02503 - acc: 0.7436 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1681  | total loss: \u001b[1m\u001b[32m0.97195\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 561 | loss: 0.97195 - acc: 0.7526 -- iter: 08/22\n",
            "Training Step: 1682  | total loss: \u001b[1m\u001b[32m0.94053\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 561 | loss: 0.94053 - acc: 0.7398 -- iter: 16/22\n",
            "Training Step: 1683  | total loss: \u001b[1m\u001b[32m0.89321\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 561 | loss: 0.89321 - acc: 0.7409 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1684  | total loss: \u001b[1m\u001b[32m0.85298\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 562 | loss: 0.85298 - acc: 0.7501 -- iter: 08/22\n",
            "Training Step: 1685  | total loss: \u001b[1m\u001b[32m0.80838\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 562 | loss: 0.80838 - acc: 0.7418 -- iter: 16/22\n",
            "Training Step: 1686  | total loss: \u001b[1m\u001b[32m0.78993\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 562 | loss: 0.78993 - acc: 0.7426 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1687  | total loss: \u001b[1m\u001b[32m0.76683\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 563 | loss: 0.76683 - acc: 0.7433 -- iter: 08/22\n",
            "Training Step: 1688  | total loss: \u001b[1m\u001b[32m0.73047\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 563 | loss: 0.73047 - acc: 0.7357 -- iter: 16/22\n",
            "Training Step: 1689  | total loss: \u001b[1m\u001b[32m0.69094\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 563 | loss: 0.69094 - acc: 0.7621 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1690  | total loss: \u001b[1m\u001b[32m0.67858\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 564 | loss: 0.67858 - acc: 0.7484 -- iter: 08/22\n",
            "Training Step: 1691  | total loss: \u001b[1m\u001b[32m0.67697\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 564 | loss: 0.67697 - acc: 0.7360 -- iter: 16/22\n",
            "Training Step: 1692  | total loss: \u001b[1m\u001b[32m0.64230\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 564 | loss: 0.64230 - acc: 0.7624 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1693  | total loss: \u001b[1m\u001b[32m0.61825\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 565 | loss: 0.61825 - acc: 0.7529 -- iter: 08/22\n",
            "Training Step: 1694  | total loss: \u001b[1m\u001b[32m0.60638\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 565 | loss: 0.60638 - acc: 0.7651 -- iter: 16/22\n",
            "Training Step: 1695  | total loss: \u001b[1m\u001b[32m0.61287\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 565 | loss: 0.61287 - acc: 0.7511 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1696  | total loss: \u001b[1m\u001b[32m0.59138\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 566 | loss: 0.59138 - acc: 0.7426 -- iter: 08/22\n",
            "Training Step: 1697  | total loss: \u001b[1m\u001b[32m0.59768\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 566 | loss: 0.59768 - acc: 0.7184 -- iter: 16/22\n",
            "Training Step: 1698  | total loss: \u001b[1m\u001b[32m0.59835\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 566 | loss: 0.59835 - acc: 0.7090 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1699  | total loss: \u001b[1m\u001b[32m0.57542\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 567 | loss: 0.57542 - acc: 0.7381 -- iter: 08/22\n",
            "Training Step: 1700  | total loss: \u001b[1m\u001b[32m0.58301\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 567 | loss: 0.58301 - acc: 0.7143 -- iter: 16/22\n",
            "Training Step: 1701  | total loss: \u001b[1m\u001b[32m0.57205\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 567 | loss: 0.57205 - acc: 0.7262 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1702  | total loss: \u001b[1m\u001b[32m0.57018\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 568 | loss: 0.57018 - acc: 0.7161 -- iter: 08/22\n",
            "Training Step: 1703  | total loss: \u001b[1m\u001b[32m0.56796\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 568 | loss: 0.56796 - acc: 0.7195 -- iter: 16/22\n",
            "Training Step: 1704  | total loss: \u001b[1m\u001b[32m0.55827\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 568 | loss: 0.55827 - acc: 0.7309 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1705  | total loss: \u001b[1m\u001b[32m0.55804\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 569 | loss: 0.55804 - acc: 0.7411 -- iter: 08/22\n",
            "Training Step: 1706  | total loss: \u001b[1m\u001b[32m0.55184\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 569 | loss: 0.55184 - acc: 0.7545 -- iter: 16/22\n",
            "Training Step: 1707  | total loss: \u001b[1m\u001b[32m0.55029\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 569 | loss: 0.55029 - acc: 0.7541 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1708  | total loss: \u001b[1m\u001b[32m0.55064\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 570 | loss: 0.55064 - acc: 0.7620 -- iter: 08/22\n",
            "Training Step: 1709  | total loss: \u001b[1m\u001b[32m0.55589\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 570 | loss: 0.55589 - acc: 0.7691 -- iter: 16/22\n",
            "Training Step: 1710  | total loss: \u001b[1m\u001b[32m0.54609\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 570 | loss: 0.54609 - acc: 0.7672 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1711  | total loss: \u001b[1m\u001b[32m0.54469\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 571 | loss: 0.54469 - acc: 0.7780 -- iter: 08/22\n",
            "Training Step: 1712  | total loss: \u001b[1m\u001b[32m0.99513\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 571 | loss: 0.99513 - acc: 0.7335 -- iter: 16/22\n",
            "Training Step: 1713  | total loss: \u001b[1m\u001b[32m0.94684\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 571 | loss: 0.94684 - acc: 0.7435 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1714  | total loss: \u001b[1m\u001b[32m0.90314\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 572 | loss: 0.90314 - acc: 0.7567 -- iter: 08/22\n",
            "Training Step: 1715  | total loss: \u001b[1m\u001b[32m0.86742\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 572 | loss: 0.86742 - acc: 0.7560 -- iter: 16/22\n",
            "Training Step: 1716  | total loss: \u001b[1m\u001b[32m1.40800\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 572 | loss: 1.40800 - acc: 0.6804 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1717  | total loss: \u001b[1m\u001b[32m1.33585\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 573 | loss: 1.33585 - acc: 0.6790 -- iter: 08/22\n",
            "Training Step: 1718  | total loss: \u001b[1m\u001b[32m1.25279\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 573 | loss: 1.25279 - acc: 0.6986 -- iter: 16/22\n",
            "Training Step: 1719  | total loss: \u001b[1m\u001b[32m1.17009\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 573 | loss: 1.17009 - acc: 0.7163 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1720  | total loss: \u001b[1m\u001b[32m1.50946\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 574 | loss: 1.50946 - acc: 0.6613 -- iter: 08/22\n",
            "Training Step: 1721  | total loss: \u001b[1m\u001b[32m1.41691\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 574 | loss: 1.41691 - acc: 0.6785 -- iter: 16/22\n",
            "Training Step: 1722  | total loss: \u001b[1m\u001b[32m1.32303\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 574 | loss: 1.32303 - acc: 0.6856 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1723  | total loss: \u001b[1m\u001b[32m1.24452\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 575 | loss: 1.24452 - acc: 0.6921 -- iter: 08/22\n",
            "Training Step: 1724  | total loss: \u001b[1m\u001b[32m1.95537\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 575 | loss: 1.95537 - acc: 0.6229 -- iter: 16/22\n",
            "Training Step: 1725  | total loss: \u001b[1m\u001b[32m1.80849\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 575 | loss: 1.80849 - acc: 0.6439 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1726  | total loss: \u001b[1m\u001b[32m1.68165\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 576 | loss: 1.68165 - acc: 0.6545 -- iter: 08/22\n",
            "Training Step: 1727  | total loss: \u001b[1m\u001b[32m1.56958\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 576 | loss: 1.56958 - acc: 0.6766 -- iter: 16/22\n",
            "Training Step: 1728  | total loss: \u001b[1m\u001b[32m1.46166\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 576 | loss: 1.46166 - acc: 0.6923 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1729  | total loss: \u001b[1m\u001b[32m1.39614\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 577 | loss: 1.39614 - acc: 0.7230 -- iter: 08/22\n",
            "Training Step: 1730  | total loss: \u001b[1m\u001b[32m1.30640\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 577 | loss: 1.30640 - acc: 0.7132 -- iter: 16/22\n",
            "Training Step: 1731  | total loss: \u001b[1m\u001b[32m1.21339\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 577 | loss: 1.21339 - acc: 0.7294 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1732  | total loss: \u001b[1m\u001b[32m1.17262\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 578 | loss: 1.17262 - acc: 0.7565 -- iter: 08/22\n",
            "Training Step: 1733  | total loss: \u001b[1m\u001b[32m1.10563\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 578 | loss: 1.10563 - acc: 0.7641 -- iter: 16/22\n",
            "Training Step: 1734  | total loss: \u001b[1m\u001b[32m1.04487\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 578 | loss: 1.04487 - acc: 0.7752 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1735  | total loss: \u001b[1m\u001b[32m1.00124\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 579 | loss: 1.00124 - acc: 0.7727 -- iter: 08/22\n",
            "Training Step: 1736  | total loss: \u001b[1m\u001b[32m0.95156\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 579 | loss: 0.95156 - acc: 0.7788 -- iter: 16/22\n",
            "Training Step: 1737  | total loss: \u001b[1m\u001b[32m0.89599\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 579 | loss: 0.89599 - acc: 0.7676 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1738  | total loss: \u001b[1m\u001b[32m0.86702\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 580 | loss: 0.86702 - acc: 0.7783 -- iter: 08/22\n",
            "Training Step: 1739  | total loss: \u001b[1m\u001b[32m0.83842\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 580 | loss: 0.83842 - acc: 0.7880 -- iter: 16/22\n",
            "Training Step: 1740  | total loss: \u001b[1m\u001b[32m0.95575\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 580 | loss: 0.95575 - acc: 0.7592 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1741  | total loss: \u001b[1m\u001b[32m0.91597\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 581 | loss: 0.91597 - acc: 0.7666 -- iter: 08/22\n",
            "Training Step: 1742  | total loss: \u001b[1m\u001b[32m0.86643\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 581 | loss: 0.86643 - acc: 0.7899 -- iter: 16/22\n",
            "Training Step: 1743  | total loss: \u001b[1m\u001b[32m0.84424\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 581 | loss: 0.84424 - acc: 0.7734 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1744  | total loss: \u001b[1m\u001b[32m1.05452\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 582 | loss: 1.05452 - acc: 0.7294 -- iter: 08/22\n",
            "Training Step: 1745  | total loss: \u001b[1m\u001b[32m0.99649\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 582 | loss: 0.99649 - acc: 0.7398 -- iter: 16/22\n",
            "Training Step: 1746  | total loss: \u001b[1m\u001b[32m0.96568\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 582 | loss: 0.96568 - acc: 0.7533 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1747  | total loss: \u001b[1m\u001b[32m0.91331\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 583 | loss: 0.91331 - acc: 0.7530 -- iter: 08/22\n",
            "Training Step: 1748  | total loss: \u001b[1m\u001b[32m0.86965\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 583 | loss: 0.86965 - acc: 0.7610 -- iter: 16/22\n",
            "Training Step: 1749  | total loss: \u001b[1m\u001b[32m0.82215\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 583 | loss: 0.82215 - acc: 0.7683 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1750  | total loss: \u001b[1m\u001b[32m0.78642\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 584 | loss: 0.78642 - acc: 0.7664 -- iter: 08/22\n",
            "Training Step: 1751  | total loss: \u001b[1m\u001b[32m0.78045\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 584 | loss: 0.78045 - acc: 0.7648 -- iter: 16/22\n",
            "Training Step: 1752  | total loss: \u001b[1m\u001b[32m0.74185\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 584 | loss: 0.74185 - acc: 0.7716 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1753  | total loss: \u001b[1m\u001b[32m0.75191\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 585 | loss: 0.75191 - acc: 0.7778 -- iter: 08/22\n",
            "Training Step: 1754  | total loss: \u001b[1m\u001b[32m0.70472\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 585 | loss: 0.70472 - acc: 0.7875 -- iter: 16/22\n",
            "Training Step: 1755  | total loss: \u001b[1m\u001b[32m0.69163\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 585 | loss: 0.69163 - acc: 0.7713 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1756  | total loss: \u001b[1m\u001b[32m0.70644\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 586 | loss: 0.70644 - acc: 0.7775 -- iter: 08/22\n",
            "Training Step: 1757  | total loss: \u001b[1m\u001b[32m0.68251\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 586 | loss: 0.68251 - acc: 0.7831 -- iter: 16/22\n",
            "Training Step: 1758  | total loss: \u001b[1m\u001b[32m0.66537\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 586 | loss: 0.66537 - acc: 0.7923 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1759  | total loss: \u001b[1m\u001b[32m0.66068\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 587 | loss: 0.66068 - acc: 0.7755 -- iter: 08/22\n",
            "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.64118\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 587 | loss: 0.64118 - acc: 0.7813 -- iter: 16/22\n",
            "Training Step: 1761  | total loss: \u001b[1m\u001b[32m0.61746\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 587 | loss: 0.61746 - acc: 0.7865 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1762  | total loss: \u001b[1m\u001b[32m0.58044\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 588 | loss: 0.58044 - acc: 0.8079 -- iter: 08/22\n",
            "Training Step: 1763  | total loss: \u001b[1m\u001b[32m0.61474\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 588 | loss: 0.61474 - acc: 0.7771 -- iter: 16/22\n",
            "Training Step: 1764  | total loss: \u001b[1m\u001b[32m0.59333\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 588 | loss: 0.59333 - acc: 0.7827 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1765  | total loss: \u001b[1m\u001b[32m0.59103\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 589 | loss: 0.59103 - acc: 0.7878 -- iter: 08/22\n",
            "Training Step: 1766  | total loss: \u001b[1m\u001b[32m0.57566\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 589 | loss: 0.57566 - acc: 0.7715 -- iter: 16/22\n",
            "Training Step: 1767  | total loss: \u001b[1m\u001b[32m0.57812\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 589 | loss: 0.57812 - acc: 0.7818 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1768  | total loss: \u001b[1m\u001b[32m0.57705\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 590 | loss: 0.57705 - acc: 0.7870 -- iter: 08/22\n",
            "Training Step: 1769  | total loss: \u001b[1m\u001b[32m0.56115\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 590 | loss: 0.56115 - acc: 0.7916 -- iter: 16/22\n",
            "Training Step: 1770  | total loss: \u001b[1m\u001b[32m0.56054\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 590 | loss: 0.56054 - acc: 0.7875 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1771  | total loss: \u001b[1m\u001b[32m0.56327\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 591 | loss: 0.56327 - acc: 0.7837 -- iter: 08/22\n",
            "Training Step: 1772  | total loss: \u001b[1m\u001b[32m0.54840\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 591 | loss: 0.54840 - acc: 0.7887 -- iter: 16/22\n",
            "Training Step: 1773  | total loss: \u001b[1m\u001b[32m0.52764\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 591 | loss: 0.52764 - acc: 0.7931 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1774  | total loss: \u001b[1m\u001b[32m0.52347\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 592 | loss: 0.52347 - acc: 0.8013 -- iter: 08/22\n",
            "Training Step: 1775  | total loss: \u001b[1m\u001b[32m0.54178\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 592 | loss: 0.54178 - acc: 0.7837 -- iter: 16/22\n",
            "Training Step: 1776  | total loss: \u001b[1m\u001b[32m0.52130\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 592 | loss: 0.52130 - acc: 0.7887 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1777  | total loss: \u001b[1m\u001b[32m0.53780\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 593 | loss: 0.53780 - acc: 0.7765 -- iter: 08/22\n",
            "Training Step: 1778  | total loss: \u001b[1m\u001b[32m0.53375\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 593 | loss: 0.53375 - acc: 0.7863 -- iter: 16/22\n",
            "Training Step: 1779  | total loss: \u001b[1m\u001b[32m0.52307\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 593 | loss: 0.52307 - acc: 0.7827 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1780  | total loss: \u001b[1m\u001b[32m0.53929\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 594 | loss: 0.53929 - acc: 0.7711 -- iter: 08/22\n",
            "Training Step: 1781  | total loss: \u001b[1m\u001b[32m0.55620\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 594 | loss: 0.55620 - acc: 0.7773 -- iter: 16/22\n",
            "Training Step: 1782  | total loss: \u001b[1m\u001b[32m0.55476\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 594 | loss: 0.55476 - acc: 0.7621 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1783  | total loss: \u001b[1m\u001b[32m0.53517\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 595 | loss: 0.53517 - acc: 0.7734 -- iter: 08/22\n",
            "Training Step: 1784  | total loss: \u001b[1m\u001b[32m0.55233\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 595 | loss: 0.55233 - acc: 0.7794 -- iter: 16/22\n",
            "Training Step: 1785  | total loss: \u001b[1m\u001b[32m0.57390\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 595 | loss: 0.57390 - acc: 0.7681 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1786  | total loss: \u001b[1m\u001b[32m0.55891\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 596 | loss: 0.55891 - acc: 0.7788 -- iter: 08/22\n",
            "Training Step: 1787  | total loss: \u001b[1m\u001b[32m0.54562\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 596 | loss: 0.54562 - acc: 0.7759 -- iter: 16/22\n",
            "Training Step: 1788  | total loss: \u001b[1m\u001b[32m0.56763\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 596 | loss: 0.56763 - acc: 0.7650 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1789  | total loss: \u001b[1m\u001b[32m0.55308\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 597 | loss: 0.55308 - acc: 0.7885 -- iter: 08/22\n",
            "Training Step: 1790  | total loss: \u001b[1m\u001b[32m0.54613\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 597 | loss: 0.54613 - acc: 0.7971 -- iter: 16/22\n",
            "Training Step: 1791  | total loss: \u001b[1m\u001b[32m0.55345\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 597 | loss: 0.55345 - acc: 0.7674 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1792  | total loss: \u001b[1m\u001b[32m0.54011\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 598 | loss: 0.54011 - acc: 0.7907 -- iter: 08/22\n",
            "Training Step: 1793  | total loss: \u001b[1m\u001b[32m0.50965\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 598 | loss: 0.50965 - acc: 0.7949 -- iter: 16/22\n",
            "Training Step: 1794  | total loss: \u001b[1m\u001b[32m0.51746\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 598 | loss: 0.51746 - acc: 0.8030 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1795  | total loss: \u001b[1m\u001b[32m0.53059\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 599 | loss: 0.53059 - acc: 0.7852 -- iter: 08/22\n",
            "Training Step: 1796  | total loss: \u001b[1m\u001b[32m0.50094\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 599 | loss: 0.50094 - acc: 0.7900 -- iter: 16/22\n",
            "Training Step: 1797  | total loss: \u001b[1m\u001b[32m0.52021\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 599 | loss: 0.52021 - acc: 0.7776 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1798  | total loss: \u001b[1m\u001b[32m0.51202\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 600 | loss: 0.51202 - acc: 0.7874 -- iter: 08/22\n",
            "Training Step: 1799  | total loss: \u001b[1m\u001b[32m0.50572\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 600 | loss: 0.50572 - acc: 0.7836 -- iter: 16/22\n",
            "Training Step: 1800  | total loss: \u001b[1m\u001b[32m0.52434\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 600 | loss: 0.52434 - acc: 0.7719 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1801  | total loss: \u001b[1m\u001b[32m0.53994\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 601 | loss: 0.53994 - acc: 0.7614 -- iter: 08/22\n",
            "Training Step: 1802  | total loss: \u001b[1m\u001b[32m0.53888\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 601 | loss: 0.53888 - acc: 0.7478 -- iter: 16/22\n",
            "Training Step: 1803  | total loss: \u001b[1m\u001b[32m0.52133\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 601 | loss: 0.52133 - acc: 0.7730 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1804  | total loss: \u001b[1m\u001b[32m0.53695\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 602 | loss: 0.53695 - acc: 0.7790 -- iter: 08/22\n",
            "Training Step: 1805  | total loss: \u001b[1m\u001b[32m0.56464\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 602 | loss: 0.56464 - acc: 0.7845 -- iter: 16/22\n",
            "Training Step: 1806  | total loss: \u001b[1m\u001b[32m0.55396\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 602 | loss: 0.55396 - acc: 0.7810 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1807  | total loss: \u001b[1m\u001b[32m0.53157\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 603 | loss: 0.53157 - acc: 0.7904 -- iter: 08/22\n",
            "Training Step: 1808  | total loss: \u001b[1m\u001b[32m0.55935\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 603 | loss: 0.55935 - acc: 0.8114 -- iter: 16/22\n",
            "Training Step: 1809  | total loss: \u001b[1m\u001b[32m0.56001\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 603 | loss: 0.56001 - acc: 0.8136 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1810  | total loss: \u001b[1m\u001b[32m0.57555\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 604 | loss: 0.57555 - acc: 0.8072 -- iter: 08/22\n",
            "Training Step: 1811  | total loss: \u001b[1m\u001b[32m0.54319\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 604 | loss: 0.54319 - acc: 0.8265 -- iter: 16/22\n",
            "Training Step: 1812  | total loss: \u001b[1m\u001b[32m0.54524\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 604 | loss: 0.54524 - acc: 0.8272 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1813  | total loss: \u001b[1m\u001b[32m0.51929\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 605 | loss: 0.51929 - acc: 0.8445 -- iter: 08/22\n",
            "Training Step: 1814  | total loss: \u001b[1m\u001b[32m0.52554\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 605 | loss: 0.52554 - acc: 0.8475 -- iter: 16/22\n",
            "Training Step: 1815  | total loss: \u001b[1m\u001b[32m0.53206\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 605 | loss: 0.53206 - acc: 0.8378 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1816  | total loss: \u001b[1m\u001b[32m0.50727\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 606 | loss: 0.50727 - acc: 0.8540 -- iter: 08/22\n",
            "Training Step: 1817  | total loss: \u001b[1m\u001b[32m0.49502\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 606 | loss: 0.49502 - acc: 0.8686 -- iter: 16/22\n",
            "Training Step: 1818  | total loss: \u001b[1m\u001b[32m0.50783\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 606 | loss: 0.50783 - acc: 0.8567 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1819  | total loss: \u001b[1m\u001b[32m0.50407\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 607 | loss: 0.50407 - acc: 0.8586 -- iter: 08/22\n",
            "Training Step: 1820  | total loss: \u001b[1m\u001b[32m0.49194\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 607 | loss: 0.49194 - acc: 0.8727 -- iter: 16/22\n",
            "Training Step: 1821  | total loss: \u001b[1m\u001b[32m0.49547\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 607 | loss: 0.49547 - acc: 0.8854 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1822  | total loss: \u001b[1m\u001b[32m0.48459\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 608 | loss: 0.48459 - acc: 0.8844 -- iter: 08/22\n",
            "Training Step: 1823  | total loss: \u001b[1m\u001b[32m0.49578\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 608 | loss: 0.49578 - acc: 0.8709 -- iter: 16/22\n",
            "Training Step: 1824  | total loss: \u001b[1m\u001b[32m0.49856\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 608 | loss: 0.49856 - acc: 0.8839 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1825  | total loss: \u001b[1m\u001b[32m0.49100\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 609 | loss: 0.49100 - acc: 0.8788 -- iter: 08/22\n",
            "Training Step: 1826  | total loss: \u001b[1m\u001b[32m0.51041\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 609 | loss: 0.51041 - acc: 0.8659 -- iter: 16/22\n",
            "Training Step: 1827  | total loss: \u001b[1m\u001b[32m0.49639\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 609 | loss: 0.49639 - acc: 0.8793 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1828  | total loss: \u001b[1m\u001b[32m0.48900\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 610 | loss: 0.48900 - acc: 0.8747 -- iter: 08/22\n",
            "Training Step: 1829  | total loss: \u001b[1m\u001b[32m0.49208\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 610 | loss: 0.49208 - acc: 0.8873 -- iter: 16/22\n",
            "Training Step: 1830  | total loss: \u001b[1m\u001b[32m0.48483\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 610 | loss: 0.48483 - acc: 0.8860 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1831  | total loss: \u001b[1m\u001b[32m0.49229\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 611 | loss: 0.49229 - acc: 0.8724 -- iter: 08/22\n",
            "Training Step: 1832  | total loss: \u001b[1m\u001b[32m0.49478\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 611 | loss: 0.49478 - acc: 0.8852 -- iter: 16/22\n",
            "Training Step: 1833  | total loss: \u001b[1m\u001b[32m0.48567\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 611 | loss: 0.48567 - acc: 0.8967 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1834  | total loss: \u001b[1m\u001b[32m0.49334\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 612 | loss: 0.49334 - acc: 0.8945 -- iter: 08/22\n",
            "Training Step: 1835  | total loss: \u001b[1m\u001b[32m0.49389\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 612 | loss: 0.49389 - acc: 0.8800 -- iter: 16/22\n",
            "Training Step: 1836  | total loss: \u001b[1m\u001b[32m0.48475\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 612 | loss: 0.48475 - acc: 0.8920 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1837  | total loss: \u001b[1m\u001b[32m0.50935\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 613 | loss: 0.50935 - acc: 0.8862 -- iter: 08/22\n",
            "Training Step: 1838  | total loss: \u001b[1m\u001b[32m0.49967\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 613 | loss: 0.49967 - acc: 0.8851 -- iter: 16/22\n",
            "Training Step: 1839  | total loss: \u001b[1m\u001b[32m0.48962\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 613 | loss: 0.48962 - acc: 0.8841 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1840  | total loss: \u001b[1m\u001b[32m0.51349\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 614 | loss: 0.51349 - acc: 0.8790 -- iter: 08/22\n",
            "Training Step: 1841  | total loss: \u001b[1m\u001b[32m0.50742\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 614 | loss: 0.50742 - acc: 0.8744 -- iter: 16/22\n",
            "Training Step: 1842  | total loss: \u001b[1m\u001b[32m0.51250\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 614 | loss: 0.51250 - acc: 0.8870 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1843  | total loss: \u001b[1m\u001b[32m0.50695\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 615 | loss: 0.50695 - acc: 0.8733 -- iter: 08/22\n",
            "Training Step: 1844  | total loss: \u001b[1m\u001b[32m0.50136\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 615 | loss: 0.50136 - acc: 0.8693 -- iter: 16/22\n",
            "Training Step: 1845  | total loss: \u001b[1m\u001b[32m0.50907\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 615 | loss: 0.50907 - acc: 0.8824 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1846  | total loss: \u001b[1m\u001b[32m0.50005\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 616 | loss: 0.50005 - acc: 0.8816 -- iter: 08/22\n",
            "Training Step: 1847  | total loss: \u001b[1m\u001b[32m0.49984\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 616 | loss: 0.49984 - acc: 0.8685 -- iter: 16/22\n",
            "Training Step: 1848  | total loss: \u001b[1m\u001b[32m0.50730\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 616 | loss: 0.50730 - acc: 0.8816 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1849  | total loss: \u001b[1m\u001b[32m0.50256\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 617 | loss: 0.50256 - acc: 0.8934 -- iter: 08/22\n",
            "Training Step: 1850  | total loss: \u001b[1m\u001b[32m0.50609\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 617 | loss: 0.50609 - acc: 0.8916 -- iter: 16/22\n",
            "Training Step: 1851  | total loss: \u001b[1m\u001b[32m0.50174\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 617 | loss: 0.50174 - acc: 0.8774 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1852  | total loss: \u001b[1m\u001b[32m0.49722\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 618 | loss: 0.49722 - acc: 0.8897 -- iter: 08/22\n",
            "Training Step: 1853  | total loss: \u001b[1m\u001b[32m0.50572\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 618 | loss: 0.50572 - acc: 0.8841 -- iter: 16/22\n",
            "Training Step: 1854  | total loss: \u001b[1m\u001b[32m0.49841\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 618 | loss: 0.49841 - acc: 0.8957 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1855  | total loss: \u001b[1m\u001b[32m0.49560\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 619 | loss: 0.49560 - acc: 0.8811 -- iter: 08/22\n",
            "Training Step: 1856  | total loss: \u001b[1m\u001b[32m0.50410\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 619 | loss: 0.50410 - acc: 0.8763 -- iter: 16/22\n",
            "Training Step: 1857  | total loss: \u001b[1m\u001b[32m0.49011\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 619 | loss: 0.49011 - acc: 0.8720 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1858  | total loss: \u001b[1m\u001b[32m0.48582\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 620 | loss: 0.48582 - acc: 0.8848 -- iter: 08/22\n",
            "Training Step: 1859  | total loss: \u001b[1m\u001b[32m0.49870\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 620 | loss: 0.49870 - acc: 0.8713 -- iter: 16/22\n",
            "Training Step: 1860  | total loss: \u001b[1m\u001b[32m0.48502\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 620 | loss: 0.48502 - acc: 0.8675 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1861  | total loss: \u001b[1m\u001b[32m0.50838\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 621 | loss: 0.50838 - acc: 0.8641 -- iter: 08/22\n",
            "Training Step: 1862  | total loss: \u001b[1m\u001b[32m0.48827\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 621 | loss: 0.48827 - acc: 0.8777 -- iter: 16/22\n",
            "Training Step: 1863  | total loss: \u001b[1m\u001b[32m0.48780\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 621 | loss: 0.48780 - acc: 0.8649 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1864  | total loss: \u001b[1m\u001b[32m1.02042\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 622 | loss: 1.02042 - acc: 0.7951 -- iter: 08/22\n",
            "Training Step: 1865  | total loss: \u001b[1m\u001b[32m0.98820\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 622 | loss: 0.98820 - acc: 0.7989 -- iter: 16/22\n",
            "Training Step: 1866  | total loss: \u001b[1m\u001b[32m0.95656\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 622 | loss: 0.95656 - acc: 0.7940 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1867  | total loss: \u001b[1m\u001b[32m0.87398\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 623 | loss: 0.87398 - acc: 0.8146 -- iter: 08/22\n",
            "Training Step: 1868  | total loss: \u001b[1m\u001b[32m1.25544\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 623 | loss: 1.25544 - acc: 0.7498 -- iter: 16/22\n",
            "Training Step: 1869  | total loss: \u001b[1m\u001b[32m1.17868\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 623 | loss: 1.17868 - acc: 0.7582 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1870  | total loss: \u001b[1m\u001b[32m1.12689\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 624 | loss: 1.12689 - acc: 0.7699 -- iter: 08/22\n",
            "Training Step: 1871  | total loss: \u001b[1m\u001b[32m1.04416\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 624 | loss: 1.04416 - acc: 0.7929 -- iter: 16/22\n",
            "Training Step: 1872  | total loss: \u001b[1m\u001b[32m1.68238\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 624 | loss: 1.68238 - acc: 0.7303 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1873  | total loss: \u001b[1m\u001b[32m1.57886\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 625 | loss: 1.57886 - acc: 0.7406 -- iter: 08/22\n",
            "Training Step: 1874  | total loss: \u001b[1m\u001b[32m1.45208\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 625 | loss: 1.45208 - acc: 0.7665 -- iter: 16/22\n",
            "Training Step: 1875  | total loss: \u001b[1m\u001b[32m1.36031\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 625 | loss: 1.36031 - acc: 0.7899 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1876  | total loss: \u001b[1m\u001b[32m1.28902\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 626 | loss: 1.28902 - acc: 0.7942 -- iter: 08/22\n",
            "Training Step: 1877  | total loss: \u001b[1m\u001b[32m1.20359\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 626 | loss: 1.20359 - acc: 0.8148 -- iter: 16/22\n",
            "Training Step: 1878  | total loss: \u001b[1m\u001b[32m1.15275\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 626 | loss: 1.15275 - acc: 0.8208 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1879  | total loss: \u001b[1m\u001b[32m1.06891\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 627 | loss: 1.06891 - acc: 0.8387 -- iter: 08/22\n",
            "Training Step: 1880  | total loss: \u001b[1m\u001b[32m1.00558\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 627 | loss: 1.00558 - acc: 0.8549 -- iter: 16/22\n",
            "Training Step: 1881  | total loss: \u001b[1m\u001b[32m0.95461\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 627 | loss: 0.95461 - acc: 0.8527 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1882  | total loss: \u001b[1m\u001b[32m0.90646\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 628 | loss: 0.90646 - acc: 0.8674 -- iter: 08/22\n",
            "Training Step: 1883  | total loss: \u001b[1m\u001b[32m0.86497\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 628 | loss: 0.86497 - acc: 0.8807 -- iter: 16/22\n",
            "Training Step: 1884  | total loss: \u001b[1m\u001b[32m0.82808\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 628 | loss: 0.82808 - acc: 0.8593 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1885  | total loss: \u001b[1m\u001b[32m0.78172\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 629 | loss: 0.78172 - acc: 0.8567 -- iter: 08/22\n",
            "Training Step: 1886  | total loss: \u001b[1m\u001b[32m0.76231\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 629 | loss: 0.76231 - acc: 0.8585 -- iter: 16/22\n",
            "Training Step: 1887  | total loss: \u001b[1m\u001b[32m0.73363\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 629 | loss: 0.73363 - acc: 0.8727 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1888  | total loss: \u001b[1m\u001b[32m0.69664\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 630 | loss: 0.69664 - acc: 0.8687 -- iter: 08/22\n",
            "Training Step: 1889  | total loss: \u001b[1m\u001b[32m0.67339\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 630 | loss: 0.67339 - acc: 0.8652 -- iter: 16/22\n",
            "Training Step: 1890  | total loss: \u001b[1m\u001b[32m0.65271\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 630 | loss: 0.65271 - acc: 0.8662 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1891  | total loss: \u001b[1m\u001b[32m0.63945\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 631 | loss: 0.63945 - acc: 0.8796 -- iter: 08/22\n",
            "Training Step: 1892  | total loss: \u001b[1m\u001b[32m1.16889\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 631 | loss: 1.16889 - acc: 0.8083 -- iter: 16/22\n",
            "Training Step: 1893  | total loss: \u001b[1m\u001b[32m1.10285\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 631 | loss: 1.10285 - acc: 0.8274 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1894  | total loss: \u001b[1m\u001b[32m1.02413\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 632 | loss: 1.02413 - acc: 0.8447 -- iter: 08/22\n",
            "Training Step: 1895  | total loss: \u001b[1m\u001b[32m0.98551\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 632 | loss: 0.98551 - acc: 0.8477 -- iter: 16/22\n",
            "Training Step: 1896  | total loss: \u001b[1m\u001b[32m0.93784\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 632 | loss: 0.93784 - acc: 0.8463 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1897  | total loss: \u001b[1m\u001b[32m0.90848\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 633 | loss: 0.90848 - acc: 0.8450 -- iter: 08/22\n",
            "Training Step: 1898  | total loss: \u001b[1m\u001b[32m0.84748\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 633 | loss: 0.84748 - acc: 0.8605 -- iter: 16/22\n",
            "Training Step: 1899  | total loss: \u001b[1m\u001b[32m0.81825\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 633 | loss: 0.81825 - acc: 0.8744 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1900  | total loss: \u001b[1m\u001b[32m1.53859\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 634 | loss: 1.53859 - acc: 0.7870 -- iter: 08/22\n",
            "Training Step: 1901  | total loss: \u001b[1m\u001b[32m1.42187\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 634 | loss: 1.42187 - acc: 0.7916 -- iter: 16/22\n",
            "Training Step: 1902  | total loss: \u001b[1m\u001b[32m1.35562\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 634 | loss: 1.35562 - acc: 0.8125 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1903  | total loss: \u001b[1m\u001b[32m1.25020\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 635 | loss: 1.25020 - acc: 0.8312 -- iter: 08/22\n",
            "Training Step: 1904  | total loss: \u001b[1m\u001b[32m1.94153\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 635 | loss: 1.94153 - acc: 0.7481 -- iter: 16/22\n",
            "Training Step: 1905  | total loss: \u001b[1m\u001b[32m1.81393\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 635 | loss: 1.81393 - acc: 0.7400 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1906  | total loss: \u001b[1m\u001b[32m1.66904\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 636 | loss: 1.66904 - acc: 0.7660 -- iter: 08/22\n",
            "Training Step: 1907  | total loss: \u001b[1m\u001b[32m1.55019\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 636 | loss: 1.55019 - acc: 0.7894 -- iter: 16/22\n",
            "Training Step: 1908  | total loss: \u001b[1m\u001b[32m2.22400\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 636 | loss: 2.22400 - acc: 0.7104 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1909  | total loss: \u001b[1m\u001b[32m2.03564\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 637 | loss: 2.03564 - acc: 0.7394 -- iter: 08/22\n",
            "Training Step: 1910  | total loss: \u001b[1m\u001b[32m1.89100\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 637 | loss: 1.89100 - acc: 0.7404 -- iter: 16/22\n",
            "Training Step: 1911  | total loss: \u001b[1m\u001b[32m1.75270\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 637 | loss: 1.75270 - acc: 0.7664 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1912  | total loss: \u001b[1m\u001b[32m1.61154\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 638 | loss: 1.61154 - acc: 0.7898 -- iter: 08/22\n",
            "Training Step: 1913  | total loss: \u001b[1m\u001b[32m1.50029\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 638 | loss: 1.50029 - acc: 0.8108 -- iter: 16/22\n",
            "Training Step: 1914  | total loss: \u001b[1m\u001b[32m1.39557\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 638 | loss: 1.39557 - acc: 0.8047 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1915  | total loss: \u001b[1m\u001b[32m1.30925\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 639 | loss: 1.30925 - acc: 0.8242 -- iter: 08/22\n",
            "Training Step: 1916  | total loss: \u001b[1m\u001b[32m1.22821\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 639 | loss: 1.22821 - acc: 0.8418 -- iter: 16/22\n",
            "Training Step: 1917  | total loss: \u001b[1m\u001b[32m1.16382\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 639 | loss: 1.16382 - acc: 0.8410 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1918  | total loss: \u001b[1m\u001b[32m1.09975\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 640 | loss: 1.09975 - acc: 0.8569 -- iter: 08/22\n",
            "Training Step: 1919  | total loss: \u001b[1m\u001b[32m1.02985\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 640 | loss: 1.02985 - acc: 0.8462 -- iter: 16/22\n",
            "Training Step: 1920  | total loss: \u001b[1m\u001b[32m0.98536\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 640 | loss: 0.98536 - acc: 0.8449 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1921  | total loss: \u001b[1m\u001b[32m0.93999\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 641 | loss: 0.93999 - acc: 0.8604 -- iter: 08/22\n",
            "Training Step: 1922  | total loss: \u001b[1m\u001b[32m0.89041\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 641 | loss: 0.89041 - acc: 0.8619 -- iter: 16/22\n",
            "Training Step: 1923  | total loss: \u001b[1m\u001b[32m0.85326\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 641 | loss: 0.85326 - acc: 0.8507 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1924  | total loss: \u001b[1m\u001b[32m0.82095\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 642 | loss: 0.82095 - acc: 0.8656 -- iter: 08/22\n",
            "Training Step: 1925  | total loss: \u001b[1m\u001b[32m0.80134\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 642 | loss: 0.80134 - acc: 0.8624 -- iter: 16/22\n",
            "Training Step: 1926  | total loss: \u001b[1m\u001b[32m0.76232\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 642 | loss: 0.76232 - acc: 0.8636 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1927  | total loss: \u001b[1m\u001b[32m0.73398\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 643 | loss: 0.73398 - acc: 0.8648 -- iter: 08/22\n",
            "Training Step: 1928  | total loss: \u001b[1m\u001b[32m1.23252\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 643 | loss: 1.23252 - acc: 0.7950 -- iter: 16/22\n",
            "Training Step: 1929  | total loss: \u001b[1m\u001b[32m1.13255\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 643 | loss: 1.13255 - acc: 0.7988 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1930  | total loss: \u001b[1m\u001b[32m1.08162\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 644 | loss: 1.08162 - acc: 0.8064 -- iter: 08/22\n",
            "Training Step: 1931  | total loss: \u001b[1m\u001b[32m1.02958\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 644 | loss: 1.02958 - acc: 0.8133 -- iter: 16/22\n",
            "Training Step: 1932  | total loss: \u001b[1m\u001b[32m0.95007\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 644 | loss: 0.95007 - acc: 0.8153 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1933  | total loss: \u001b[1m\u001b[32m0.91010\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 645 | loss: 0.91010 - acc: 0.8171 -- iter: 08/22\n",
            "Training Step: 1934  | total loss: \u001b[1m\u001b[32m0.86655\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 645 | loss: 0.86655 - acc: 0.8104 -- iter: 16/22\n",
            "Training Step: 1935  | total loss: \u001b[1m\u001b[32m0.82740\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 645 | loss: 0.82740 - acc: 0.8293 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1936  | total loss: \u001b[1m\u001b[32m0.79963\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 646 | loss: 0.79963 - acc: 0.8297 -- iter: 08/22\n",
            "Training Step: 1937  | total loss: \u001b[1m\u001b[32m0.74371\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 646 | loss: 0.74371 - acc: 0.8468 -- iter: 16/22\n",
            "Training Step: 1938  | total loss: \u001b[1m\u001b[32m0.71660\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 646 | loss: 0.71660 - acc: 0.8496 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1939  | total loss: \u001b[1m\u001b[32m0.71577\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 647 | loss: 0.71577 - acc: 0.8396 -- iter: 08/22\n",
            "Training Step: 1940  | total loss: \u001b[1m\u001b[32m0.66816\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 647 | loss: 0.66816 - acc: 0.8557 -- iter: 16/22\n",
            "Training Step: 1941  | total loss: \u001b[1m\u001b[32m0.64456\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 647 | loss: 0.64456 - acc: 0.8701 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1942  | total loss: \u001b[1m\u001b[32m0.63201\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 648 | loss: 0.63201 - acc: 0.8581 -- iter: 08/22\n",
            "Training Step: 1943  | total loss: \u001b[1m\u001b[32m0.62024\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 648 | loss: 0.62024 - acc: 0.8598 -- iter: 16/22\n",
            "Training Step: 1944  | total loss: \u001b[1m\u001b[32m0.60140\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 648 | loss: 0.60140 - acc: 0.8738 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1945  | total loss: \u001b[1m\u001b[32m0.56303\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 649 | loss: 0.56303 - acc: 0.8864 -- iter: 08/22\n",
            "Training Step: 1946  | total loss: \u001b[1m\u001b[32m0.58250\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 649 | loss: 0.58250 - acc: 0.8603 -- iter: 16/22\n",
            "Training Step: 1947  | total loss: \u001b[1m\u001b[32m0.56752\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 649 | loss: 0.56752 - acc: 0.8743 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1948  | total loss: \u001b[1m\u001b[32m0.53237\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 650 | loss: 0.53237 - acc: 0.8868 -- iter: 08/22\n",
            "Training Step: 1949  | total loss: \u001b[1m\u001b[32m0.53764\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 650 | loss: 0.53764 - acc: 0.8815 -- iter: 16/22\n",
            "Training Step: 1950  | total loss: \u001b[1m\u001b[32m0.52811\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 650 | loss: 0.52811 - acc: 0.8808 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1951  | total loss: \u001b[1m\u001b[32m0.52208\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 651 | loss: 0.52208 - acc: 0.8802 -- iter: 08/22\n",
            "Training Step: 1952  | total loss: \u001b[1m\u001b[32m0.52819\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 651 | loss: 0.52819 - acc: 0.8756 -- iter: 16/22\n",
            "Training Step: 1953  | total loss: \u001b[1m\u001b[32m0.52674\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 651 | loss: 0.52674 - acc: 0.8713 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1954  | total loss: \u001b[1m\u001b[32m0.54108\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 652 | loss: 0.54108 - acc: 0.8592 -- iter: 08/22\n",
            "Training Step: 1955  | total loss: \u001b[1m\u001b[32m0.51580\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 652 | loss: 0.51580 - acc: 0.8733 -- iter: 16/22\n",
            "Training Step: 1956  | total loss: \u001b[1m\u001b[32m0.51537\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 652 | loss: 0.51537 - acc: 0.8860 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1957  | total loss: \u001b[1m\u001b[32m0.51445\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 653 | loss: 0.51445 - acc: 0.8807 -- iter: 08/22\n",
            "Training Step: 1958  | total loss: \u001b[1m\u001b[32m0.50295\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 653 | loss: 0.50295 - acc: 0.8801 -- iter: 16/22\n",
            "Training Step: 1959  | total loss: \u001b[1m\u001b[32m0.50858\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 653 | loss: 0.50858 - acc: 0.8921 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1960  | total loss: \u001b[1m\u001b[32m0.50811\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 654 | loss: 0.50811 - acc: 0.8862 -- iter: 08/22\n",
            "Training Step: 1961  | total loss: \u001b[1m\u001b[32m0.47861\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 654 | loss: 0.47861 - acc: 0.8976 -- iter: 16/22\n",
            "Training Step: 1962  | total loss: \u001b[1m\u001b[32m0.48340\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 654 | loss: 0.48340 - acc: 0.9078 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1963  | total loss: \u001b[1m\u001b[32m0.49971\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 655 | loss: 0.49971 - acc: 0.9046 -- iter: 08/22\n",
            "Training Step: 1964  | total loss: \u001b[1m\u001b[32m0.47077\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 655 | loss: 0.47077 - acc: 0.9141 -- iter: 16/22\n",
            "Training Step: 1965  | total loss: \u001b[1m\u001b[32m0.44919\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 655 | loss: 0.44919 - acc: 0.9227 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1966  | total loss: \u001b[1m\u001b[32m0.46695\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 656 | loss: 0.46695 - acc: 0.9179 -- iter: 08/22\n",
            "Training Step: 1967  | total loss: \u001b[1m\u001b[32m0.47108\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 656 | loss: 0.47108 - acc: 0.9136 -- iter: 16/22\n",
            "Training Step: 1968  | total loss: \u001b[1m\u001b[32m0.44929\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 656 | loss: 0.44929 - acc: 0.9223 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1969  | total loss: \u001b[1m\u001b[32m0.45741\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 657 | loss: 0.45741 - acc: 0.9300 -- iter: 08/22\n",
            "Training Step: 1970  | total loss: \u001b[1m\u001b[32m0.46583\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 657 | loss: 0.46583 - acc: 0.9245 -- iter: 16/22\n",
            "Training Step: 1971  | total loss: \u001b[1m\u001b[32m0.45743\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 657 | loss: 0.45743 - acc: 0.9196 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1972  | total loss: \u001b[1m\u001b[32m0.46451\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 658 | loss: 0.46451 - acc: 0.9276 -- iter: 08/22\n",
            "Training Step: 1973  | total loss: \u001b[1m\u001b[32m0.46545\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 658 | loss: 0.46545 - acc: 0.9349 -- iter: 16/22\n",
            "Training Step: 1974  | total loss: \u001b[1m\u001b[32m0.47665\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 658 | loss: 0.47665 - acc: 0.9164 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1975  | total loss: \u001b[1m\u001b[32m0.46726\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 659 | loss: 0.46726 - acc: 0.9247 -- iter: 08/22\n",
            "Training Step: 1976  | total loss: \u001b[1m\u001b[32m0.46765\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 659 | loss: 0.46765 - acc: 0.9323 -- iter: 16/22\n",
            "Training Step: 1977  | total loss: \u001b[1m\u001b[32m0.45639\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 659 | loss: 0.45639 - acc: 0.9390 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1978  | total loss: \u001b[1m\u001b[32m0.45529\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 660 | loss: 0.45529 - acc: 0.9451 -- iter: 08/22\n",
            "Training Step: 1979  | total loss: \u001b[1m\u001b[32m0.46954\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 660 | loss: 0.46954 - acc: 0.9381 -- iter: 16/22\n",
            "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.45805\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 660 | loss: 0.45805 - acc: 0.9443 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1981  | total loss: \u001b[1m\u001b[32m0.48278\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 661 | loss: 0.48278 - acc: 0.9499 -- iter: 08/22\n",
            "Training Step: 1982  | total loss: \u001b[1m\u001b[32m0.45047\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 661 | loss: 0.45047 - acc: 0.9549 -- iter: 16/22\n",
            "Training Step: 1983  | total loss: \u001b[1m\u001b[32m0.46705\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 661 | loss: 0.46705 - acc: 0.9344 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1984  | total loss: \u001b[1m\u001b[32m0.49058\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 662 | loss: 0.49058 - acc: 0.9410 -- iter: 08/22\n",
            "Training Step: 1985  | total loss: \u001b[1m\u001b[32m0.48368\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 662 | loss: 0.48368 - acc: 0.9469 -- iter: 16/22\n",
            "Training Step: 1986  | total loss: \u001b[1m\u001b[32m0.49632\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 662 | loss: 0.49632 - acc: 0.9397 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1987  | total loss: \u001b[1m\u001b[32m0.48397\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 663 | loss: 0.48397 - acc: 0.9332 -- iter: 08/22\n",
            "Training Step: 1988  | total loss: \u001b[1m\u001b[32m0.47759\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 663 | loss: 0.47759 - acc: 0.9399 -- iter: 16/22\n",
            "Training Step: 1989  | total loss: \u001b[1m\u001b[32m0.46267\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 663 | loss: 0.46267 - acc: 0.9459 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1990  | total loss: \u001b[1m\u001b[32m0.47296\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 664 | loss: 0.47296 - acc: 0.9263 -- iter: 08/22\n",
            "Training Step: 1991  | total loss: \u001b[1m\u001b[32m0.47398\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 664 | loss: 0.47398 - acc: 0.9212 -- iter: 16/22\n",
            "Training Step: 1992  | total loss: \u001b[1m\u001b[32m0.45906\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 664 | loss: 0.45906 - acc: 0.9291 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1993  | total loss: \u001b[1m\u001b[32m0.45971\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 665 | loss: 0.45971 - acc: 0.9195 -- iter: 08/22\n",
            "Training Step: 1994  | total loss: \u001b[1m\u001b[32m0.47034\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 665 | loss: 0.47034 - acc: 0.9150 -- iter: 16/22\n",
            "Training Step: 1995  | total loss: \u001b[1m\u001b[32m0.46063\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 665 | loss: 0.46063 - acc: 0.9110 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1996  | total loss: \u001b[1m\u001b[32m0.46111\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 666 | loss: 0.46111 - acc: 0.9033 -- iter: 08/22\n",
            "Training Step: 1997  | total loss: \u001b[1m\u001b[32m0.42584\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 666 | loss: 0.42584 - acc: 0.9129 -- iter: 16/22\n",
            "Training Step: 1998  | total loss: \u001b[1m\u001b[32m0.44487\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 666 | loss: 0.44487 - acc: 0.9091 -- iter: 22/22\n",
            "--\n",
            "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.45903\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 667 | loss: 0.45903 - acc: 0.8932 -- iter: 08/22\n",
            "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.42387\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 667 | loss: 0.42387 - acc: 0.9039 -- iter: 16/22\n",
            "Training Step: 2001  | total loss: \u001b[1m\u001b[32m0.40998\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 667 | loss: 0.40998 - acc: 0.9135 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2002  | total loss: \u001b[1m\u001b[32m0.42522\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 668 | loss: 0.42522 - acc: 0.8847 -- iter: 08/22\n",
            "Training Step: 2003  | total loss: \u001b[1m\u001b[32m0.43308\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 668 | loss: 0.43308 - acc: 0.8962 -- iter: 16/22\n",
            "Training Step: 2004  | total loss: \u001b[1m\u001b[32m0.41808\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 668 | loss: 0.41808 - acc: 0.9066 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2005  | total loss: \u001b[1m\u001b[32m0.41102\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 669 | loss: 0.41102 - acc: 0.8993 -- iter: 08/22\n",
            "Training Step: 2006  | total loss: \u001b[1m\u001b[32m0.41256\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 669 | loss: 0.41256 - acc: 0.8968 -- iter: 16/22\n",
            "Training Step: 2007  | total loss: \u001b[1m\u001b[32m0.43008\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 669 | loss: 0.43008 - acc: 0.8946 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2008  | total loss: \u001b[1m\u001b[32m0.42171\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 670 | loss: 0.42171 - acc: 0.8885 -- iter: 08/22\n",
            "Training Step: 2009  | total loss: \u001b[1m\u001b[32m0.43937\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 670 | loss: 0.43937 - acc: 0.8830 -- iter: 16/22\n",
            "Training Step: 2010  | total loss: \u001b[1m\u001b[32m0.43302\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 670 | loss: 0.43302 - acc: 0.8822 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2011  | total loss: \u001b[1m\u001b[32m0.43439\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 671 | loss: 0.43439 - acc: 0.8815 -- iter: 08/22\n",
            "Training Step: 2012  | total loss: \u001b[1m\u001b[32m0.45055\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 671 | loss: 0.45055 - acc: 0.8767 -- iter: 16/22\n",
            "Training Step: 2013  | total loss: \u001b[1m\u001b[32m0.45980\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 671 | loss: 0.45980 - acc: 0.8723 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2014  | total loss: \u001b[1m\u001b[32m0.45663\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 672 | loss: 0.45663 - acc: 0.8601 -- iter: 08/22\n",
            "Training Step: 2015  | total loss: \u001b[1m\u001b[32m0.45409\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 672 | loss: 0.45409 - acc: 0.8741 -- iter: 16/22\n",
            "Training Step: 2016  | total loss: \u001b[1m\u001b[32m0.46281\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 672 | loss: 0.46281 - acc: 0.8700 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2017  | total loss: \u001b[1m\u001b[32m0.47092\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 673 | loss: 0.47092 - acc: 0.8663 -- iter: 08/22\n",
            "Training Step: 2018  | total loss: \u001b[1m\u001b[32m0.46313\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 673 | loss: 0.46313 - acc: 0.8797 -- iter: 16/22\n",
            "Training Step: 2019  | total loss: \u001b[1m\u001b[32m0.46298\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 673 | loss: 0.46298 - acc: 0.8667 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2020  | total loss: \u001b[1m\u001b[32m0.47088\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 674 | loss: 0.47088 - acc: 0.8634 -- iter: 08/22\n",
            "Training Step: 2021  | total loss: \u001b[1m\u001b[32m0.47791\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 674 | loss: 0.47791 - acc: 0.8604 -- iter: 16/22\n",
            "Training Step: 2022  | total loss: \u001b[1m\u001b[32m0.45987\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 674 | loss: 0.45987 - acc: 0.8744 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2023  | total loss: \u001b[1m\u001b[32m0.46932\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 675 | loss: 0.46932 - acc: 0.8619 -- iter: 08/22\n",
            "Training Step: 2024  | total loss: \u001b[1m\u001b[32m0.47632\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 675 | loss: 0.47632 - acc: 0.8591 -- iter: 16/22\n",
            "Training Step: 2025  | total loss: \u001b[1m\u001b[32m0.47621\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 675 | loss: 0.47621 - acc: 0.8732 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2026  | total loss: \u001b[1m\u001b[32m0.46561\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 676 | loss: 0.46561 - acc: 0.8608 -- iter: 08/22\n",
            "Training Step: 2027  | total loss: \u001b[1m\u001b[32m0.47166\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 676 | loss: 0.47166 - acc: 0.8623 -- iter: 16/22\n",
            "Training Step: 2028  | total loss: \u001b[1m\u001b[32m0.47180\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 676 | loss: 0.47180 - acc: 0.8760 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2029  | total loss: \u001b[1m\u001b[32m0.45860\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 677 | loss: 0.45860 - acc: 0.8718 -- iter: 08/22\n",
            "Training Step: 2030  | total loss: \u001b[1m\u001b[32m0.44540\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 677 | loss: 0.44540 - acc: 0.8846 -- iter: 16/22\n",
            "Training Step: 2031  | total loss: \u001b[1m\u001b[32m0.46748\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 677 | loss: 0.46748 - acc: 0.8711 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2032  | total loss: \u001b[1m\u001b[32m0.45467\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 678 | loss: 0.45467 - acc: 0.8673 -- iter: 08/22\n",
            "Training Step: 2033  | total loss: \u001b[1m\u001b[32m0.45819\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 678 | loss: 0.45819 - acc: 0.8473 -- iter: 16/22\n",
            "Training Step: 2034  | total loss: \u001b[1m\u001b[32m0.46810\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 678 | loss: 0.46810 - acc: 0.8500 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2035  | total loss: \u001b[1m\u001b[32m0.45318\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 679 | loss: 0.45318 - acc: 0.8650 -- iter: 08/22\n",
            "Training Step: 2036  | total loss: \u001b[1m\u001b[32m0.45668\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 679 | loss: 0.45668 - acc: 0.8452 -- iter: 16/22\n",
            "Training Step: 2037  | total loss: \u001b[1m\u001b[32m0.46422\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 679 | loss: 0.46422 - acc: 0.8607 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2038  | total loss: \u001b[1m\u001b[32m0.47599\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 680 | loss: 0.47599 - acc: 0.8371 -- iter: 08/22\n",
            "Training Step: 2039  | total loss: \u001b[1m\u001b[32m0.45419\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 680 | loss: 0.45419 - acc: 0.8534 -- iter: 16/22\n",
            "Training Step: 2040  | total loss: \u001b[1m\u001b[32m0.46190\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 680 | loss: 0.46190 - acc: 0.8681 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2041  | total loss: \u001b[1m\u001b[32m0.48269\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 681 | loss: 0.48269 - acc: 0.8646 -- iter: 08/22\n",
            "Training Step: 2042  | total loss: \u001b[1m\u001b[32m0.48362\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 681 | loss: 0.48362 - acc: 0.8531 -- iter: 16/22\n",
            "Training Step: 2043  | total loss: \u001b[1m\u001b[32m0.45929\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 681 | loss: 0.45929 - acc: 0.8678 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2044  | total loss: \u001b[1m\u001b[32m1.22647\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 682 | loss: 1.22647 - acc: 0.8144 -- iter: 08/22\n",
            "Training Step: 2045  | total loss: \u001b[1m\u001b[32m1.14395\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 682 | loss: 1.14395 - acc: 0.8163 -- iter: 16/22\n",
            "Training Step: 2046  | total loss: \u001b[1m\u001b[32m1.07324\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 682 | loss: 1.07324 - acc: 0.8346 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2047  | total loss: \u001b[1m\u001b[32m1.01549\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 683 | loss: 1.01549 - acc: 0.8262 -- iter: 08/22\n",
            "Training Step: 2048  | total loss: \u001b[1m\u001b[32m1.25935\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 683 | loss: 1.25935 - acc: 0.7936 -- iter: 16/22\n",
            "Training Step: 2049  | total loss: \u001b[1m\u001b[32m1.20398\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 683 | loss: 1.20398 - acc: 0.7975 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2050  | total loss: \u001b[1m\u001b[32m1.10570\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 684 | loss: 1.10570 - acc: 0.8053 -- iter: 08/22\n",
            "Training Step: 2051  | total loss: \u001b[1m\u001b[32m1.04373\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 684 | loss: 1.04373 - acc: 0.8123 -- iter: 16/22\n",
            "Training Step: 2052  | total loss: \u001b[1m\u001b[32m1.13491\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 684 | loss: 1.13491 - acc: 0.7644 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2053  | total loss: \u001b[1m\u001b[32m1.09571\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 685 | loss: 1.09571 - acc: 0.7546 -- iter: 08/22\n",
            "Training Step: 2054  | total loss: \u001b[1m\u001b[32m1.02522\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 685 | loss: 1.02522 - acc: 0.7791 -- iter: 16/22\n",
            "Training Step: 2055  | total loss: \u001b[1m\u001b[32m0.95185\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 685 | loss: 0.95185 - acc: 0.7887 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2056  | total loss: \u001b[1m\u001b[32m0.93088\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 686 | loss: 0.93088 - acc: 0.7765 -- iter: 08/22\n",
            "Training Step: 2057  | total loss: \u001b[1m\u001b[32m0.89614\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 686 | loss: 0.89614 - acc: 0.7822 -- iter: 16/22\n",
            "Training Step: 2058  | total loss: \u001b[1m\u001b[32m0.85554\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 686 | loss: 0.85554 - acc: 0.7915 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2059  | total loss: \u001b[1m\u001b[32m0.80138\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 687 | loss: 0.80138 - acc: 0.7998 -- iter: 08/22\n",
            "Training Step: 2060  | total loss: \u001b[1m\u001b[32m0.77950\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 687 | loss: 0.77950 - acc: 0.8032 -- iter: 16/22\n",
            "Training Step: 2061  | total loss: \u001b[1m\u001b[32m0.76032\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 687 | loss: 0.76032 - acc: 0.8062 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2062  | total loss: \u001b[1m\u001b[32m0.70420\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 688 | loss: 0.70420 - acc: 0.8256 -- iter: 08/22\n",
            "Training Step: 2063  | total loss: \u001b[1m\u001b[32m0.69391\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 688 | loss: 0.69391 - acc: 0.8180 -- iter: 16/22\n",
            "Training Step: 2064  | total loss: \u001b[1m\u001b[32m0.68311\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 688 | loss: 0.68311 - acc: 0.8195 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2065  | total loss: \u001b[1m\u001b[32m0.66716\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 689 | loss: 0.66716 - acc: 0.8376 -- iter: 08/22\n",
            "Training Step: 2066  | total loss: \u001b[1m\u001b[32m0.63804\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 689 | loss: 0.63804 - acc: 0.8288 -- iter: 16/22\n",
            "Training Step: 2067  | total loss: \u001b[1m\u001b[32m0.62127\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 689 | loss: 0.62127 - acc: 0.8335 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2068  | total loss: \u001b[1m\u001b[32m0.61131\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 690 | loss: 0.61131 - acc: 0.8501 -- iter: 08/22\n",
            "Training Step: 2069  | total loss: \u001b[1m\u001b[32m0.59266\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 690 | loss: 0.59266 - acc: 0.8484 -- iter: 16/22\n",
            "Training Step: 2070  | total loss: \u001b[1m\u001b[32m0.57929\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 690 | loss: 0.57929 - acc: 0.8511 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2071  | total loss: \u001b[1m\u001b[32m0.56713\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 691 | loss: 0.56713 - acc: 0.8535 -- iter: 08/22\n",
            "Training Step: 2072  | total loss: \u001b[1m\u001b[32m0.55268\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 691 | loss: 0.55268 - acc: 0.8515 -- iter: 16/22\n",
            "Training Step: 2073  | total loss: \u001b[1m\u001b[32m0.55278\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 691 | loss: 0.55278 - acc: 0.8663 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2074  | total loss: \u001b[1m\u001b[32m0.53791\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 692 | loss: 0.53791 - acc: 0.8547 -- iter: 08/22\n",
            "Training Step: 2075  | total loss: \u001b[1m\u001b[32m0.52521\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 692 | loss: 0.52521 - acc: 0.8567 -- iter: 16/22\n",
            "Training Step: 2076  | total loss: \u001b[1m\u001b[32m0.52789\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 692 | loss: 0.52789 - acc: 0.8710 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2077  | total loss: \u001b[1m\u001b[32m0.53949\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 693 | loss: 0.53949 - acc: 0.8339 -- iter: 08/22\n",
            "Training Step: 2078  | total loss: \u001b[1m\u001b[32m0.52054\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 693 | loss: 0.52054 - acc: 0.8505 -- iter: 16/22\n",
            "Training Step: 2079  | total loss: \u001b[1m\u001b[32m0.50775\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 693 | loss: 0.50775 - acc: 0.8655 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2080  | total loss: \u001b[1m\u001b[32m0.52107\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 694 | loss: 0.52107 - acc: 0.8289 -- iter: 08/22\n",
            "Training Step: 2081  | total loss: \u001b[1m\u001b[32m0.52349\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 694 | loss: 0.52349 - acc: 0.8127 -- iter: 16/22\n",
            "Training Step: 2082  | total loss: \u001b[1m\u001b[32m0.51791\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 694 | loss: 0.51791 - acc: 0.8189 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2083  | total loss: \u001b[1m\u001b[32m0.50048\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 695 | loss: 0.50048 - acc: 0.8370 -- iter: 08/22\n",
            "Training Step: 2084  | total loss: \u001b[1m\u001b[32m0.50470\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 695 | loss: 0.50470 - acc: 0.8200 -- iter: 16/22\n",
            "Training Step: 2085  | total loss: \u001b[1m\u001b[32m0.52661\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 695 | loss: 0.52661 - acc: 0.8047 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2086  | total loss: \u001b[1m\u001b[32m0.51593\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 696 | loss: 0.51593 - acc: 0.8117 -- iter: 08/22\n",
            "Training Step: 2087  | total loss: \u001b[1m\u001b[32m0.48960\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 696 | loss: 0.48960 - acc: 0.8305 -- iter: 16/22\n",
            "Training Step: 2088  | total loss: \u001b[1m\u001b[32m0.51281\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 696 | loss: 0.51281 - acc: 0.8308 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2089  | total loss: \u001b[1m\u001b[32m0.52073\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 697 | loss: 0.52073 - acc: 0.8311 -- iter: 08/22\n",
            "Training Step: 2090  | total loss: \u001b[1m\u001b[32m0.51096\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 697 | loss: 0.51096 - acc: 0.8355 -- iter: 16/22\n",
            "Training Step: 2091  | total loss: \u001b[1m\u001b[32m0.49420\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 697 | loss: 0.49420 - acc: 0.8519 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2092  | total loss: \u001b[1m\u001b[32m0.50367\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 698 | loss: 0.50367 - acc: 0.8501 -- iter: 08/22\n",
            "Training Step: 2093  | total loss: \u001b[1m\u001b[32m0.51834\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 698 | loss: 0.51834 - acc: 0.8317 -- iter: 16/22\n",
            "Training Step: 2094  | total loss: \u001b[1m\u001b[32m0.50205\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 698 | loss: 0.50205 - acc: 0.8485 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2095  | total loss: \u001b[1m\u001b[32m0.48803\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 699 | loss: 0.48803 - acc: 0.8637 -- iter: 08/22\n",
            "Training Step: 2096  | total loss: \u001b[1m\u001b[32m0.50409\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 699 | loss: 0.50409 - acc: 0.8607 -- iter: 16/22\n",
            "Training Step: 2097  | total loss: \u001b[1m\u001b[32m0.49954\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 699 | loss: 0.49954 - acc: 0.8579 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2098  | total loss: \u001b[1m\u001b[32m0.50871\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 700 | loss: 0.50871 - acc: 0.8721 -- iter: 08/22\n",
            "Training Step: 2099  | total loss: \u001b[1m\u001b[32m0.48439\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 700 | loss: 0.48439 - acc: 0.8849 -- iter: 16/22\n",
            "Training Step: 2100  | total loss: \u001b[1m\u001b[32m0.48162\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 700 | loss: 0.48162 - acc: 0.8798 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2101  | total loss: \u001b[1m\u001b[32m0.48405\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 701 | loss: 0.48405 - acc: 0.8918 -- iter: 08/22\n",
            "Training Step: 2102  | total loss: \u001b[1m\u001b[32m0.48028\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 701 | loss: 0.48028 - acc: 0.8901 -- iter: 16/22\n",
            "Training Step: 2103  | total loss: \u001b[1m\u001b[32m0.46924\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 701 | loss: 0.46924 - acc: 0.9011 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2104  | total loss: \u001b[1m\u001b[32m0.47273\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 702 | loss: 0.47273 - acc: 0.9110 -- iter: 08/22\n",
            "Training Step: 2105  | total loss: \u001b[1m\u001b[32m0.46535\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 702 | loss: 0.46535 - acc: 0.9199 -- iter: 16/22\n",
            "Training Step: 2106  | total loss: \u001b[1m\u001b[32m0.47217\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 702 | loss: 0.47217 - acc: 0.9279 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2107  | total loss: \u001b[1m\u001b[32m0.46082\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 703 | loss: 0.46082 - acc: 0.9226 -- iter: 08/22\n",
            "Training Step: 2108  | total loss: \u001b[1m\u001b[32m0.45426\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 703 | loss: 0.45426 - acc: 0.9303 -- iter: 16/22\n",
            "Training Step: 2109  | total loss: \u001b[1m\u001b[32m0.44688\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 703 | loss: 0.44688 - acc: 0.9373 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2110  | total loss: \u001b[1m\u001b[32m0.44687\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 704 | loss: 0.44687 - acc: 0.9436 -- iter: 08/22\n",
            "Training Step: 2111  | total loss: \u001b[1m\u001b[32m0.44760\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 704 | loss: 0.44760 - acc: 0.9367 -- iter: 16/22\n",
            "Training Step: 2112  | total loss: \u001b[1m\u001b[32m0.44045\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 704 | loss: 0.44045 - acc: 0.9431 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2113  | total loss: \u001b[1m\u001b[32m0.43932\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 705 | loss: 0.43932 - acc: 0.9487 -- iter: 08/22\n",
            "Training Step: 2114  | total loss: \u001b[1m\u001b[32m0.44007\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 705 | loss: 0.44007 - acc: 0.9539 -- iter: 16/22\n",
            "Training Step: 2115  | total loss: \u001b[1m\u001b[32m0.43719\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 705 | loss: 0.43719 - acc: 0.9335 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2116  | total loss: \u001b[1m\u001b[32m0.43645\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 706 | loss: 0.43645 - acc: 0.9401 -- iter: 08/22\n",
            "Training Step: 2117  | total loss: \u001b[1m\u001b[32m0.45048\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 706 | loss: 0.45048 - acc: 0.9295 -- iter: 16/22\n",
            "Training Step: 2118  | total loss: \u001b[1m\u001b[32m0.43379\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 706 | loss: 0.43379 - acc: 0.9365 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2119  | total loss: \u001b[1m\u001b[32m0.43631\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 707 | loss: 0.43631 - acc: 0.9304 -- iter: 08/22\n",
            "Training Step: 2120  | total loss: \u001b[1m\u001b[32m0.45025\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 707 | loss: 0.45025 - acc: 0.9207 -- iter: 16/22\n",
            "Training Step: 2121  | total loss: \u001b[1m\u001b[32m0.45063\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 707 | loss: 0.45063 - acc: 0.8953 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2122  | total loss: \u001b[1m\u001b[32m0.44463\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 708 | loss: 0.44463 - acc: 0.9057 -- iter: 08/22\n",
            "Training Step: 2123  | total loss: \u001b[1m\u001b[32m0.44418\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 708 | loss: 0.44418 - acc: 0.9152 -- iter: 16/22\n",
            "Training Step: 2124  | total loss: \u001b[1m\u001b[32m0.44492\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 708 | loss: 0.44492 - acc: 0.8903 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2125  | total loss: \u001b[1m\u001b[32m0.44493\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 709 | loss: 0.44493 - acc: 0.9013 -- iter: 08/22\n",
            "Training Step: 2126  | total loss: \u001b[1m\u001b[32m0.43702\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 709 | loss: 0.43702 - acc: 0.8987 -- iter: 16/22\n",
            "Training Step: 2127  | total loss: \u001b[1m\u001b[32m0.43999\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 709 | loss: 0.43999 - acc: 0.8963 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2128  | total loss: \u001b[1m\u001b[32m0.44043\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 710 | loss: 0.44043 - acc: 0.9067 -- iter: 08/22\n",
            "Training Step: 2129  | total loss: \u001b[1m\u001b[32m0.45034\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 710 | loss: 0.45034 - acc: 0.9160 -- iter: 16/22\n",
            "Training Step: 2130  | total loss: \u001b[1m\u001b[32m0.45000\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 710 | loss: 0.45000 - acc: 0.8994 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2131  | total loss: \u001b[1m\u001b[32m0.43619\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 711 | loss: 0.43619 - acc: 0.9095 -- iter: 08/22\n",
            "Training Step: 2132  | total loss: \u001b[1m\u001b[32m0.44613\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 711 | loss: 0.44613 - acc: 0.9185 -- iter: 16/22\n",
            "Training Step: 2133  | total loss: \u001b[1m\u001b[32m0.42554\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 711 | loss: 0.42554 - acc: 0.9267 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2134  | total loss: \u001b[1m\u001b[32m0.44033\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 712 | loss: 0.44033 - acc: 0.9215 -- iter: 08/22\n",
            "Training Step: 2135  | total loss: \u001b[1m\u001b[32m0.43678\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 712 | loss: 0.43678 - acc: 0.9168 -- iter: 16/22\n",
            "Training Step: 2136  | total loss: \u001b[1m\u001b[32m0.41703\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 712 | loss: 0.41703 - acc: 0.9252 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2137  | total loss: \u001b[1m\u001b[32m0.41779\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 713 | loss: 0.41779 - acc: 0.9326 -- iter: 08/22\n",
            "Training Step: 2138  | total loss: \u001b[1m\u001b[32m0.40654\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 713 | loss: 0.40654 - acc: 0.9269 -- iter: 16/22\n",
            "Training Step: 2139  | total loss: \u001b[1m\u001b[32m0.41900\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 713 | loss: 0.41900 - acc: 0.9092 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2140  | total loss: \u001b[1m\u001b[32m0.41917\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 714 | loss: 0.41917 - acc: 0.9183 -- iter: 08/22\n",
            "Training Step: 2141  | total loss: \u001b[1m\u001b[32m0.41404\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 714 | loss: 0.41404 - acc: 0.9264 -- iter: 16/22\n",
            "Training Step: 2142  | total loss: \u001b[1m\u001b[32m0.40599\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 714 | loss: 0.40599 - acc: 0.9213 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2143  | total loss: \u001b[1m\u001b[32m0.41950\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 715 | loss: 0.41950 - acc: 0.9042 -- iter: 08/22\n",
            "Training Step: 2144  | total loss: \u001b[1m\u001b[32m0.41402\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 715 | loss: 0.41402 - acc: 0.9138 -- iter: 16/22\n",
            "Training Step: 2145  | total loss: \u001b[1m\u001b[32m0.41646\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 715 | loss: 0.41646 - acc: 0.9057 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2146  | total loss: \u001b[1m\u001b[32m0.41960\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 716 | loss: 0.41960 - acc: 0.9026 -- iter: 08/22\n",
            "Training Step: 2147  | total loss: \u001b[1m\u001b[32m0.41456\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 716 | loss: 0.41456 - acc: 0.8999 -- iter: 16/22\n",
            "Training Step: 2148  | total loss: \u001b[1m\u001b[32m0.41672\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 716 | loss: 0.41672 - acc: 0.8932 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2149  | total loss: \u001b[1m\u001b[32m0.41715\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 717 | loss: 0.41715 - acc: 0.8872 -- iter: 08/22\n",
            "Training Step: 2150  | total loss: \u001b[1m\u001b[32m0.42479\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 717 | loss: 0.42479 - acc: 0.8860 -- iter: 16/22\n",
            "Training Step: 2151  | total loss: \u001b[1m\u001b[32m0.41554\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 717 | loss: 0.41554 - acc: 0.8849 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2152  | total loss: \u001b[1m\u001b[32m0.41605\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 718 | loss: 0.41605 - acc: 0.8798 -- iter: 08/22\n",
            "Training Step: 2153  | total loss: \u001b[1m\u001b[32m0.41381\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 718 | loss: 0.41381 - acc: 0.8918 -- iter: 16/22\n",
            "Training Step: 2154  | total loss: \u001b[1m\u001b[32m0.41548\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 718 | loss: 0.41548 - acc: 0.8776 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2155  | total loss: \u001b[1m\u001b[32m0.41515\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 719 | loss: 0.41515 - acc: 0.8773 -- iter: 08/22\n",
            "Training Step: 2156  | total loss: \u001b[1m\u001b[32m0.95081\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 719 | loss: 0.95081 - acc: 0.8063 -- iter: 16/22\n",
            "Training Step: 2157  | total loss: \u001b[1m\u001b[32m0.89471\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 719 | loss: 0.89471 - acc: 0.8256 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2158  | total loss: \u001b[1m\u001b[32m0.84147\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 720 | loss: 0.84147 - acc: 0.8181 -- iter: 08/22\n",
            "Training Step: 2159  | total loss: \u001b[1m\u001b[32m0.80575\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 720 | loss: 0.80575 - acc: 0.8238 -- iter: 16/22\n",
            "Training Step: 2160  | total loss: \u001b[1m\u001b[32m0.76392\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 720 | loss: 0.76392 - acc: 0.8414 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2161  | total loss: \u001b[1m\u001b[32m0.72005\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 721 | loss: 0.72005 - acc: 0.8573 -- iter: 08/22\n",
            "Training Step: 2162  | total loss: \u001b[1m\u001b[32m0.67951\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 721 | loss: 0.67951 - acc: 0.8715 -- iter: 16/22\n",
            "Training Step: 2163  | total loss: \u001b[1m\u001b[32m0.66990\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 721 | loss: 0.66990 - acc: 0.8469 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2164  | total loss: \u001b[1m\u001b[32m0.63516\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 722 | loss: 0.63516 - acc: 0.8622 -- iter: 08/22\n",
            "Training Step: 2165  | total loss: \u001b[1m\u001b[32m0.62688\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 722 | loss: 0.62688 - acc: 0.8593 -- iter: 16/22\n",
            "Training Step: 2166  | total loss: \u001b[1m\u001b[32m0.60036\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 722 | loss: 0.60036 - acc: 0.8734 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2167  | total loss: \u001b[1m\u001b[32m0.57699\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 723 | loss: 0.57699 - acc: 0.8610 -- iter: 08/22\n",
            "Training Step: 2168  | total loss: \u001b[1m\u001b[32m0.57420\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 723 | loss: 0.57420 - acc: 0.8583 -- iter: 16/22\n",
            "Training Step: 2169  | total loss: \u001b[1m\u001b[32m0.57484\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 723 | loss: 0.57484 - acc: 0.8391 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2170  | total loss: \u001b[1m\u001b[32m0.55720\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 724 | loss: 0.55720 - acc: 0.8427 -- iter: 08/22\n",
            "Training Step: 2171  | total loss: \u001b[1m\u001b[32m0.53208\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 724 | loss: 0.53208 - acc: 0.8584 -- iter: 16/22\n",
            "Training Step: 2172  | total loss: \u001b[1m\u001b[32m0.53696\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 724 | loss: 0.53696 - acc: 0.8392 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2173  | total loss: \u001b[1m\u001b[32m0.52568\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 725 | loss: 0.52568 - acc: 0.8220 -- iter: 08/22\n",
            "Training Step: 2174  | total loss: \u001b[1m\u001b[32m0.51723\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 725 | loss: 0.51723 - acc: 0.8398 -- iter: 16/22\n",
            "Training Step: 2175  | total loss: \u001b[1m\u001b[32m0.50343\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 725 | loss: 0.50343 - acc: 0.8433 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2176  | total loss: \u001b[1m\u001b[32m0.49518\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 726 | loss: 0.49518 - acc: 0.8256 -- iter: 08/22\n",
            "Training Step: 2177  | total loss: \u001b[1m\u001b[32m0.49932\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 726 | loss: 0.49932 - acc: 0.8264 -- iter: 16/22\n",
            "Training Step: 2178  | total loss: \u001b[1m\u001b[32m0.47866\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 726 | loss: 0.47866 - acc: 0.8313 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2179  | total loss: \u001b[1m\u001b[32m0.47456\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 727 | loss: 0.47456 - acc: 0.8356 -- iter: 08/22\n",
            "Training Step: 2180  | total loss: \u001b[1m\u001b[32m0.48070\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 727 | loss: 0.48070 - acc: 0.8354 -- iter: 16/22\n",
            "Training Step: 2181  | total loss: \u001b[1m\u001b[32m0.46101\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 727 | loss: 0.46101 - acc: 0.8519 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2182  | total loss: \u001b[1m\u001b[32m0.44558\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 728 | loss: 0.44558 - acc: 0.8542 -- iter: 08/22\n",
            "Training Step: 2183  | total loss: \u001b[1m\u001b[32m0.46187\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 728 | loss: 0.46187 - acc: 0.8438 -- iter: 16/22\n",
            "Training Step: 2184  | total loss: \u001b[1m\u001b[32m0.44410\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 728 | loss: 0.44410 - acc: 0.8594 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2185  | total loss: \u001b[1m\u001b[32m0.46162\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 729 | loss: 0.46162 - acc: 0.8568 -- iter: 08/22\n",
            "Training Step: 2186  | total loss: \u001b[1m\u001b[32m0.43877\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 729 | loss: 0.43877 - acc: 0.8711 -- iter: 16/22\n",
            "Training Step: 2187  | total loss: \u001b[1m\u001b[32m0.43757\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 729 | loss: 0.43757 - acc: 0.8590 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2188  | total loss: \u001b[1m\u001b[32m0.45557\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 730 | loss: 0.45557 - acc: 0.8564 -- iter: 08/22\n",
            "Training Step: 2189  | total loss: \u001b[1m\u001b[32m0.45618\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 730 | loss: 0.45618 - acc: 0.8375 -- iter: 16/22\n",
            "Training Step: 2190  | total loss: \u001b[1m\u001b[32m0.45491\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 730 | loss: 0.45491 - acc: 0.8412 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2191  | total loss: \u001b[1m\u001b[32m0.44245\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 731 | loss: 0.44245 - acc: 0.8571 -- iter: 08/22\n",
            "Training Step: 2192  | total loss: \u001b[1m\u001b[32m0.44418\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 731 | loss: 0.44418 - acc: 0.8380 -- iter: 16/22\n",
            "Training Step: 2193  | total loss: \u001b[1m\u001b[32m0.42992\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 731 | loss: 0.42992 - acc: 0.8542 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2194  | total loss: \u001b[1m\u001b[32m0.42087\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 732 | loss: 0.42087 - acc: 0.8563 -- iter: 08/22\n",
            "Training Step: 2195  | total loss: \u001b[1m\u001b[32m0.43377\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 732 | loss: 0.43377 - acc: 0.8457 -- iter: 16/22\n",
            "Training Step: 2196  | total loss: \u001b[1m\u001b[32m0.42042\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 732 | loss: 0.42042 - acc: 0.8611 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2197  | total loss: \u001b[1m\u001b[32m0.42001\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 733 | loss: 0.42001 - acc: 0.8583 -- iter: 08/22\n",
            "Training Step: 2198  | total loss: \u001b[1m\u001b[32m0.42017\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 733 | loss: 0.42017 - acc: 0.8725 -- iter: 16/22\n",
            "Training Step: 2199  | total loss: \u001b[1m\u001b[32m0.41590\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 733 | loss: 0.41590 - acc: 0.8603 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2200  | total loss: \u001b[1m\u001b[32m0.41577\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 734 | loss: 0.41577 - acc: 0.8576 -- iter: 08/22\n",
            "Training Step: 2201  | total loss: \u001b[1m\u001b[32m0.40955\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 734 | loss: 0.40955 - acc: 0.8385 -- iter: 16/22\n",
            "Training Step: 2202  | total loss: \u001b[1m\u001b[32m0.40394\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 734 | loss: 0.40394 - acc: 0.8546 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2203  | total loss: \u001b[1m\u001b[32m0.41248\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 735 | loss: 0.41248 - acc: 0.8567 -- iter: 08/22\n",
            "Training Step: 2204  | total loss: \u001b[1m\u001b[32m0.40622\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 735 | loss: 0.40622 - acc: 0.8377 -- iter: 16/22\n",
            "Training Step: 2205  | total loss: \u001b[1m\u001b[32m0.41157\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 735 | loss: 0.41157 - acc: 0.8206 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2206  | total loss: \u001b[1m\u001b[32m0.42578\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 736 | loss: 0.42578 - acc: 0.8260 -- iter: 08/22\n",
            "Training Step: 2207  | total loss: \u001b[1m\u001b[32m0.40361\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 736 | loss: 0.40361 - acc: 0.8434 -- iter: 16/22\n",
            "Training Step: 2208  | total loss: \u001b[1m\u001b[32m0.40886\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 736 | loss: 0.40886 - acc: 0.8257 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2209  | total loss: \u001b[1m\u001b[32m0.40012\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 737 | loss: 0.40012 - acc: 0.8432 -- iter: 08/22\n",
            "Training Step: 2210  | total loss: \u001b[1m\u001b[32m0.39075\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 737 | loss: 0.39075 - acc: 0.8463 -- iter: 16/22\n",
            "Training Step: 2211  | total loss: \u001b[1m\u001b[32m0.40663\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 737 | loss: 0.40663 - acc: 0.8367 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2212  | total loss: \u001b[1m\u001b[32m0.39815\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 738 | loss: 0.39815 - acc: 0.8530 -- iter: 08/22\n",
            "Training Step: 2213  | total loss: \u001b[1m\u001b[32m0.38667\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 738 | loss: 0.38667 - acc: 0.8677 -- iter: 16/22\n",
            "Training Step: 2214  | total loss: \u001b[1m\u001b[32m0.38367\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 738 | loss: 0.38367 - acc: 0.8685 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2215  | total loss: \u001b[1m\u001b[32m0.39774\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 739 | loss: 0.39774 - acc: 0.8566 -- iter: 08/22\n",
            "Training Step: 2216  | total loss: \u001b[1m\u001b[32m1.17268\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 739 | loss: 1.17268 - acc: 0.8210 -- iter: 16/22\n",
            "Training Step: 2217  | total loss: \u001b[1m\u001b[32m1.07640\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 739 | loss: 1.07640 - acc: 0.8389 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2218  | total loss: \u001b[1m\u001b[32m1.01900\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 740 | loss: 1.01900 - acc: 0.8300 -- iter: 08/22\n",
            "Training Step: 2219  | total loss: \u001b[1m\u001b[32m0.96035\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 740 | loss: 0.96035 - acc: 0.8470 -- iter: 16/22\n",
            "Training Step: 2220  | total loss: \u001b[1m\u001b[32m0.88554\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 740 | loss: 0.88554 - acc: 0.8623 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2221  | total loss: \u001b[1m\u001b[32m0.83299\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 741 | loss: 0.83299 - acc: 0.8760 -- iter: 08/22\n",
            "Training Step: 2222  | total loss: \u001b[1m\u001b[32m0.80434\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 741 | loss: 0.80434 - acc: 0.8759 -- iter: 16/22\n",
            "Training Step: 2223  | total loss: \u001b[1m\u001b[32m0.75171\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 741 | loss: 0.75171 - acc: 0.8759 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2224  | total loss: \u001b[1m\u001b[32m0.71246\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 742 | loss: 0.71246 - acc: 0.8883 -- iter: 08/22\n",
            "Training Step: 2225  | total loss: \u001b[1m\u001b[32m0.68073\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 742 | loss: 0.68073 - acc: 0.8828 -- iter: 16/22\n",
            "Training Step: 2226  | total loss: \u001b[1m\u001b[32m0.64877\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 742 | loss: 0.64877 - acc: 0.8945 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2227  | total loss: \u001b[1m\u001b[32m0.62745\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 743 | loss: 0.62745 - acc: 0.8925 -- iter: 08/22\n",
            "Training Step: 2228  | total loss: \u001b[1m\u001b[32m0.60416\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 743 | loss: 0.60416 - acc: 0.8866 -- iter: 16/22\n",
            "Training Step: 2229  | total loss: \u001b[1m\u001b[32m0.60243\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 743 | loss: 0.60243 - acc: 0.8813 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2230  | total loss: \u001b[1m\u001b[32m0.59016\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 744 | loss: 0.59016 - acc: 0.8807 -- iter: 08/22\n",
            "Training Step: 2231  | total loss: \u001b[1m\u001b[32m0.54827\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 744 | loss: 0.54827 - acc: 0.8926 -- iter: 16/22\n",
            "Training Step: 2232  | total loss: \u001b[1m\u001b[32m0.55192\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 744 | loss: 0.55192 - acc: 0.8867 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2233  | total loss: \u001b[1m\u001b[32m0.51434\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 745 | loss: 0.51434 - acc: 0.8980 -- iter: 08/22\n",
            "Training Step: 2234  | total loss: \u001b[1m\u001b[32m0.49580\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 745 | loss: 0.49580 - acc: 0.8957 -- iter: 16/22\n",
            "Training Step: 2235  | total loss: \u001b[1m\u001b[32m0.50889\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 745 | loss: 0.50889 - acc: 0.8936 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2236  | total loss: \u001b[1m\u001b[32m0.47555\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 746 | loss: 0.47555 - acc: 0.9043 -- iter: 08/22\n",
            "Training Step: 2237  | total loss: \u001b[1m\u001b[32m0.46223\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 746 | loss: 0.46223 - acc: 0.9138 -- iter: 16/22\n",
            "Training Step: 2238  | total loss: \u001b[1m\u001b[32m0.44296\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 746 | loss: 0.44296 - acc: 0.9100 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2239  | total loss: \u001b[1m\u001b[32m0.45448\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 747 | loss: 0.45448 - acc: 0.9065 -- iter: 08/22\n",
            "Training Step: 2240  | total loss: \u001b[1m\u001b[32m0.44310\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 747 | loss: 0.44310 - acc: 0.9158 -- iter: 16/22\n",
            "Training Step: 2241  | total loss: \u001b[1m\u001b[32m0.42810\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 747 | loss: 0.42810 - acc: 0.9076 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2242  | total loss: \u001b[1m\u001b[32m0.42092\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 748 | loss: 0.42092 - acc: 0.9043 -- iter: 08/22\n",
            "Training Step: 2243  | total loss: \u001b[1m\u001b[32m0.42928\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 748 | loss: 0.42928 - acc: 0.9139 -- iter: 16/22\n",
            "Training Step: 2244  | total loss: \u001b[1m\u001b[32m0.41547\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 748 | loss: 0.41547 - acc: 0.9058 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2245  | total loss: \u001b[1m\u001b[32m0.40827\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 749 | loss: 0.40827 - acc: 0.8986 -- iter: 08/22\n",
            "Training Step: 2246  | total loss: \u001b[1m\u001b[32m0.40392\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 749 | loss: 0.40392 - acc: 0.9087 -- iter: 16/22\n",
            "Training Step: 2247  | total loss: \u001b[1m\u001b[32m0.40892\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 749 | loss: 0.40892 - acc: 0.9053 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2248  | total loss: \u001b[1m\u001b[32m0.40215\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 750 | loss: 0.40215 - acc: 0.8981 -- iter: 08/22\n",
            "Training Step: 2249  | total loss: \u001b[1m\u001b[32m0.39172\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 750 | loss: 0.39172 - acc: 0.9083 -- iter: 16/22\n",
            "Training Step: 2250  | total loss: \u001b[1m\u001b[32m0.40371\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 750 | loss: 0.40371 - acc: 0.9050 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2251  | total loss: \u001b[1m\u001b[32m0.39702\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 751 | loss: 0.39702 - acc: 0.9145 -- iter: 08/22\n",
            "Training Step: 2252  | total loss: \u001b[1m\u001b[32m0.38706\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 751 | loss: 0.38706 - acc: 0.9230 -- iter: 16/22\n",
            "Training Step: 2253  | total loss: \u001b[1m\u001b[32m0.38076\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 751 | loss: 0.38076 - acc: 0.9307 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2254  | total loss: \u001b[1m\u001b[32m0.37057\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 752 | loss: 0.37057 - acc: 0.9377 -- iter: 08/22\n",
            "Training Step: 2255  | total loss: \u001b[1m\u001b[32m0.38813\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 752 | loss: 0.38813 - acc: 0.9314 -- iter: 16/22\n",
            "Training Step: 2256  | total loss: \u001b[1m\u001b[32m0.38163\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 752 | loss: 0.38163 - acc: 0.9383 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2257  | total loss: \u001b[1m\u001b[32m0.36400\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 753 | loss: 0.36400 - acc: 0.9444 -- iter: 08/22\n",
            "Training Step: 2258  | total loss: \u001b[1m\u001b[32m0.38684\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 753 | loss: 0.38684 - acc: 0.9375 -- iter: 16/22\n",
            "Training Step: 2259  | total loss: \u001b[1m\u001b[32m0.37994\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 753 | loss: 0.37994 - acc: 0.9437 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2260  | total loss: \u001b[1m\u001b[32m0.36231\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 754 | loss: 0.36231 - acc: 0.9494 -- iter: 08/22\n",
            "Training Step: 2261  | total loss: \u001b[1m\u001b[32m0.35487\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 754 | loss: 0.35487 - acc: 0.9544 -- iter: 16/22\n",
            "Training Step: 2262  | total loss: \u001b[1m\u001b[32m0.37416\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 754 | loss: 0.37416 - acc: 0.9465 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2263  | total loss: \u001b[1m\u001b[32m0.36637\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 755 | loss: 0.36637 - acc: 0.9518 -- iter: 08/22\n",
            "Training Step: 2264  | total loss: \u001b[1m\u001b[32m0.35839\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 755 | loss: 0.35839 - acc: 0.9567 -- iter: 16/22\n",
            "Training Step: 2265  | total loss: \u001b[1m\u001b[32m0.36113\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 755 | loss: 0.36113 - acc: 0.9610 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2266  | total loss: \u001b[1m\u001b[32m0.35208\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 756 | loss: 0.35208 - acc: 0.9649 -- iter: 08/22\n",
            "Training Step: 2267  | total loss: \u001b[1m\u001b[32m0.36663\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 756 | loss: 0.36663 - acc: 0.9559 -- iter: 16/22\n",
            "Training Step: 2268  | total loss: \u001b[1m\u001b[32m0.36815\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 756 | loss: 0.36815 - acc: 0.9603 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2269  | total loss: \u001b[1m\u001b[32m0.36669\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 757 | loss: 0.36669 - acc: 0.9643 -- iter: 08/22\n",
            "Training Step: 2270  | total loss: \u001b[1m\u001b[32m0.37288\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 757 | loss: 0.37288 - acc: 0.9554 -- iter: 16/22\n",
            "Training Step: 2271  | total loss: \u001b[1m\u001b[32m0.37288\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 757 | loss: 0.37288 - acc: 0.9473 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2272  | total loss: \u001b[1m\u001b[32m0.37059\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 758 | loss: 0.37059 - acc: 0.9526 -- iter: 08/22\n",
            "Training Step: 2273  | total loss: \u001b[1m\u001b[32m0.39008\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 758 | loss: 0.39008 - acc: 0.9407 -- iter: 16/22\n",
            "Training Step: 2274  | total loss: \u001b[1m\u001b[32m0.37349\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 758 | loss: 0.37349 - acc: 0.9466 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2275  | total loss: \u001b[1m\u001b[32m0.37598\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 759 | loss: 0.37598 - acc: 0.9394 -- iter: 08/22\n",
            "Training Step: 2276  | total loss: \u001b[1m\u001b[32m0.39488\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 759 | loss: 0.39488 - acc: 0.9288 -- iter: 16/22\n",
            "Training Step: 2277  | total loss: \u001b[1m\u001b[32m0.38488\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 759 | loss: 0.38488 - acc: 0.9359 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2278  | total loss: \u001b[1m\u001b[32m0.38351\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 760 | loss: 0.38351 - acc: 0.9423 -- iter: 08/22\n",
            "Training Step: 2279  | total loss: \u001b[1m\u001b[32m0.39034\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 760 | loss: 0.39034 - acc: 0.9231 -- iter: 16/22\n",
            "Training Step: 2280  | total loss: \u001b[1m\u001b[32m0.38059\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 760 | loss: 0.38059 - acc: 0.9308 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2281  | total loss: \u001b[1m\u001b[32m0.39513\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 761 | loss: 0.39513 - acc: 0.9211 -- iter: 08/22\n",
            "Training Step: 2282  | total loss: \u001b[1m\u001b[32m0.38305\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 761 | loss: 0.38305 - acc: 0.9289 -- iter: 16/22\n",
            "Training Step: 2283  | total loss: \u001b[1m\u001b[32m0.38176\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 761 | loss: 0.38176 - acc: 0.9236 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2284  | total loss: \u001b[1m\u001b[32m0.39594\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 762 | loss: 0.39594 - acc: 0.9145 -- iter: 08/22\n",
            "Training Step: 2285  | total loss: \u001b[1m\u001b[32m0.38557\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 762 | loss: 0.38557 - acc: 0.9231 -- iter: 16/22\n",
            "Training Step: 2286  | total loss: \u001b[1m\u001b[32m0.38897\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 762 | loss: 0.38897 - acc: 0.9058 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2287  | total loss: \u001b[1m\u001b[32m0.38963\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 763 | loss: 0.38963 - acc: 0.9152 -- iter: 08/22\n",
            "Training Step: 2288  | total loss: \u001b[1m\u001b[32m0.37974\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 763 | loss: 0.37974 - acc: 0.9237 -- iter: 16/22\n",
            "Training Step: 2289  | total loss: \u001b[1m\u001b[32m0.38756\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 763 | loss: 0.38756 - acc: 0.8980 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2290  | total loss: \u001b[1m\u001b[32m0.39412\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 764 | loss: 0.39412 - acc: 0.9082 -- iter: 08/22\n",
            "Training Step: 2291  | total loss: \u001b[1m\u001b[32m0.37805\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 764 | loss: 0.37805 - acc: 0.9174 -- iter: 16/22\n",
            "Training Step: 2292  | total loss: \u001b[1m\u001b[32m0.38590\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 764 | loss: 0.38590 - acc: 0.8923 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2293  | total loss: \u001b[1m\u001b[32m0.36864\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 765 | loss: 0.36864 - acc: 0.9031 -- iter: 08/22\n",
            "Training Step: 2294  | total loss: \u001b[1m\u001b[32m0.36858\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 765 | loss: 0.36858 - acc: 0.9128 -- iter: 16/22\n",
            "Training Step: 2295  | total loss: \u001b[1m\u001b[32m0.38150\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 765 | loss: 0.38150 - acc: 0.8965 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2296  | total loss: \u001b[1m\u001b[32m0.36448\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 766 | loss: 0.36448 - acc: 0.9068 -- iter: 08/22\n",
            "Training Step: 2297  | total loss: \u001b[1m\u001b[32m0.37263\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 766 | loss: 0.37263 - acc: 0.8995 -- iter: 16/22\n",
            "Training Step: 2298  | total loss: \u001b[1m\u001b[32m0.36594\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 766 | loss: 0.36594 - acc: 0.8970 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2299  | total loss: \u001b[1m\u001b[32m0.36594\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 767 | loss: 0.36594 - acc: 0.9073 -- iter: 08/22\n",
            "Training Step: 2300  | total loss: \u001b[1m\u001b[32m0.37373\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 767 | loss: 0.37373 - acc: 0.8999 -- iter: 16/22\n",
            "Training Step: 2301  | total loss: \u001b[1m\u001b[32m0.37765\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 767 | loss: 0.37765 - acc: 0.9099 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2302  | total loss: \u001b[1m\u001b[32m0.37747\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 768 | loss: 0.37747 - acc: 0.8939 -- iter: 08/22\n",
            "Training Step: 2303  | total loss: \u001b[1m\u001b[32m0.37296\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 768 | loss: 0.37296 - acc: 0.9045 -- iter: 16/22\n",
            "Training Step: 2304  | total loss: \u001b[1m\u001b[32m0.37695\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 768 | loss: 0.37695 - acc: 0.9141 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2305  | total loss: \u001b[1m\u001b[32m0.36062\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 769 | loss: 0.36062 - acc: 0.9227 -- iter: 08/22\n",
            "Training Step: 2306  | total loss: \u001b[1m\u001b[32m0.36415\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 769 | loss: 0.36415 - acc: 0.9304 -- iter: 16/22\n",
            "Training Step: 2307  | total loss: \u001b[1m\u001b[32m0.37357\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 769 | loss: 0.37357 - acc: 0.9124 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2308  | total loss: \u001b[1m\u001b[32m0.35742\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 770 | loss: 0.35742 - acc: 0.9211 -- iter: 08/22\n",
            "Training Step: 2309  | total loss: \u001b[1m\u001b[32m0.35739\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 770 | loss: 0.35739 - acc: 0.9124 -- iter: 16/22\n",
            "Training Step: 2310  | total loss: \u001b[1m\u001b[32m0.36753\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 770 | loss: 0.36753 - acc: 0.9086 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2311  | total loss: \u001b[1m\u001b[32m0.35915\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 771 | loss: 0.35915 - acc: 0.9178 -- iter: 08/22\n",
            "Training Step: 2312  | total loss: \u001b[1m\u001b[32m0.35886\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 771 | loss: 0.35886 - acc: 0.9093 -- iter: 16/22\n",
            "Training Step: 2313  | total loss: \u001b[1m\u001b[32m0.35670\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 771 | loss: 0.35670 - acc: 0.9184 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2314  | total loss: \u001b[1m\u001b[32m0.34649\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 772 | loss: 0.34649 - acc: 0.9265 -- iter: 08/22\n",
            "Training Step: 2315  | total loss: \u001b[1m\u001b[32m0.36176\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 772 | loss: 0.36176 - acc: 0.9089 -- iter: 16/22\n",
            "Training Step: 2316  | total loss: \u001b[1m\u001b[32m0.35916\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 772 | loss: 0.35916 - acc: 0.9180 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2317  | total loss: \u001b[1m\u001b[32m0.35521\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 773 | loss: 0.35521 - acc: 0.9262 -- iter: 08/22\n",
            "Training Step: 2318  | total loss: \u001b[1m\u001b[32m0.34333\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 773 | loss: 0.34333 - acc: 0.9336 -- iter: 16/22\n",
            "Training Step: 2319  | total loss: \u001b[1m\u001b[32m0.36163\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 773 | loss: 0.36163 - acc: 0.9152 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2320  | total loss: \u001b[1m\u001b[32m0.35731\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 774 | loss: 0.35731 - acc: 0.9237 -- iter: 08/22\n",
            "Training Step: 2321  | total loss: \u001b[1m\u001b[32m0.36564\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 774 | loss: 0.36564 - acc: 0.9147 -- iter: 16/22\n",
            "Training Step: 2322  | total loss: \u001b[1m\u001b[32m0.36362\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 774 | loss: 0.36362 - acc: 0.9232 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2323  | total loss: \u001b[1m\u001b[32m0.35958\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 775 | loss: 0.35958 - acc: 0.9184 -- iter: 08/22\n",
            "Training Step: 2324  | total loss: \u001b[1m\u001b[32m0.36748\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 775 | loss: 0.36748 - acc: 0.9099 -- iter: 16/22\n",
            "Training Step: 2325  | total loss: \u001b[1m\u001b[32m0.35611\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 775 | loss: 0.35611 - acc: 0.9189 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2326  | total loss: \u001b[1m\u001b[32m0.36423\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 776 | loss: 0.36423 - acc: 0.9270 -- iter: 08/22\n",
            "Training Step: 2327  | total loss: \u001b[1m\u001b[32m0.36451\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 776 | loss: 0.36451 - acc: 0.9093 -- iter: 16/22\n",
            "Training Step: 2328  | total loss: \u001b[1m\u001b[32m0.35336\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 776 | loss: 0.35336 - acc: 0.9184 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2329  | total loss: \u001b[1m\u001b[32m0.34595\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 777 | loss: 0.34595 - acc: 0.9265 -- iter: 08/22\n",
            "Training Step: 2330  | total loss: \u001b[1m\u001b[32m0.35846\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 777 | loss: 0.35846 - acc: 0.9214 -- iter: 16/22\n",
            "Training Step: 2331  | total loss: \u001b[1m\u001b[32m0.35376\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 777 | loss: 0.35376 - acc: 0.9167 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2332  | total loss: \u001b[1m\u001b[32m0.34604\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 778 | loss: 0.34604 - acc: 0.9251 -- iter: 08/22\n",
            "Training Step: 2333  | total loss: \u001b[1m\u001b[32m0.34073\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 778 | loss: 0.34073 - acc: 0.9159 -- iter: 16/22\n",
            "Training Step: 2334  | total loss: \u001b[1m\u001b[32m0.35179\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 778 | loss: 0.35179 - acc: 0.9118 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2335  | total loss: \u001b[1m\u001b[32m0.34818\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 779 | loss: 0.34818 - acc: 0.9206 -- iter: 08/22\n",
            "Training Step: 2336  | total loss: \u001b[1m\u001b[32m0.34258\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 779 | loss: 0.34258 - acc: 0.9119 -- iter: 16/22\n",
            "Training Step: 2337  | total loss: \u001b[1m\u001b[32m0.34758\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 779 | loss: 0.34758 - acc: 0.9040 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2338  | total loss: \u001b[1m\u001b[32m0.35018\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 780 | loss: 0.35018 - acc: 0.9011 -- iter: 08/22\n",
            "Training Step: 2339  | total loss: \u001b[1m\u001b[32m0.34668\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 780 | loss: 0.34668 - acc: 0.9110 -- iter: 16/22\n",
            "Training Step: 2340  | total loss: \u001b[1m\u001b[32m0.35114\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 780 | loss: 0.35114 - acc: 0.9033 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2341  | total loss: \u001b[1m\u001b[32m0.35446\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 781 | loss: 0.35446 - acc: 0.8963 -- iter: 08/22\n",
            "Training Step: 2342  | total loss: \u001b[1m\u001b[32m0.35530\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 781 | loss: 0.35530 - acc: 0.8941 -- iter: 16/22\n",
            "Training Step: 2343  | total loss: \u001b[1m\u001b[32m0.35257\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 781 | loss: 0.35257 - acc: 0.9047 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2344  | total loss: \u001b[1m\u001b[32m0.35563\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 782 | loss: 0.35563 - acc: 0.8976 -- iter: 08/22\n",
            "Training Step: 2345  | total loss: \u001b[1m\u001b[32m0.35957\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 782 | loss: 0.35957 - acc: 0.9078 -- iter: 16/22\n",
            "Training Step: 2346  | total loss: \u001b[1m\u001b[32m0.35073\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 782 | loss: 0.35073 - acc: 0.9170 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2347  | total loss: \u001b[1m\u001b[32m0.35647\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 783 | loss: 0.35647 - acc: 0.9003 -- iter: 08/22\n",
            "Training Step: 2348  | total loss: \u001b[1m\u001b[32m0.66503\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 783 | loss: 0.66503 - acc: 0.8103 -- iter: 16/22\n",
            "Training Step: 2349  | total loss: \u001b[1m\u001b[32m0.64506\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 783 | loss: 0.64506 - acc: 0.8126 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2350  | total loss: \u001b[1m\u001b[32m0.61262\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 784 | loss: 0.61262 - acc: 0.8188 -- iter: 08/22\n",
            "Training Step: 2351  | total loss: \u001b[1m\u001b[32m0.58180\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 784 | loss: 0.58180 - acc: 0.8370 -- iter: 16/22\n",
            "Training Step: 2352  | total loss: \u001b[1m\u001b[32m0.56971\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 784 | loss: 0.56971 - acc: 0.8366 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2353  | total loss: \u001b[1m\u001b[32m0.53920\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 785 | loss: 0.53920 - acc: 0.8363 -- iter: 08/22\n",
            "Training Step: 2354  | total loss: \u001b[1m\u001b[32m0.51922\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 785 | loss: 0.51922 - acc: 0.8526 -- iter: 16/22\n",
            "Training Step: 2355  | total loss: \u001b[1m\u001b[32m0.51059\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 785 | loss: 0.51059 - acc: 0.8549 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2356  | total loss: \u001b[1m\u001b[32m0.48595\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 786 | loss: 0.48595 - acc: 0.8527 -- iter: 08/22\n",
            "Training Step: 2357  | total loss: \u001b[1m\u001b[32m0.46989\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 786 | loss: 0.46989 - acc: 0.8508 -- iter: 16/22\n",
            "Training Step: 2358  | total loss: \u001b[1m\u001b[32m0.48367\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 786 | loss: 0.48367 - acc: 0.8532 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2359  | total loss: \u001b[1m\u001b[32m0.44713\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 787 | loss: 0.44713 - acc: 0.8679 -- iter: 08/22\n",
            "Training Step: 2360  | total loss: \u001b[1m\u001b[32m0.43500\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 787 | loss: 0.43500 - acc: 0.8644 -- iter: 16/22\n",
            "Training Step: 2361  | total loss: \u001b[1m\u001b[32m0.42468\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 787 | loss: 0.42468 - acc: 0.8780 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2362  | total loss: \u001b[1m\u001b[32m0.43430\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 788 | loss: 0.43430 - acc: 0.8777 -- iter: 08/22\n",
            "Training Step: 2363  | total loss: \u001b[1m\u001b[32m0.41070\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 788 | loss: 0.41070 - acc: 0.8774 -- iter: 16/22\n",
            "Training Step: 2364  | total loss: \u001b[1m\u001b[32m0.40263\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 788 | loss: 0.40263 - acc: 0.8897 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2365  | total loss: \u001b[1m\u001b[32m0.39121\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 789 | loss: 0.39121 - acc: 0.9007 -- iter: 08/22\n",
            "Training Step: 2366  | total loss: \u001b[1m\u001b[32m0.39183\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 789 | loss: 0.39183 - acc: 0.8981 -- iter: 16/22\n",
            "Training Step: 2367  | total loss: \u001b[1m\u001b[32m0.38774\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 789 | loss: 0.38774 - acc: 0.8958 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2368  | total loss: \u001b[1m\u001b[32m0.37764\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 790 | loss: 0.37764 - acc: 0.9062 -- iter: 08/22\n",
            "Training Step: 2369  | total loss: \u001b[1m\u001b[32m0.37711\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 790 | loss: 0.37711 - acc: 0.8990 -- iter: 16/22\n",
            "Training Step: 2370  | total loss: \u001b[1m\u001b[32m0.36680\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 790 | loss: 0.36680 - acc: 0.9091 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2371  | total loss: \u001b[1m\u001b[32m0.37095\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 791 | loss: 0.37095 - acc: 0.9057 -- iter: 08/22\n",
            "Training Step: 2372  | total loss: \u001b[1m\u001b[32m0.37097\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 791 | loss: 0.37097 - acc: 0.8984 -- iter: 16/22\n",
            "Training Step: 2373  | total loss: \u001b[1m\u001b[32m0.36526\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 791 | loss: 0.36526 - acc: 0.9086 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2374  | total loss: \u001b[1m\u001b[32m0.36248\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 792 | loss: 0.36248 - acc: 0.9052 -- iter: 08/22\n",
            "Training Step: 2375  | total loss: \u001b[1m\u001b[32m0.36469\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 792 | loss: 0.36469 - acc: 0.9022 -- iter: 16/22\n",
            "Training Step: 2376  | total loss: \u001b[1m\u001b[32m0.35947\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 792 | loss: 0.35947 - acc: 0.9120 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2377  | total loss: \u001b[1m\u001b[32m0.35264\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 793 | loss: 0.35264 - acc: 0.9208 -- iter: 08/22\n",
            "Training Step: 2378  | total loss: \u001b[1m\u001b[32m0.36516\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 793 | loss: 0.36516 - acc: 0.9162 -- iter: 16/22\n",
            "Training Step: 2379  | total loss: \u001b[1m\u001b[32m0.35443\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 793 | loss: 0.35443 - acc: 0.9121 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2380  | total loss: \u001b[1m\u001b[32m0.34785\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 794 | loss: 0.34785 - acc: 0.9209 -- iter: 08/22\n",
            "Training Step: 2381  | total loss: \u001b[1m\u001b[32m0.36645\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 794 | loss: 0.36645 - acc: 0.9121 -- iter: 16/22\n",
            "Training Step: 2382  | total loss: \u001b[1m\u001b[32m0.36013\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 794 | loss: 0.36013 - acc: 0.9084 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2383  | total loss: \u001b[1m\u001b[32m0.34867\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 795 | loss: 0.34867 - acc: 0.9176 -- iter: 08/22\n",
            "Training Step: 2384  | total loss: \u001b[1m\u001b[32m0.36707\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 795 | loss: 0.36707 - acc: 0.9091 -- iter: 16/22\n",
            "Training Step: 2385  | total loss: \u001b[1m\u001b[32m0.36418\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 795 | loss: 0.36418 - acc: 0.9016 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2386  | total loss: \u001b[1m\u001b[32m0.36809\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 796 | loss: 0.36809 - acc: 0.8989 -- iter: 08/22\n",
            "Training Step: 2387  | total loss: \u001b[1m\u001b[32m0.36008\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 796 | loss: 0.36008 - acc: 0.9090 -- iter: 16/22\n",
            "Training Step: 2388  | total loss: \u001b[1m\u001b[32m0.35777\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 796 | loss: 0.35777 - acc: 0.9014 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2389  | total loss: \u001b[1m\u001b[32m0.34939\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 797 | loss: 0.34939 - acc: 0.9113 -- iter: 08/22\n",
            "Training Step: 2390  | total loss: \u001b[1m\u001b[32m0.35308\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 797 | loss: 0.35308 - acc: 0.9077 -- iter: 16/22\n",
            "Training Step: 2391  | total loss: \u001b[1m\u001b[32m0.35268\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 797 | loss: 0.35268 - acc: 0.9044 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2392  | total loss: \u001b[1m\u001b[32m0.34465\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 798 | loss: 0.34465 - acc: 0.9140 -- iter: 08/22\n",
            "Training Step: 2393  | total loss: \u001b[1m\u001b[32m0.33652\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 798 | loss: 0.33652 - acc: 0.9226 -- iter: 16/22\n",
            "Training Step: 2394  | total loss: \u001b[1m\u001b[32m0.32488\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 798 | loss: 0.32488 - acc: 0.9303 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2395  | total loss: \u001b[1m\u001b[32m0.34447\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 799 | loss: 0.34447 - acc: 0.9123 -- iter: 08/22\n",
            "Training Step: 2396  | total loss: \u001b[1m\u001b[32m0.33602\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 799 | loss: 0.33602 - acc: 0.9211 -- iter: 16/22\n",
            "Training Step: 2397  | total loss: \u001b[1m\u001b[32m0.35646\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 799 | loss: 0.35646 - acc: 0.8956 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2398  | total loss: \u001b[1m\u001b[32m0.35160\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 800 | loss: 0.35160 - acc: 0.9061 -- iter: 08/22\n",
            "Training Step: 2399  | total loss: \u001b[1m\u001b[32m0.33829\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 800 | loss: 0.33829 - acc: 0.9154 -- iter: 16/22\n",
            "Training Step: 2400  | total loss: \u001b[1m\u001b[32m0.35847\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 800 | loss: 0.35847 - acc: 0.8906 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2401  | total loss: \u001b[1m\u001b[32m0.35418\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 801 | loss: 0.35418 - acc: 0.9015 -- iter: 08/22\n",
            "Training Step: 2402  | total loss: \u001b[1m\u001b[32m0.33989\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 801 | loss: 0.33989 - acc: 0.9114 -- iter: 16/22\n",
            "Training Step: 2403  | total loss: \u001b[1m\u001b[32m0.35398\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 801 | loss: 0.35398 - acc: 0.8952 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2404  | total loss: \u001b[1m\u001b[32m0.34989\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 802 | loss: 0.34989 - acc: 0.9057 -- iter: 08/22\n",
            "Training Step: 2405  | total loss: \u001b[1m\u001b[32m0.34104\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 802 | loss: 0.34104 - acc: 0.9151 -- iter: 16/22\n",
            "Training Step: 2406  | total loss: \u001b[1m\u001b[32m0.33401\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 802 | loss: 0.33401 - acc: 0.9236 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2407  | total loss: \u001b[1m\u001b[32m0.34635\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 803 | loss: 0.34635 - acc: 0.9063 -- iter: 08/22\n",
            "Training Step: 2408  | total loss: \u001b[1m\u001b[32m0.33771\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 803 | loss: 0.33771 - acc: 0.9156 -- iter: 16/22\n",
            "Training Step: 2409  | total loss: \u001b[1m\u001b[32m0.33527\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 803 | loss: 0.33527 - acc: 0.9241 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2410  | total loss: \u001b[1m\u001b[32m0.32301\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 804 | loss: 0.32301 - acc: 0.9317 -- iter: 08/22\n",
            "Training Step: 2411  | total loss: \u001b[1m\u001b[32m0.33800\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 804 | loss: 0.33800 - acc: 0.9135 -- iter: 16/22\n",
            "Training Step: 2412  | total loss: \u001b[1m\u001b[32m0.33529\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 804 | loss: 0.33529 - acc: 0.9221 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2413  | total loss: \u001b[1m\u001b[32m0.32777\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 805 | loss: 0.32777 - acc: 0.9299 -- iter: 08/22\n",
            "Training Step: 2414  | total loss: \u001b[1m\u001b[32m0.31306\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 805 | loss: 0.31306 - acc: 0.9369 -- iter: 16/22\n",
            "Training Step: 2415  | total loss: \u001b[1m\u001b[32m0.33578\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 805 | loss: 0.33578 - acc: 0.9182 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2416  | total loss: \u001b[1m\u001b[32m0.32810\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 806 | loss: 0.32810 - acc: 0.9264 -- iter: 08/22\n",
            "Training Step: 2417  | total loss: \u001b[1m\u001b[32m0.32758\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 806 | loss: 0.32758 - acc: 0.9338 -- iter: 16/22\n",
            "Training Step: 2418  | total loss: \u001b[1m\u001b[32m0.32362\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 806 | loss: 0.32362 - acc: 0.9279 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2419  | total loss: \u001b[1m\u001b[32m0.32954\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 807 | loss: 0.32954 - acc: 0.9226 -- iter: 08/22\n",
            "Training Step: 2420  | total loss: \u001b[1m\u001b[32m0.32863\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 807 | loss: 0.32863 - acc: 0.9303 -- iter: 16/22\n",
            "Training Step: 2421  | total loss: \u001b[1m\u001b[32m0.31505\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 807 | loss: 0.31505 - acc: 0.9373 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2422  | total loss: \u001b[1m\u001b[32m0.31847\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 808 | loss: 0.31847 - acc: 0.9311 -- iter: 08/22\n",
            "Training Step: 2423  | total loss: \u001b[1m\u001b[32m0.32811\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 808 | loss: 0.32811 - acc: 0.9255 -- iter: 16/22\n",
            "Training Step: 2424  | total loss: \u001b[1m\u001b[32m0.31437\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 808 | loss: 0.31437 - acc: 0.9329 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2425  | total loss: \u001b[1m\u001b[32m0.31685\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 809 | loss: 0.31685 - acc: 0.9396 -- iter: 08/22\n",
            "Training Step: 2426  | total loss: \u001b[1m\u001b[32m0.33368\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 809 | loss: 0.33368 - acc: 0.9332 -- iter: 16/22\n",
            "Training Step: 2427  | total loss: \u001b[1m\u001b[32m0.31697\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 809 | loss: 0.31697 - acc: 0.9274 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2428  | total loss: \u001b[1m\u001b[32m0.31881\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 810 | loss: 0.31881 - acc: 0.9346 -- iter: 08/22\n",
            "Training Step: 2429  | total loss: \u001b[1m\u001b[32m0.33063\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 810 | loss: 0.33063 - acc: 0.9245 -- iter: 16/22\n",
            "Training Step: 2430  | total loss: \u001b[1m\u001b[32m0.32625\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 810 | loss: 0.32625 - acc: 0.9320 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2431  | total loss: \u001b[1m\u001b[32m0.32229\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 811 | loss: 0.32229 - acc: 0.9263 -- iter: 08/22\n",
            "Training Step: 2432  | total loss: \u001b[1m\u001b[32m0.33355\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 811 | loss: 0.33355 - acc: 0.9170 -- iter: 16/22\n",
            "Training Step: 2433  | total loss: \u001b[1m\u001b[32m0.33225\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 811 | loss: 0.33225 - acc: 0.9087 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2434  | total loss: \u001b[1m\u001b[32m0.32183\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 812 | loss: 0.32183 - acc: 0.9053 -- iter: 08/22\n",
            "Training Step: 2435  | total loss: \u001b[1m\u001b[32m0.33252\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 812 | loss: 0.33252 - acc: 0.9023 -- iter: 16/22\n",
            "Training Step: 2436  | total loss: \u001b[1m\u001b[32m0.33121\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 812 | loss: 0.33121 - acc: 0.8954 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2437  | total loss: \u001b[1m\u001b[32m0.35023\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 813 | loss: 0.35023 - acc: 0.8892 -- iter: 08/22\n",
            "Training Step: 2438  | total loss: \u001b[1m\u001b[32m0.33878\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 813 | loss: 0.33878 - acc: 0.8878 -- iter: 16/22\n",
            "Training Step: 2439  | total loss: \u001b[1m\u001b[32m0.33154\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 813 | loss: 0.33154 - acc: 0.8865 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2440  | total loss: \u001b[1m\u001b[32m0.35034\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 814 | loss: 0.35034 - acc: 0.8812 -- iter: 08/22\n",
            "Training Step: 2441  | total loss: \u001b[1m\u001b[32m0.33828\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 814 | loss: 0.33828 - acc: 0.8930 -- iter: 16/22\n",
            "Training Step: 2442  | total loss: \u001b[1m\u001b[32m0.33329\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 814 | loss: 0.33329 - acc: 0.8912 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2443  | total loss: \u001b[1m\u001b[32m0.34272\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 815 | loss: 0.34272 - acc: 0.8896 -- iter: 08/22\n",
            "Training Step: 2444  | total loss: \u001b[1m\u001b[32m0.33129\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 815 | loss: 0.33129 - acc: 0.9007 -- iter: 16/22\n",
            "Training Step: 2445  | total loss: \u001b[1m\u001b[32m0.33310\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 815 | loss: 0.33310 - acc: 0.9106 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2446  | total loss: \u001b[1m\u001b[32m0.31234\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 816 | loss: 0.31234 - acc: 0.9195 -- iter: 08/22\n",
            "Training Step: 2447  | total loss: \u001b[1m\u001b[32m0.33085\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 816 | loss: 0.33085 - acc: 0.9026 -- iter: 16/22\n",
            "Training Step: 2448  | total loss: \u001b[1m\u001b[32m0.33244\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 816 | loss: 0.33244 - acc: 0.9123 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2449  | total loss: \u001b[1m\u001b[32m0.33305\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 817 | loss: 0.33305 - acc: 0.9211 -- iter: 08/22\n",
            "Training Step: 2450  | total loss: \u001b[1m\u001b[32m0.33857\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 817 | loss: 0.33857 - acc: 0.9040 -- iter: 16/22\n",
            "Training Step: 2451  | total loss: \u001b[1m\u001b[32m0.32862\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 817 | loss: 0.32862 - acc: 0.9011 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2452  | total loss: \u001b[1m\u001b[32m0.32925\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 818 | loss: 0.32925 - acc: 0.9110 -- iter: 08/22\n",
            "Training Step: 2453  | total loss: \u001b[1m\u001b[32m0.33196\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 818 | loss: 0.33196 - acc: 0.9032 -- iter: 16/22\n",
            "Training Step: 2454  | total loss: \u001b[1m\u001b[32m0.31671\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 818 | loss: 0.31671 - acc: 0.9129 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2455  | total loss: \u001b[1m\u001b[32m0.32801\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 819 | loss: 0.32801 - acc: 0.8966 -- iter: 08/22\n",
            "Training Step: 2456  | total loss: \u001b[1m\u001b[32m0.33070\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 819 | loss: 0.33070 - acc: 0.8903 -- iter: 16/22\n",
            "Training Step: 2457  | total loss: \u001b[1m\u001b[32m0.32822\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 819 | loss: 0.32822 - acc: 0.8846 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2458  | total loss: \u001b[1m\u001b[32m0.33865\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 820 | loss: 0.33865 - acc: 0.8711 -- iter: 08/22\n",
            "Training Step: 2459  | total loss: \u001b[1m\u001b[32m0.32583\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 820 | loss: 0.32583 - acc: 0.8840 -- iter: 16/22\n",
            "Training Step: 2460  | total loss: \u001b[1m\u001b[32m0.32364\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 820 | loss: 0.32364 - acc: 0.8789 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2461  | total loss: \u001b[1m\u001b[32m0.31683\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 821 | loss: 0.31683 - acc: 0.8910 -- iter: 08/22\n",
            "Training Step: 2462  | total loss: \u001b[1m\u001b[32m0.30387\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 821 | loss: 0.30387 - acc: 0.8894 -- iter: 16/22\n",
            "Training Step: 2463  | total loss: \u001b[1m\u001b[32m0.32231\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 821 | loss: 0.32231 - acc: 0.8755 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2464  | total loss: \u001b[1m\u001b[32m0.31549\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 822 | loss: 0.31549 - acc: 0.8879 -- iter: 08/22\n",
            "Training Step: 2465  | total loss: \u001b[1m\u001b[32m0.32751\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 822 | loss: 0.32751 - acc: 0.8825 -- iter: 16/22\n",
            "Training Step: 2466  | total loss: \u001b[1m\u001b[32m0.33354\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 822 | loss: 0.33354 - acc: 0.8817 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2467  | total loss: \u001b[1m\u001b[32m0.31503\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 823 | loss: 0.31503 - acc: 0.8936 -- iter: 08/22\n",
            "Training Step: 2468  | total loss: \u001b[1m\u001b[32m0.32695\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 823 | loss: 0.32695 - acc: 0.8875 -- iter: 16/22\n",
            "Training Step: 2469  | total loss: \u001b[1m\u001b[32m0.33653\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 823 | loss: 0.33653 - acc: 0.8655 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2470  | total loss: \u001b[1m\u001b[32m0.33358\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 824 | loss: 0.33358 - acc: 0.8789 -- iter: 08/22\n",
            "Training Step: 2471  | total loss: \u001b[1m\u001b[32m0.32370\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 824 | loss: 0.32370 - acc: 0.8910 -- iter: 16/22\n",
            "Training Step: 2472  | total loss: \u001b[1m\u001b[32m0.33337\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 824 | loss: 0.33337 - acc: 0.8686 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2473  | total loss: \u001b[1m\u001b[32m0.32996\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 825 | loss: 0.32996 - acc: 0.8651 -- iter: 08/22\n",
            "Training Step: 2474  | total loss: \u001b[1m\u001b[32m0.32108\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 825 | loss: 0.32108 - acc: 0.8786 -- iter: 16/22\n",
            "Training Step: 2475  | total loss: \u001b[1m\u001b[32m0.32771\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 825 | loss: 0.32771 - acc: 0.8782 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2476  | total loss: \u001b[1m\u001b[32m0.32461\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 826 | loss: 0.32461 - acc: 0.8737 -- iter: 08/22\n",
            "Training Step: 2477  | total loss: \u001b[1m\u001b[32m0.31767\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 826 | loss: 0.31767 - acc: 0.8863 -- iter: 16/22\n",
            "Training Step: 2478  | total loss: \u001b[1m\u001b[32m0.30620\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 826 | loss: 0.30620 - acc: 0.8852 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2479  | total loss: \u001b[1m\u001b[32m0.32091\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 827 | loss: 0.32091 - acc: 0.8842 -- iter: 08/22\n",
            "Training Step: 2480  | total loss: \u001b[1m\u001b[32m0.31419\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 827 | loss: 0.31419 - acc: 0.8958 -- iter: 16/22\n",
            "Training Step: 2481  | total loss: \u001b[1m\u001b[32m0.33127\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 827 | loss: 0.33127 - acc: 0.8895 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2482  | total loss: \u001b[1m\u001b[32m0.32447\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 828 | loss: 0.32447 - acc: 0.8881 -- iter: 08/22\n",
            "Training Step: 2483  | total loss: \u001b[1m\u001b[32m0.31376\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 828 | loss: 0.31376 - acc: 0.8993 -- iter: 16/22\n",
            "Training Step: 2484  | total loss: \u001b[1m\u001b[32m0.33046\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 828 | loss: 0.33046 - acc: 0.8927 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2485  | total loss: \u001b[1m\u001b[32m0.31764\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 829 | loss: 0.31764 - acc: 0.8867 -- iter: 08/22\n",
            "Training Step: 2486  | total loss: \u001b[1m\u001b[32m0.31676\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 829 | loss: 0.31676 - acc: 0.8856 -- iter: 16/22\n",
            "Training Step: 2487  | total loss: \u001b[1m\u001b[32m0.32282\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 829 | loss: 0.32282 - acc: 0.8970 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2488  | total loss: \u001b[1m\u001b[32m0.31043\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 830 | loss: 0.31043 - acc: 0.8906 -- iter: 08/22\n",
            "Training Step: 2489  | total loss: \u001b[1m\u001b[32m0.32337\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 830 | loss: 0.32337 - acc: 0.8964 -- iter: 16/22\n",
            "Training Step: 2490  | total loss: \u001b[1m\u001b[32m0.31886\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 830 | loss: 0.31886 - acc: 0.8964 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2491  | total loss: \u001b[1m\u001b[32m0.30945\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 831 | loss: 0.30945 - acc: 0.9068 -- iter: 08/22\n",
            "Training Step: 2492  | total loss: \u001b[1m\u001b[32m0.32212\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 831 | loss: 0.32212 - acc: 0.8994 -- iter: 16/22\n",
            "Training Step: 2493  | total loss: \u001b[1m\u001b[32m0.30427\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 831 | loss: 0.30427 - acc: 0.9095 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2494  | total loss: \u001b[1m\u001b[32m0.31838\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 832 | loss: 0.31838 - acc: 0.9060 -- iter: 08/22\n",
            "Training Step: 2495  | total loss: \u001b[1m\u001b[32m0.31399\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 832 | loss: 0.31399 - acc: 0.9154 -- iter: 16/22\n",
            "Training Step: 2496  | total loss: \u001b[1m\u001b[32m0.29686\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 832 | loss: 0.29686 - acc: 0.9315 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2497  | total loss: \u001b[1m\u001b[32m0.30789\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 833 | loss: 0.30789 - acc: 0.9315 -- iter: 08/22\n",
            "Training Step: 2498  | total loss: \u001b[1m\u001b[32m0.30448\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 833 | loss: 0.30448 - acc: 0.9384 -- iter: 16/22\n",
            "Training Step: 2499  | total loss: \u001b[1m\u001b[32m0.29843\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 833 | loss: 0.29843 - acc: 0.9320 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2500  | total loss: \u001b[1m\u001b[32m0.30923\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 834 | loss: 0.30923 - acc: 0.9388 -- iter: 08/22\n",
            "Training Step: 2501  | total loss: \u001b[1m\u001b[32m0.31333\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 834 | loss: 0.31333 - acc: 0.9449 -- iter: 16/22\n",
            "Training Step: 2502  | total loss: \u001b[1m\u001b[32m0.30749\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 834 | loss: 0.30749 - acc: 0.9504 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2503  | total loss: \u001b[1m\u001b[32m0.30694\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 835 | loss: 0.30694 - acc: 0.9429 -- iter: 08/22\n",
            "Training Step: 2504  | total loss: \u001b[1m\u001b[32m0.31102\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 835 | loss: 0.31102 - acc: 0.9486 -- iter: 16/22\n",
            "Training Step: 2505  | total loss: \u001b[1m\u001b[32m0.31041\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 835 | loss: 0.31041 - acc: 0.9537 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2506  | total loss: \u001b[1m\u001b[32m0.30830\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 836 | loss: 0.30830 - acc: 0.9584 -- iter: 08/22\n",
            "Training Step: 2507  | total loss: \u001b[1m\u001b[32m0.30724\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 836 | loss: 0.30724 - acc: 0.9500 -- iter: 16/22\n",
            "Training Step: 2508  | total loss: \u001b[1m\u001b[32m0.30659\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 836 | loss: 0.30659 - acc: 0.9550 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2509  | total loss: \u001b[1m\u001b[32m0.29630\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 837 | loss: 0.29630 - acc: 0.9595 -- iter: 08/22\n",
            "Training Step: 2510  | total loss: \u001b[1m\u001b[32m0.29710\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 837 | loss: 0.29710 - acc: 0.9636 -- iter: 16/22\n",
            "Training Step: 2511  | total loss: \u001b[1m\u001b[32m0.30282\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 837 | loss: 0.30282 - acc: 0.9547 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2512  | total loss: \u001b[1m\u001b[32m0.29260\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 838 | loss: 0.29260 - acc: 0.9592 -- iter: 08/22\n",
            "Training Step: 2513  | total loss: \u001b[1m\u001b[32m0.28067\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 838 | loss: 0.28067 - acc: 0.9633 -- iter: 16/22\n",
            "Training Step: 2514  | total loss: \u001b[1m\u001b[32m0.30178\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 838 | loss: 0.30178 - acc: 0.9545 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2515  | total loss: \u001b[1m\u001b[32m0.29009\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 839 | loss: 0.29009 - acc: 0.9590 -- iter: 08/22\n",
            "Training Step: 2516  | total loss: \u001b[1m\u001b[32m0.27814\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 839 | loss: 0.27814 - acc: 0.9631 -- iter: 16/22\n",
            "Training Step: 2517  | total loss: \u001b[1m\u001b[32m0.27393\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 839 | loss: 0.27393 - acc: 0.9668 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2518  | total loss: \u001b[1m\u001b[32m0.28971\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 840 | loss: 0.28971 - acc: 0.9451 -- iter: 08/22\n",
            "Training Step: 2519  | total loss: \u001b[1m\u001b[32m0.28021\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 840 | loss: 0.28021 - acc: 0.9506 -- iter: 16/22\n",
            "Training Step: 2520  | total loss: \u001b[1m\u001b[32m0.27545\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 840 | loss: 0.27545 - acc: 0.9556 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2521  | total loss: \u001b[1m\u001b[32m0.27240\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 841 | loss: 0.27240 - acc: 0.9600 -- iter: 08/22\n",
            "Training Step: 2522  | total loss: \u001b[1m\u001b[32m0.27294\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 841 | loss: 0.27294 - acc: 0.9515 -- iter: 16/22\n",
            "Training Step: 2523  | total loss: \u001b[1m\u001b[32m0.27935\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 841 | loss: 0.27935 - acc: 0.9439 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2524  | total loss: \u001b[1m\u001b[32m0.27588\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 842 | loss: 0.27588 - acc: 0.9495 -- iter: 08/22\n",
            "Training Step: 2525  | total loss: \u001b[1m\u001b[32m0.26033\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 842 | loss: 0.26033 - acc: 0.9545 -- iter: 16/22\n",
            "Training Step: 2526  | total loss: \u001b[1m\u001b[32m0.26165\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 842 | loss: 0.26165 - acc: 0.9591 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2527  | total loss: \u001b[1m\u001b[32m0.27862\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 843 | loss: 0.27862 - acc: 0.9382 -- iter: 08/22\n",
            "Training Step: 2528  | total loss: \u001b[1m\u001b[32m0.26270\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 843 | loss: 0.26270 - acc: 0.9443 -- iter: 16/22\n",
            "Training Step: 2529  | total loss: \u001b[1m\u001b[32m0.26952\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 843 | loss: 0.26952 - acc: 0.9499 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2530  | total loss: \u001b[1m\u001b[32m0.27268\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 844 | loss: 0.27268 - acc: 0.9299 -- iter: 08/22\n",
            "Training Step: 2531  | total loss: \u001b[1m\u001b[32m0.26962\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 844 | loss: 0.26962 - acc: 0.9369 -- iter: 16/22\n",
            "Training Step: 2532  | total loss: \u001b[1m\u001b[32m0.27556\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 844 | loss: 0.27556 - acc: 0.9432 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2533  | total loss: \u001b[1m\u001b[32m0.26492\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 845 | loss: 0.26492 - acc: 0.9489 -- iter: 08/22\n",
            "Training Step: 2534  | total loss: \u001b[1m\u001b[32m0.26363\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 845 | loss: 0.26363 - acc: 0.9415 -- iter: 16/22\n",
            "Training Step: 2535  | total loss: \u001b[1m\u001b[32m0.27804\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 845 | loss: 0.27804 - acc: 0.9349 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2536  | total loss: \u001b[1m\u001b[32m0.26695\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 846 | loss: 0.26695 - acc: 0.9414 -- iter: 08/22\n",
            "Training Step: 2537  | total loss: \u001b[1m\u001b[32m0.27608\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 846 | loss: 0.27608 - acc: 0.9306 -- iter: 16/22\n",
            "Training Step: 2538  | total loss: \u001b[1m\u001b[32m0.27273\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 846 | loss: 0.27273 - acc: 0.9250 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2539  | total loss: \u001b[1m\u001b[32m0.27255\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 847 | loss: 0.27255 - acc: 0.9325 -- iter: 08/22\n",
            "Training Step: 2540  | total loss: \u001b[1m\u001b[32m0.28095\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 847 | loss: 0.28095 - acc: 0.9226 -- iter: 16/22\n",
            "Training Step: 2541  | total loss: \u001b[1m\u001b[32m0.28600\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 847 | loss: 0.28600 - acc: 0.9303 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2542  | total loss: \u001b[1m\u001b[32m0.28543\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 848 | loss: 0.28543 - acc: 0.9248 -- iter: 08/22\n",
            "Training Step: 2543  | total loss: \u001b[1m\u001b[32m0.28181\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 848 | loss: 0.28181 - acc: 0.9198 -- iter: 16/22\n",
            "Training Step: 2544  | total loss: \u001b[1m\u001b[32m0.28640\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 848 | loss: 0.28640 - acc: 0.9278 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2545  | total loss: \u001b[1m\u001b[32m0.29021\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 849 | loss: 0.29021 - acc: 0.9351 -- iter: 08/22\n",
            "Training Step: 2546  | total loss: \u001b[1m\u001b[32m0.28907\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 849 | loss: 0.28907 - acc: 0.9416 -- iter: 16/22\n",
            "Training Step: 2547  | total loss: \u001b[1m\u001b[32m0.28524\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 849 | loss: 0.28524 - acc: 0.9349 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2548  | total loss: \u001b[1m\u001b[32m0.28878\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 850 | loss: 0.28878 - acc: 0.9414 -- iter: 08/22\n",
            "Training Step: 2549  | total loss: \u001b[1m\u001b[32m0.28175\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 850 | loss: 0.28175 - acc: 0.9473 -- iter: 16/22\n",
            "Training Step: 2550  | total loss: \u001b[1m\u001b[32m0.29638\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 850 | loss: 0.29638 - acc: 0.9400 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2551  | total loss: \u001b[1m\u001b[32m0.28427\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 851 | loss: 0.28427 - acc: 0.9460 -- iter: 08/22\n",
            "Training Step: 2552  | total loss: \u001b[1m\u001b[32m0.27742\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 851 | loss: 0.27742 - acc: 0.9514 -- iter: 16/22\n",
            "Training Step: 2553  | total loss: \u001b[1m\u001b[32m0.26964\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 851 | loss: 0.26964 - acc: 0.9396 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2554  | total loss: \u001b[1m\u001b[32m0.27364\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 852 | loss: 0.27364 - acc: 0.9457 -- iter: 08/22\n",
            "Training Step: 2555  | total loss: \u001b[1m\u001b[32m0.27655\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 852 | loss: 0.27655 - acc: 0.9511 -- iter: 16/22\n",
            "Training Step: 2556  | total loss: \u001b[1m\u001b[32m0.26866\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 852 | loss: 0.26866 - acc: 0.9393 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2557  | total loss: \u001b[1m\u001b[32m0.27769\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 853 | loss: 0.27769 - acc: 0.9287 -- iter: 08/22\n",
            "Training Step: 2558  | total loss: \u001b[1m\u001b[32m0.28470\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 853 | loss: 0.28470 - acc: 0.9358 -- iter: 16/22\n",
            "Training Step: 2559  | total loss: \u001b[1m\u001b[32m0.27037\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 853 | loss: 0.27037 - acc: 0.9423 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2560  | total loss: \u001b[1m\u001b[32m0.27903\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 854 | loss: 0.27903 - acc: 0.9314 -- iter: 08/22\n",
            "Training Step: 2561  | total loss: \u001b[1m\u001b[32m0.28238\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 854 | loss: 0.28238 - acc: 0.9382 -- iter: 16/22\n",
            "Training Step: 2562  | total loss: \u001b[1m\u001b[32m0.27970\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 854 | loss: 0.27970 - acc: 0.9319 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2563  | total loss: \u001b[1m\u001b[32m0.27816\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 855 | loss: 0.27816 - acc: 0.9387 -- iter: 08/22\n",
            "Training Step: 2564  | total loss: \u001b[1m\u001b[32m0.28119\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 855 | loss: 0.28119 - acc: 0.9448 -- iter: 16/22\n",
            "Training Step: 2565  | total loss: \u001b[1m\u001b[32m0.27795\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 855 | loss: 0.27795 - acc: 0.9504 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2566  | total loss: \u001b[1m\u001b[32m0.28528\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 856 | loss: 0.28528 - acc: 0.9428 -- iter: 08/22\n",
            "Training Step: 2567  | total loss: \u001b[1m\u001b[32m0.27797\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 856 | loss: 0.27797 - acc: 0.9485 -- iter: 16/22\n",
            "Training Step: 2568  | total loss: \u001b[1m\u001b[32m0.27495\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 856 | loss: 0.27495 - acc: 0.9537 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2569  | total loss: \u001b[1m\u001b[32m0.27303\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 857 | loss: 0.27303 - acc: 0.9417 -- iter: 08/22\n",
            "Training Step: 2570  | total loss: \u001b[1m\u001b[32m0.26922\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 857 | loss: 0.26922 - acc: 0.9475 -- iter: 16/22\n",
            "Training Step: 2571  | total loss: \u001b[1m\u001b[32m0.27409\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 857 | loss: 0.27409 - acc: 0.9527 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2572  | total loss: \u001b[1m\u001b[32m0.27208\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 858 | loss: 0.27208 - acc: 0.9408 -- iter: 08/22\n",
            "Training Step: 2573  | total loss: \u001b[1m\u001b[32m0.26158\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 858 | loss: 0.26158 - acc: 0.9467 -- iter: 16/22\n",
            "Training Step: 2574  | total loss: \u001b[1m\u001b[32m0.25530\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 858 | loss: 0.25530 - acc: 0.9520 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2575  | total loss: \u001b[1m\u001b[32m0.27133\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 859 | loss: 0.27133 - acc: 0.9443 -- iter: 08/22\n",
            "Training Step: 2576  | total loss: \u001b[1m\u001b[32m0.26083\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 859 | loss: 0.26083 - acc: 0.9499 -- iter: 16/22\n",
            "Training Step: 2577  | total loss: \u001b[1m\u001b[32m0.26497\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 859 | loss: 0.26497 - acc: 0.9549 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2578  | total loss: \u001b[1m\u001b[32m0.26962\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 860 | loss: 0.26962 - acc: 0.9469 -- iter: 08/22\n",
            "Training Step: 2579  | total loss: \u001b[1m\u001b[32m0.26245\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 860 | loss: 0.26245 - acc: 0.9522 -- iter: 16/22\n",
            "Training Step: 2580  | total loss: \u001b[1m\u001b[32m0.26627\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 860 | loss: 0.26627 - acc: 0.9570 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2581  | total loss: \u001b[1m\u001b[32m0.27923\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 861 | loss: 0.27923 - acc: 0.9446 -- iter: 08/22\n",
            "Training Step: 2582  | total loss: \u001b[1m\u001b[32m0.26742\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 861 | loss: 0.26742 - acc: 0.9502 -- iter: 16/22\n",
            "Training Step: 2583  | total loss: \u001b[1m\u001b[32m0.26801\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 861 | loss: 0.26801 - acc: 0.9552 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2584  | total loss: \u001b[1m\u001b[32m0.28056\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 862 | loss: 0.28056 - acc: 0.9430 -- iter: 08/22\n",
            "Training Step: 2585  | total loss: \u001b[1m\u001b[32m0.28780\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 862 | loss: 0.28780 - acc: 0.9487 -- iter: 16/22\n",
            "Training Step: 2586  | total loss: \u001b[1m\u001b[32m0.29234\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 862 | loss: 0.29234 - acc: 0.9538 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2587  | total loss: \u001b[1m\u001b[32m0.27608\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 863 | loss: 0.27608 - acc: 0.9584 -- iter: 08/22\n",
            "Training Step: 2588  | total loss: \u001b[1m\u001b[32m0.28337\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 863 | loss: 0.28337 - acc: 0.9626 -- iter: 16/22\n",
            "Training Step: 2589  | total loss: \u001b[1m\u001b[32m0.28611\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 863 | loss: 0.28611 - acc: 0.9663 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2590  | total loss: \u001b[1m\u001b[32m0.27901\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 864 | loss: 0.27901 - acc: 0.9697 -- iter: 08/22\n",
            "Training Step: 2591  | total loss: \u001b[1m\u001b[32m0.27844\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 864 | loss: 0.27844 - acc: 0.9727 -- iter: 16/22\n",
            "Training Step: 2592  | total loss: \u001b[1m\u001b[32m0.28158\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 864 | loss: 0.28158 - acc: 0.9755 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2593  | total loss: \u001b[1m\u001b[32m0.28082\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 865 | loss: 0.28082 - acc: 0.9779 -- iter: 08/22\n",
            "Training Step: 2594  | total loss: \u001b[1m\u001b[32m0.27543\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 865 | loss: 0.27543 - acc: 0.9801 -- iter: 16/22\n",
            "Training Step: 2595  | total loss: \u001b[1m\u001b[32m0.27638\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 865 | loss: 0.27638 - acc: 0.9821 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2596  | total loss: \u001b[1m\u001b[32m0.27594\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 866 | loss: 0.27594 - acc: 0.9839 -- iter: 08/22\n",
            "Training Step: 2597  | total loss: \u001b[1m\u001b[32m0.27288\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 866 | loss: 0.27288 - acc: 0.9855 -- iter: 16/22\n",
            "Training Step: 2598  | total loss: \u001b[1m\u001b[32m0.26891\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 866 | loss: 0.26891 - acc: 0.9870 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2599  | total loss: \u001b[1m\u001b[32m0.27163\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 867 | loss: 0.27163 - acc: 0.9883 -- iter: 08/22\n",
            "Training Step: 2600  | total loss: \u001b[1m\u001b[32m0.26872\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 867 | loss: 0.26872 - acc: 0.9894 -- iter: 16/22\n",
            "Training Step: 2601  | total loss: \u001b[1m\u001b[32m0.26690\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 867 | loss: 0.26690 - acc: 0.9905 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2602  | total loss: \u001b[1m\u001b[32m0.25933\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 868 | loss: 0.25933 - acc: 0.9914 -- iter: 08/22\n",
            "Training Step: 2603  | total loss: \u001b[1m\u001b[32m0.26632\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 868 | loss: 0.26632 - acc: 0.9923 -- iter: 16/22\n",
            "Training Step: 2604  | total loss: \u001b[1m\u001b[32m0.26459\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 868 | loss: 0.26459 - acc: 0.9931 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2605  | total loss: \u001b[1m\u001b[32m0.27492\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 869 | loss: 0.27492 - acc: 0.9938 -- iter: 08/22\n",
            "Training Step: 2606  | total loss: \u001b[1m\u001b[32m0.27328\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 869 | loss: 0.27328 - acc: 0.9944 -- iter: 16/22\n",
            "Training Step: 2607  | total loss: \u001b[1m\u001b[32m0.26297\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 869 | loss: 0.26297 - acc: 0.9949 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2608  | total loss: \u001b[1m\u001b[32m0.27320\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 870 | loss: 0.27320 - acc: 0.9955 -- iter: 08/22\n",
            "Training Step: 2609  | total loss: \u001b[1m\u001b[32m0.27864\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 870 | loss: 0.27864 - acc: 0.9959 -- iter: 16/22\n",
            "Training Step: 2610  | total loss: \u001b[1m\u001b[32m0.27746\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 870 | loss: 0.27746 - acc: 0.9963 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2611  | total loss: \u001b[1m\u001b[32m0.26844\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 871 | loss: 0.26844 - acc: 0.9967 -- iter: 08/22\n",
            "Training Step: 2612  | total loss: \u001b[1m\u001b[32m0.27410\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 871 | loss: 0.27410 - acc: 0.9970 -- iter: 16/22\n",
            "Training Step: 2613  | total loss: \u001b[1m\u001b[32m0.28848\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 871 | loss: 0.28848 - acc: 0.9973 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2614  | total loss: \u001b[1m\u001b[32m0.28407\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 872 | loss: 0.28407 - acc: 0.9976 -- iter: 08/22\n",
            "Training Step: 2615  | total loss: \u001b[1m\u001b[32m0.26939\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 872 | loss: 0.26939 - acc: 0.9978 -- iter: 16/22\n",
            "Training Step: 2616  | total loss: \u001b[1m\u001b[32m0.28391\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 872 | loss: 0.28391 - acc: 0.9980 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2617  | total loss: \u001b[1m\u001b[32m0.28867\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 873 | loss: 0.28867 - acc: 0.9982 -- iter: 08/22\n",
            "Training Step: 2618  | total loss: \u001b[1m\u001b[32m0.28169\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 873 | loss: 0.28169 - acc: 0.9984 -- iter: 16/22\n",
            "Training Step: 2619  | total loss: \u001b[1m\u001b[32m0.27570\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 873 | loss: 0.27570 - acc: 0.9986 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2620  | total loss: \u001b[1m\u001b[32m0.28087\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 874 | loss: 0.28087 - acc: 0.9987 -- iter: 08/22\n",
            "Training Step: 2621  | total loss: \u001b[1m\u001b[32m0.27939\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 874 | loss: 0.27939 - acc: 0.9988 -- iter: 16/22\n",
            "Training Step: 2622  | total loss: \u001b[1m\u001b[32m0.26886\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 874 | loss: 0.26886 - acc: 0.9990 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2623  | total loss: \u001b[1m\u001b[32m0.27280\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 875 | loss: 0.27280 - acc: 0.9991 -- iter: 08/22\n",
            "Training Step: 2624  | total loss: \u001b[1m\u001b[32m0.27201\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 875 | loss: 0.27201 - acc: 0.9992 -- iter: 16/22\n",
            "Training Step: 2625  | total loss: \u001b[1m\u001b[32m0.27111\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 875 | loss: 0.27111 - acc: 0.9992 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2626  | total loss: \u001b[1m\u001b[32m0.26822\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 876 | loss: 0.26822 - acc: 0.9993 -- iter: 08/22\n",
            "Training Step: 2627  | total loss: \u001b[1m\u001b[32m0.26511\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 876 | loss: 0.26511 - acc: 0.9994 -- iter: 16/22\n",
            "Training Step: 2628  | total loss: \u001b[1m\u001b[32m0.26471\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 876 | loss: 0.26471 - acc: 0.9994 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2629  | total loss: \u001b[1m\u001b[32m0.26271\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 877 | loss: 0.26271 - acc: 0.9995 -- iter: 08/22\n",
            "Training Step: 2630  | total loss: \u001b[1m\u001b[32m0.25874\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 877 | loss: 0.25874 - acc: 0.9996 -- iter: 16/22\n",
            "Training Step: 2631  | total loss: \u001b[1m\u001b[32m0.25934\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 877 | loss: 0.25934 - acc: 0.9996 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2632  | total loss: \u001b[1m\u001b[32m0.25786\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 878 | loss: 0.25786 - acc: 0.9996 -- iter: 08/22\n",
            "Training Step: 2633  | total loss: \u001b[1m\u001b[32m0.25668\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 878 | loss: 0.25668 - acc: 0.9997 -- iter: 16/22\n",
            "Training Step: 2634  | total loss: \u001b[1m\u001b[32m0.25294\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 878 | loss: 0.25294 - acc: 0.9997 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2635  | total loss: \u001b[1m\u001b[32m0.25392\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 879 | loss: 0.25392 - acc: 0.9997 -- iter: 08/22\n",
            "Training Step: 2636  | total loss: \u001b[1m\u001b[32m0.25296\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 879 | loss: 0.25296 - acc: 0.9998 -- iter: 16/22\n",
            "Training Step: 2637  | total loss: \u001b[1m\u001b[32m0.24664\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 879 | loss: 0.24664 - acc: 0.9998 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2638  | total loss: \u001b[1m\u001b[32m0.24164\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 880 | loss: 0.24164 - acc: 0.9998 -- iter: 08/22\n",
            "Training Step: 2639  | total loss: \u001b[1m\u001b[32m0.24980\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 880 | loss: 0.24980 - acc: 0.9998 -- iter: 16/22\n",
            "Training Step: 2640  | total loss: \u001b[1m\u001b[32m0.24364\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 880 | loss: 0.24364 - acc: 0.9998 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2641  | total loss: \u001b[1m\u001b[32m0.23568\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 881 | loss: 0.23568 - acc: 0.9999 -- iter: 08/22\n",
            "Training Step: 2642  | total loss: \u001b[1m\u001b[32m0.24644\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 881 | loss: 0.24644 - acc: 0.9999 -- iter: 16/22\n",
            "Training Step: 2643  | total loss: \u001b[1m\u001b[32m0.24097\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 881 | loss: 0.24097 - acc: 0.9999 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2644  | total loss: \u001b[1m\u001b[32m0.23318\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 882 | loss: 0.23318 - acc: 0.9999 -- iter: 08/22\n",
            "Training Step: 2645  | total loss: \u001b[1m\u001b[32m0.24018\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 882 | loss: 0.24018 - acc: 0.9999 -- iter: 16/22\n",
            "Training Step: 2646  | total loss: \u001b[1m\u001b[32m0.23748\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 882 | loss: 0.23748 - acc: 0.9999 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2647  | total loss: \u001b[1m\u001b[32m0.23497\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 883 | loss: 0.23497 - acc: 0.9999 -- iter: 08/22\n",
            "Training Step: 2648  | total loss: \u001b[1m\u001b[32m0.24163\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 883 | loss: 0.24163 - acc: 0.9999 -- iter: 16/22\n",
            "Training Step: 2649  | total loss: \u001b[1m\u001b[32m0.23118\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 883 | loss: 0.23118 - acc: 0.9999 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2650  | total loss: \u001b[1m\u001b[32m0.24419\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 884 | loss: 0.24419 - acc: 0.9999 -- iter: 08/22\n",
            "Training Step: 2651  | total loss: \u001b[1m\u001b[32m0.23817\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 884 | loss: 0.23817 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2652  | total loss: \u001b[1m\u001b[32m0.22802\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 884 | loss: 0.22802 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2653  | total loss: \u001b[1m\u001b[32m0.23264\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 885 | loss: 0.23264 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2654  | total loss: \u001b[1m\u001b[32m0.23214\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 885 | loss: 0.23214 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2655  | total loss: \u001b[1m\u001b[32m0.23000\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 885 | loss: 0.23000 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2656  | total loss: \u001b[1m\u001b[32m0.23423\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 886 | loss: 0.23423 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2657  | total loss: \u001b[1m\u001b[32m0.23088\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 886 | loss: 0.23088 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2658  | total loss: \u001b[1m\u001b[32m0.23916\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 886 | loss: 0.23916 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2659  | total loss: \u001b[1m\u001b[32m0.23273\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 887 | loss: 0.23273 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2660  | total loss: \u001b[1m\u001b[32m0.22941\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 887 | loss: 0.22941 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2661  | total loss: \u001b[1m\u001b[32m0.22432\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 887 | loss: 0.22432 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2662  | total loss: \u001b[1m\u001b[32m0.21808\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 888 | loss: 0.21808 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2663  | total loss: \u001b[1m\u001b[32m0.23012\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 888 | loss: 0.23012 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2664  | total loss: \u001b[1m\u001b[32m0.22486\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 888 | loss: 0.22486 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2665  | total loss: \u001b[1m\u001b[32m0.22103\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 889 | loss: 0.22103 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2666  | total loss: \u001b[1m\u001b[32m0.21849\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 889 | loss: 0.21849 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2667  | total loss: \u001b[1m\u001b[32m0.22607\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 889 | loss: 0.22607 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2668  | total loss: \u001b[1m\u001b[32m0.22200\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 890 | loss: 0.22200 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2669  | total loss: \u001b[1m\u001b[32m0.21967\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 890 | loss: 0.21967 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2670  | total loss: \u001b[1m\u001b[32m0.21260\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 890 | loss: 0.21260 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2671  | total loss: \u001b[1m\u001b[32m0.22408\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 891 | loss: 0.22408 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2672  | total loss: \u001b[1m\u001b[32m0.22138\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 891 | loss: 0.22138 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2673  | total loss: \u001b[1m\u001b[32m0.22649\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 891 | loss: 0.22649 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2674  | total loss: \u001b[1m\u001b[32m0.22526\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 892 | loss: 0.22526 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2675  | total loss: \u001b[1m\u001b[32m0.22296\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 892 | loss: 0.22296 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2676  | total loss: \u001b[1m\u001b[32m0.22775\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 892 | loss: 0.22775 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2677  | total loss: \u001b[1m\u001b[32m0.22228\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 893 | loss: 0.22228 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2678  | total loss: \u001b[1m\u001b[32m0.23500\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 893 | loss: 0.23500 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2679  | total loss: \u001b[1m\u001b[32m0.22518\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 893 | loss: 0.22518 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2680  | total loss: \u001b[1m\u001b[32m0.21986\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 894 | loss: 0.21986 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2681  | total loss: \u001b[1m\u001b[32m0.21573\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 894 | loss: 0.21573 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2682  | total loss: \u001b[1m\u001b[32m0.21896\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 894 | loss: 0.21896 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2683  | total loss: \u001b[1m\u001b[32m0.22004\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 895 | loss: 0.22004 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2684  | total loss: \u001b[1m\u001b[32m0.21579\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 895 | loss: 0.21579 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2685  | total loss: \u001b[1m\u001b[32m0.20096\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 895 | loss: 0.20096 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2686  | total loss: \u001b[1m\u001b[32m0.20630\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 896 | loss: 0.20630 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2687  | total loss: \u001b[1m\u001b[32m0.21586\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 896 | loss: 0.21586 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2688  | total loss: \u001b[1m\u001b[32m0.20096\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 896 | loss: 0.20096 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2689  | total loss: \u001b[1m\u001b[32m0.19878\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 897 | loss: 0.19878 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2690  | total loss: \u001b[1m\u001b[32m0.19962\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 897 | loss: 0.19962 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2691  | total loss: \u001b[1m\u001b[32m0.20578\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 897 | loss: 0.20578 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2692  | total loss: \u001b[1m\u001b[32m0.20299\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 898 | loss: 0.20299 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2693  | total loss: \u001b[1m\u001b[32m0.20330\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 898 | loss: 0.20330 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2694  | total loss: \u001b[1m\u001b[32m0.19426\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 898 | loss: 0.19426 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2695  | total loss: \u001b[1m\u001b[32m0.20792\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 899 | loss: 0.20792 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2696  | total loss: \u001b[1m\u001b[32m0.20758\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 899 | loss: 0.20758 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2697  | total loss: \u001b[1m\u001b[32m0.21070\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 899 | loss: 0.21070 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2698  | total loss: \u001b[1m\u001b[32m0.21847\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 900 | loss: 0.21847 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2699  | total loss: \u001b[1m\u001b[32m0.20929\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 900 | loss: 0.20929 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2700  | total loss: \u001b[1m\u001b[32m0.21207\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 900 | loss: 0.21207 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2701  | total loss: \u001b[1m\u001b[32m0.20160\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 901 | loss: 0.20160 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2702  | total loss: \u001b[1m\u001b[32m0.20120\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 901 | loss: 0.20120 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2703  | total loss: \u001b[1m\u001b[32m0.21224\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 901 | loss: 0.21224 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2704  | total loss: \u001b[1m\u001b[32m0.20151\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 902 | loss: 0.20151 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2705  | total loss: \u001b[1m\u001b[32m0.19808\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 902 | loss: 0.19808 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2706  | total loss: \u001b[1m\u001b[32m0.20631\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 902 | loss: 0.20631 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2707  | total loss: \u001b[1m\u001b[32m0.20350\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 903 | loss: 0.20350 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2708  | total loss: \u001b[1m\u001b[32m0.19987\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 903 | loss: 0.19987 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2709  | total loss: \u001b[1m\u001b[32m0.20093\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 903 | loss: 0.20093 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2710  | total loss: \u001b[1m\u001b[32m0.19898\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 904 | loss: 0.19898 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2711  | total loss: \u001b[1m\u001b[32m0.20321\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 904 | loss: 0.20321 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2712  | total loss: \u001b[1m\u001b[32m0.20374\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 904 | loss: 0.20374 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2713  | total loss: \u001b[1m\u001b[32m0.21804\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 905 | loss: 0.21804 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2714  | total loss: \u001b[1m\u001b[32m0.20568\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 905 | loss: 0.20568 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2715  | total loss: \u001b[1m\u001b[32m0.20729\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 905 | loss: 0.20729 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2716  | total loss: \u001b[1m\u001b[32m0.22095\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 906 | loss: 0.22095 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2717  | total loss: \u001b[1m\u001b[32m0.22817\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 906 | loss: 0.22817 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2718  | total loss: \u001b[1m\u001b[32m0.22315\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 906 | loss: 0.22315 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2719  | total loss: \u001b[1m\u001b[32m0.21820\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 907 | loss: 0.21820 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2720  | total loss: \u001b[1m\u001b[32m0.22538\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 907 | loss: 0.22538 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2721  | total loss: \u001b[1m\u001b[32m0.23656\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 907 | loss: 0.23656 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2722  | total loss: \u001b[1m\u001b[32m0.22794\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 908 | loss: 0.22794 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2723  | total loss: \u001b[1m\u001b[32m0.22151\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 908 | loss: 0.22151 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2724  | total loss: \u001b[1m\u001b[32m0.23264\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 908 | loss: 0.23264 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2725  | total loss: \u001b[1m\u001b[32m0.22934\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 909 | loss: 0.22934 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2726  | total loss: \u001b[1m\u001b[32m0.22668\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 909 | loss: 0.22668 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2727  | total loss: \u001b[1m\u001b[32m0.22484\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 909 | loss: 0.22484 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2728  | total loss: \u001b[1m\u001b[32m0.22218\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 910 | loss: 0.22218 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2729  | total loss: \u001b[1m\u001b[32m0.21817\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 910 | loss: 0.21817 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2730  | total loss: \u001b[1m\u001b[32m0.21683\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 910 | loss: 0.21683 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2731  | total loss: \u001b[1m\u001b[32m0.21657\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 911 | loss: 0.21657 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2732  | total loss: \u001b[1m\u001b[32m0.21294\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 911 | loss: 0.21294 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2733  | total loss: \u001b[1m\u001b[32m0.22310\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 911 | loss: 0.22310 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2734  | total loss: \u001b[1m\u001b[32m0.21347\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 912 | loss: 0.21347 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2735  | total loss: \u001b[1m\u001b[32m0.21102\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 912 | loss: 0.21102 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2736  | total loss: \u001b[1m\u001b[32m0.22106\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 912 | loss: 0.22106 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2737  | total loss: \u001b[1m\u001b[32m0.21638\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 913 | loss: 0.21638 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2738  | total loss: \u001b[1m\u001b[32m0.22093\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 913 | loss: 0.22093 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2739  | total loss: \u001b[1m\u001b[32m0.21422\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 913 | loss: 0.21422 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2740  | total loss: \u001b[1m\u001b[32m0.20994\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 914 | loss: 0.20994 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2741  | total loss: \u001b[1m\u001b[32m0.20948\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 914 | loss: 0.20948 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2742  | total loss: \u001b[1m\u001b[32m0.20126\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 914 | loss: 0.20126 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2743  | total loss: \u001b[1m\u001b[32m0.20712\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 915 | loss: 0.20712 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2744  | total loss: \u001b[1m\u001b[32m0.20089\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 915 | loss: 0.20089 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2745  | total loss: \u001b[1m\u001b[32m0.20089\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 915 | loss: 0.20089 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2746  | total loss: \u001b[1m\u001b[32m0.20195\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 916 | loss: 0.20195 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2747  | total loss: \u001b[1m\u001b[32m0.20318\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 916 | loss: 0.20318 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2748  | total loss: \u001b[1m\u001b[32m0.19745\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 916 | loss: 0.19745 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2749  | total loss: \u001b[1m\u001b[32m0.20142\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 917 | loss: 0.20142 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2750  | total loss: \u001b[1m\u001b[32m0.19859\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 917 | loss: 0.19859 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2751  | total loss: \u001b[1m\u001b[32m0.19683\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 917 | loss: 0.19683 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2752  | total loss: \u001b[1m\u001b[32m0.20064\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 918 | loss: 0.20064 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2753  | total loss: \u001b[1m\u001b[32m0.19492\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 918 | loss: 0.19492 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2754  | total loss: \u001b[1m\u001b[32m0.19492\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 918 | loss: 0.19492 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2755  | total loss: \u001b[1m\u001b[32m0.19961\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 919 | loss: 0.19961 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2756  | total loss: \u001b[1m\u001b[32m0.20469\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 919 | loss: 0.20469 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2757  | total loss: \u001b[1m\u001b[32m0.19154\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 919 | loss: 0.19154 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2758  | total loss: \u001b[1m\u001b[32m0.20413\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 920 | loss: 0.20413 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2759  | total loss: \u001b[1m\u001b[32m0.19884\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 920 | loss: 0.19884 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2760  | total loss: \u001b[1m\u001b[32m0.18616\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 920 | loss: 0.18616 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2761  | total loss: \u001b[1m\u001b[32m0.18898\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 921 | loss: 0.18898 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2762  | total loss: \u001b[1m\u001b[32m0.18772\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 921 | loss: 0.18772 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2763  | total loss: \u001b[1m\u001b[32m0.18726\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 921 | loss: 0.18726 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2764  | total loss: \u001b[1m\u001b[32m0.18963\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 922 | loss: 0.18963 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2765  | total loss: \u001b[1m\u001b[32m0.18223\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 922 | loss: 0.18223 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2766  | total loss: \u001b[1m\u001b[32m0.17391\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 922 | loss: 0.17391 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2767  | total loss: \u001b[1m\u001b[32m0.18974\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 923 | loss: 0.18974 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2768  | total loss: \u001b[1m\u001b[32m0.18203\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 923 | loss: 0.18203 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2769  | total loss: \u001b[1m\u001b[32m0.17630\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 923 | loss: 0.17630 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2770  | total loss: \u001b[1m\u001b[32m0.17658\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 924 | loss: 0.17658 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2771  | total loss: \u001b[1m\u001b[32m0.18306\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 924 | loss: 0.18306 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2772  | total loss: \u001b[1m\u001b[32m0.17709\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 924 | loss: 0.17709 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2773  | total loss: \u001b[1m\u001b[32m0.16656\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 925 | loss: 0.16656 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2774  | total loss: \u001b[1m\u001b[32m0.16565\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 925 | loss: 0.16565 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2775  | total loss: \u001b[1m\u001b[32m0.17896\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 925 | loss: 0.17896 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2776  | total loss: \u001b[1m\u001b[32m0.16818\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 926 | loss: 0.16818 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2777  | total loss: \u001b[1m\u001b[32m0.17176\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 926 | loss: 0.17176 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2778  | total loss: \u001b[1m\u001b[32m0.16381\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 926 | loss: 0.16381 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2779  | total loss: \u001b[1m\u001b[32m0.17356\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 927 | loss: 0.17356 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2780  | total loss: \u001b[1m\u001b[32m0.17643\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 927 | loss: 0.17643 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2781  | total loss: \u001b[1m\u001b[32m0.17812\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 927 | loss: 0.17812 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2782  | total loss: \u001b[1m\u001b[32m0.17199\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 928 | loss: 0.17199 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2783  | total loss: \u001b[1m\u001b[32m0.17877\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 928 | loss: 0.17877 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2784  | total loss: \u001b[1m\u001b[32m0.18013\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 928 | loss: 0.18013 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2785  | total loss: \u001b[1m\u001b[32m0.17833\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 929 | loss: 0.17833 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2786  | total loss: \u001b[1m\u001b[32m0.18072\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 929 | loss: 0.18072 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2787  | total loss: \u001b[1m\u001b[32m0.18010\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 929 | loss: 0.18010 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2788  | total loss: \u001b[1m\u001b[32m0.17801\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 930 | loss: 0.17801 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2789  | total loss: \u001b[1m\u001b[32m0.17141\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 930 | loss: 0.17141 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2790  | total loss: \u001b[1m\u001b[32m0.17778\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 930 | loss: 0.17778 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2791  | total loss: \u001b[1m\u001b[32m0.17726\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 931 | loss: 0.17726 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2792  | total loss: \u001b[1m\u001b[32m0.17071\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 931 | loss: 0.17071 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2793  | total loss: \u001b[1m\u001b[32m0.17355\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 931 | loss: 0.17355 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2794  | total loss: \u001b[1m\u001b[32m0.17492\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 932 | loss: 0.17492 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2795  | total loss: \u001b[1m\u001b[32m0.17250\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 932 | loss: 0.17250 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2796  | total loss: \u001b[1m\u001b[32m0.17501\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 932 | loss: 0.17501 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2797  | total loss: \u001b[1m\u001b[32m0.17427\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 933 | loss: 0.17427 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2798  | total loss: \u001b[1m\u001b[32m0.17397\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 933 | loss: 0.17397 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2799  | total loss: \u001b[1m\u001b[32m0.17516\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 933 | loss: 0.17516 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2800  | total loss: \u001b[1m\u001b[32m0.17427\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 934 | loss: 0.17427 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2801  | total loss: \u001b[1m\u001b[32m0.18522\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 934 | loss: 0.18522 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2802  | total loss: \u001b[1m\u001b[32m0.18162\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 934 | loss: 0.18162 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2803  | total loss: \u001b[1m\u001b[32m0.17524\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 935 | loss: 0.17524 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2804  | total loss: \u001b[1m\u001b[32m0.18560\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 935 | loss: 0.18560 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2805  | total loss: \u001b[1m\u001b[32m0.19340\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 935 | loss: 0.19340 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2806  | total loss: \u001b[1m\u001b[32m0.19134\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 936 | loss: 0.19134 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2807  | total loss: \u001b[1m\u001b[32m0.18260\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 936 | loss: 0.18260 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2808  | total loss: \u001b[1m\u001b[32m0.19042\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 936 | loss: 0.19042 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2809  | total loss: \u001b[1m\u001b[32m0.18157\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 937 | loss: 0.18157 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2810  | total loss: \u001b[1m\u001b[32m0.18872\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 937 | loss: 0.18872 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2811  | total loss: \u001b[1m\u001b[32m0.18392\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 937 | loss: 0.18392 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2812  | total loss: \u001b[1m\u001b[32m0.17571\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 938 | loss: 0.17571 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2813  | total loss: \u001b[1m\u001b[32m0.17136\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 938 | loss: 0.17136 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2814  | total loss: \u001b[1m\u001b[32m0.17090\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 938 | loss: 0.17090 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2815  | total loss: \u001b[1m\u001b[32m0.17380\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 939 | loss: 0.17380 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2816  | total loss: \u001b[1m\u001b[32m0.16951\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 939 | loss: 0.16951 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2817  | total loss: \u001b[1m\u001b[32m0.15930\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 939 | loss: 0.15930 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2818  | total loss: \u001b[1m\u001b[32m0.16431\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 940 | loss: 0.16431 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2819  | total loss: \u001b[1m\u001b[32m0.16804\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 940 | loss: 0.16804 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2820  | total loss: \u001b[1m\u001b[32m0.15791\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 940 | loss: 0.15791 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2821  | total loss: \u001b[1m\u001b[32m0.16325\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 941 | loss: 0.16325 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2822  | total loss: \u001b[1m\u001b[32m0.15738\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 941 | loss: 0.15738 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2823  | total loss: \u001b[1m\u001b[32m0.16110\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 941 | loss: 0.16110 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2824  | total loss: \u001b[1m\u001b[32m0.16597\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 942 | loss: 0.16597 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2825  | total loss: \u001b[1m\u001b[32m0.16697\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 942 | loss: 0.16697 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2826  | total loss: \u001b[1m\u001b[32m0.16019\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 942 | loss: 0.16019 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2827  | total loss: \u001b[1m\u001b[32m0.16642\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 943 | loss: 0.16642 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2828  | total loss: \u001b[1m\u001b[32m1.41288\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 943 | loss: 1.41288 - acc: 0.9000 -- iter: 16/22\n",
            "Training Step: 2829  | total loss: \u001b[1m\u001b[32m1.27959\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 943 | loss: 1.27959 - acc: 0.9100 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2830  | total loss: \u001b[1m\u001b[32m1.17108\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 944 | loss: 1.17108 - acc: 0.9190 -- iter: 08/22\n",
            "Training Step: 2831  | total loss: \u001b[1m\u001b[32m1.07375\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 944 | loss: 1.07375 - acc: 0.9271 -- iter: 16/22\n",
            "Training Step: 2832  | total loss: \u001b[1m\u001b[32m0.97432\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 944 | loss: 0.97432 - acc: 0.9344 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2833  | total loss: \u001b[1m\u001b[32m0.89226\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 945 | loss: 0.89226 - acc: 0.9410 -- iter: 08/22\n",
            "Training Step: 2834  | total loss: \u001b[1m\u001b[32m0.81718\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 945 | loss: 0.81718 - acc: 0.9469 -- iter: 16/22\n",
            "Training Step: 2835  | total loss: \u001b[1m\u001b[32m0.75514\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 945 | loss: 0.75514 - acc: 0.9522 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2836  | total loss: \u001b[1m\u001b[32m0.69500\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 946 | loss: 0.69500 - acc: 0.9570 -- iter: 08/22\n",
            "Training Step: 2837  | total loss: \u001b[1m\u001b[32m0.64864\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 946 | loss: 0.64864 - acc: 0.9613 -- iter: 16/22\n",
            "Training Step: 2838  | total loss: \u001b[1m\u001b[32m0.60165\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 946 | loss: 0.60165 - acc: 0.9651 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2839  | total loss: \u001b[1m\u001b[32m0.55163\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 947 | loss: 0.55163 - acc: 0.9686 -- iter: 08/22\n",
            "Training Step: 2840  | total loss: \u001b[1m\u001b[32m0.51951\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 947 | loss: 0.51951 - acc: 0.9718 -- iter: 16/22\n",
            "Training Step: 2841  | total loss: \u001b[1m\u001b[32m0.48298\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 947 | loss: 0.48298 - acc: 0.9746 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2842  | total loss: \u001b[1m\u001b[32m0.45239\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 948 | loss: 0.45239 - acc: 0.9771 -- iter: 08/22\n",
            "Training Step: 2843  | total loss: \u001b[1m\u001b[32m0.42314\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 948 | loss: 0.42314 - acc: 0.9794 -- iter: 16/22\n",
            "Training Step: 2844  | total loss: \u001b[1m\u001b[32m0.39608\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 948 | loss: 0.39608 - acc: 0.9815 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2845  | total loss: \u001b[1m\u001b[32m0.37393\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 949 | loss: 0.37393 - acc: 0.9833 -- iter: 08/22\n",
            "Training Step: 2846  | total loss: \u001b[1m\u001b[32m0.35460\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 949 | loss: 0.35460 - acc: 0.9850 -- iter: 16/22\n",
            "Training Step: 2847  | total loss: \u001b[1m\u001b[32m0.33301\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 949 | loss: 0.33301 - acc: 0.9865 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2848  | total loss: \u001b[1m\u001b[32m0.31704\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 950 | loss: 0.31704 - acc: 0.9878 -- iter: 08/22\n",
            "Training Step: 2849  | total loss: \u001b[1m\u001b[32m0.29789\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 950 | loss: 0.29789 - acc: 0.9891 -- iter: 16/22\n",
            "Training Step: 2850  | total loss: \u001b[1m\u001b[32m0.29049\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 950 | loss: 0.29049 - acc: 0.9902 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2851  | total loss: \u001b[1m\u001b[32m0.27445\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 951 | loss: 0.27445 - acc: 0.9911 -- iter: 08/22\n",
            "Training Step: 2852  | total loss: \u001b[1m\u001b[32m0.25940\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 951 | loss: 0.25940 - acc: 0.9920 -- iter: 16/22\n",
            "Training Step: 2853  | total loss: \u001b[1m\u001b[32m0.24620\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 951 | loss: 0.24620 - acc: 0.9928 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2854  | total loss: \u001b[1m\u001b[32m0.24072\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 952 | loss: 0.24072 - acc: 0.9935 -- iter: 08/22\n",
            "Training Step: 2855  | total loss: \u001b[1m\u001b[32m0.23230\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 952 | loss: 0.23230 - acc: 0.9942 -- iter: 16/22\n",
            "Training Step: 2856  | total loss: \u001b[1m\u001b[32m0.22178\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 952 | loss: 0.22178 - acc: 0.9948 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2857  | total loss: \u001b[1m\u001b[32m0.22183\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 953 | loss: 0.22183 - acc: 0.9953 -- iter: 08/22\n",
            "Training Step: 2858  | total loss: \u001b[1m\u001b[32m0.21207\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 953 | loss: 0.21207 - acc: 0.9958 -- iter: 16/22\n",
            "Training Step: 2859  | total loss: \u001b[1m\u001b[32m0.20585\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 953 | loss: 0.20585 - acc: 0.9962 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2860  | total loss: \u001b[1m\u001b[32m0.20726\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 954 | loss: 0.20726 - acc: 0.9966 -- iter: 08/22\n",
            "Training Step: 2861  | total loss: \u001b[1m\u001b[32m0.20738\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 954 | loss: 0.20738 - acc: 0.9969 -- iter: 16/22\n",
            "Training Step: 2862  | total loss: \u001b[1m\u001b[32m0.20669\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 954 | loss: 0.20669 - acc: 0.9972 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2863  | total loss: \u001b[1m\u001b[32m0.19410\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 955 | loss: 0.19410 - acc: 0.9975 -- iter: 08/22\n",
            "Training Step: 2864  | total loss: \u001b[1m\u001b[32m0.19528\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 955 | loss: 0.19528 - acc: 0.9977 -- iter: 16/22\n",
            "Training Step: 2865  | total loss: \u001b[1m\u001b[32m0.18740\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 955 | loss: 0.18740 - acc: 0.9980 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2866  | total loss: \u001b[1m\u001b[32m0.17557\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 956 | loss: 0.17557 - acc: 0.9982 -- iter: 08/22\n",
            "Training Step: 2867  | total loss: \u001b[1m\u001b[32m0.18568\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 956 | loss: 0.18568 - acc: 0.9984 -- iter: 16/22\n",
            "Training Step: 2868  | total loss: \u001b[1m\u001b[32m0.17849\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 956 | loss: 0.17849 - acc: 0.9985 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2869  | total loss: \u001b[1m\u001b[32m0.17754\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 957 | loss: 0.17754 - acc: 0.9987 -- iter: 08/22\n",
            "Training Step: 2870  | total loss: \u001b[1m\u001b[32m0.16694\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 957 | loss: 0.16694 - acc: 0.9988 -- iter: 16/22\n",
            "Training Step: 2871  | total loss: \u001b[1m\u001b[32m0.17337\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 957 | loss: 0.17337 - acc: 0.9989 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2872  | total loss: \u001b[1m\u001b[32m0.17259\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 958 | loss: 0.17259 - acc: 0.9990 -- iter: 08/22\n",
            "Training Step: 2873  | total loss: \u001b[1m\u001b[32m0.16367\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 958 | loss: 0.16367 - acc: 0.9991 -- iter: 16/22\n",
            "Training Step: 2874  | total loss: \u001b[1m\u001b[32m0.17130\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 958 | loss: 0.17130 - acc: 0.9992 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2875  | total loss: \u001b[1m\u001b[32m0.16631\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 959 | loss: 0.16631 - acc: 0.9993 -- iter: 08/22\n",
            "Training Step: 2876  | total loss: \u001b[1m\u001b[32m0.81809\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 959 | loss: 0.81809 - acc: 0.9327 -- iter: 16/22\n",
            "Training Step: 2877  | total loss: \u001b[1m\u001b[32m0.74935\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 959 | loss: 0.74935 - acc: 0.9394 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2878  | total loss: \u001b[1m\u001b[32m0.69395\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 960 | loss: 0.69395 - acc: 0.9455 -- iter: 08/22\n",
            "Training Step: 2879  | total loss: \u001b[1m\u001b[32m0.63748\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 960 | loss: 0.63748 - acc: 0.9509 -- iter: 16/22\n",
            "Training Step: 2880  | total loss: \u001b[1m\u001b[32m0.58661\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 960 | loss: 0.58661 - acc: 0.9558 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2881  | total loss: \u001b[1m\u001b[32m0.54599\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 961 | loss: 0.54599 - acc: 0.9603 -- iter: 08/22\n",
            "Training Step: 2882  | total loss: \u001b[1m\u001b[32m0.50335\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 961 | loss: 0.50335 - acc: 0.9642 -- iter: 16/22\n",
            "Training Step: 2883  | total loss: \u001b[1m\u001b[32m0.46984\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 961 | loss: 0.46984 - acc: 0.9678 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2884  | total loss: \u001b[1m\u001b[32m0.44093\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 962 | loss: 0.44093 - acc: 0.9710 -- iter: 08/22\n",
            "Training Step: 2885  | total loss: \u001b[1m\u001b[32m0.42006\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 962 | loss: 0.42006 - acc: 0.9739 -- iter: 16/22\n",
            "Training Step: 2886  | total loss: \u001b[1m\u001b[32m0.39272\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 962 | loss: 0.39272 - acc: 0.9765 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2887  | total loss: \u001b[1m\u001b[32m0.36381\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 963 | loss: 0.36381 - acc: 0.9789 -- iter: 08/22\n",
            "Training Step: 2888  | total loss: \u001b[1m\u001b[32m0.35049\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 963 | loss: 0.35049 - acc: 0.9810 -- iter: 16/22\n",
            "Training Step: 2889  | total loss: \u001b[1m\u001b[32m0.32929\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 963 | loss: 0.32929 - acc: 0.9829 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2890  | total loss: \u001b[1m\u001b[32m0.31317\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 964 | loss: 0.31317 - acc: 0.9846 -- iter: 08/22\n",
            "Training Step: 2891  | total loss: \u001b[1m\u001b[32m0.29687\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 964 | loss: 0.29687 - acc: 0.9861 -- iter: 16/22\n",
            "Training Step: 2892  | total loss: \u001b[1m\u001b[32m0.28098\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 964 | loss: 0.28098 - acc: 0.9875 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2893  | total loss: \u001b[1m\u001b[32m0.27006\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 965 | loss: 0.27006 - acc: 0.9888 -- iter: 08/22\n",
            "Training Step: 2894  | total loss: \u001b[1m\u001b[32m0.25988\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 965 | loss: 0.25988 - acc: 0.9899 -- iter: 16/22\n",
            "Training Step: 2895  | total loss: \u001b[1m\u001b[32m0.24613\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 965 | loss: 0.24613 - acc: 0.9909 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2896  | total loss: \u001b[1m\u001b[32m0.23848\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 966 | loss: 0.23848 - acc: 0.9918 -- iter: 08/22\n",
            "Training Step: 2897  | total loss: \u001b[1m\u001b[32m0.23419\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 966 | loss: 0.23419 - acc: 0.9926 -- iter: 16/22\n",
            "Training Step: 2898  | total loss: \u001b[1m\u001b[32m0.22341\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 966 | loss: 0.22341 - acc: 0.9934 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2899  | total loss: \u001b[1m\u001b[32m0.21532\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 967 | loss: 0.21532 - acc: 0.9940 -- iter: 08/22\n",
            "Training Step: 2900  | total loss: \u001b[1m\u001b[32m0.21309\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 967 | loss: 0.21309 - acc: 0.9946 -- iter: 16/22\n",
            "Training Step: 2901  | total loss: \u001b[1m\u001b[32m0.21071\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 967 | loss: 0.21071 - acc: 0.9952 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2902  | total loss: \u001b[1m\u001b[32m0.20042\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 968 | loss: 0.20042 - acc: 0.9957 -- iter: 08/22\n",
            "Training Step: 2903  | total loss: \u001b[1m\u001b[32m0.19656\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 968 | loss: 0.19656 - acc: 0.9961 -- iter: 16/22\n",
            "Training Step: 2904  | total loss: \u001b[1m\u001b[32m0.19555\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 968 | loss: 0.19555 - acc: 0.9965 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2905  | total loss: \u001b[1m\u001b[32m0.18314\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 969 | loss: 0.18314 - acc: 0.9968 -- iter: 08/22\n",
            "Training Step: 2906  | total loss: \u001b[1m\u001b[32m0.18532\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 969 | loss: 0.18532 - acc: 0.9971 -- iter: 16/22\n",
            "Training Step: 2907  | total loss: \u001b[1m\u001b[32m0.18162\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 969 | loss: 0.18162 - acc: 0.9974 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2908  | total loss: \u001b[1m\u001b[32m0.17045\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 970 | loss: 0.17045 - acc: 0.9977 -- iter: 08/22\n",
            "Training Step: 2909  | total loss: \u001b[1m\u001b[32m0.16844\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 970 | loss: 0.16844 - acc: 0.9979 -- iter: 16/22\n",
            "Training Step: 2910  | total loss: \u001b[1m\u001b[32m0.15964\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 970 | loss: 0.15964 - acc: 0.9981 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2911  | total loss: \u001b[1m\u001b[32m0.16473\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 971 | loss: 0.16473 - acc: 0.9983 -- iter: 08/22\n",
            "Training Step: 2912  | total loss: \u001b[1m\u001b[32m0.16307\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 971 | loss: 0.16307 - acc: 0.9985 -- iter: 16/22\n",
            "Training Step: 2913  | total loss: \u001b[1m\u001b[32m0.16240\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 971 | loss: 0.16240 - acc: 0.9986 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2914  | total loss: \u001b[1m\u001b[32m0.15597\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 972 | loss: 0.15597 - acc: 0.9988 -- iter: 08/22\n",
            "Training Step: 2915  | total loss: \u001b[1m\u001b[32m0.15892\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 972 | loss: 0.15892 - acc: 0.9989 -- iter: 16/22\n",
            "Training Step: 2916  | total loss: \u001b[1m\u001b[32m0.15840\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 972 | loss: 0.15840 - acc: 0.9990 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2917  | total loss: \u001b[1m\u001b[32m0.16005\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 973 | loss: 0.16005 - acc: 0.9991 -- iter: 08/22\n",
            "Training Step: 2918  | total loss: \u001b[1m\u001b[32m0.16055\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 973 | loss: 0.16055 - acc: 0.9993 -- iter: 16/22\n",
            "Training Step: 2919  | total loss: \u001b[1m\u001b[32m0.15467\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 973 | loss: 0.15467 - acc: 0.9993 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2920  | total loss: \u001b[1m\u001b[32m0.15654\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 974 | loss: 0.15654 - acc: 0.9993 -- iter: 08/22\n",
            "Training Step: 2921  | total loss: \u001b[1m\u001b[32m0.15725\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 974 | loss: 0.15725 - acc: 0.9994 -- iter: 16/22\n",
            "Training Step: 2922  | total loss: \u001b[1m\u001b[32m0.15727\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 974 | loss: 0.15727 - acc: 0.9995 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2923  | total loss: \u001b[1m\u001b[32m0.15304\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 975 | loss: 0.15304 - acc: 0.9995 -- iter: 08/22\n",
            "Training Step: 2924  | total loss: \u001b[1m\u001b[32m0.15388\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 975 | loss: 0.15388 - acc: 0.9996 -- iter: 16/22\n",
            "Training Step: 2925  | total loss: \u001b[1m\u001b[32m0.15519\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 975 | loss: 0.15519 - acc: 0.9996 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2926  | total loss: \u001b[1m\u001b[32m0.15776\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 976 | loss: 0.15776 - acc: 0.9997 -- iter: 08/22\n",
            "Training Step: 2927  | total loss: \u001b[1m\u001b[32m0.15060\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 976 | loss: 0.15060 - acc: 0.9997 -- iter: 16/22\n",
            "Training Step: 2928  | total loss: \u001b[1m\u001b[32m0.15205\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 976 | loss: 0.15205 - acc: 0.9997 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2929  | total loss: \u001b[1m\u001b[32m0.15780\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 977 | loss: 0.15780 - acc: 0.9997 -- iter: 08/22\n",
            "Training Step: 2930  | total loss: \u001b[1m\u001b[32m0.15460\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 977 | loss: 0.15460 - acc: 0.9998 -- iter: 16/22\n",
            "Training Step: 2931  | total loss: \u001b[1m\u001b[32m0.14974\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 977 | loss: 0.14974 - acc: 0.9998 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2932  | total loss: \u001b[1m\u001b[32m0.15555\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 978 | loss: 0.15555 - acc: 0.9998 -- iter: 08/22\n",
            "Training Step: 2933  | total loss: \u001b[1m\u001b[32m0.15667\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 978 | loss: 0.15667 - acc: 0.9998 -- iter: 16/22\n",
            "Training Step: 2934  | total loss: \u001b[1m\u001b[32m0.15462\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 978 | loss: 0.15462 - acc: 0.9999 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2935  | total loss: \u001b[1m\u001b[32m0.15153\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 979 | loss: 0.15153 - acc: 0.9999 -- iter: 08/22\n",
            "Training Step: 2936  | total loss: \u001b[1m\u001b[32m0.15284\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 979 | loss: 0.15284 - acc: 0.9999 -- iter: 16/22\n",
            "Training Step: 2937  | total loss: \u001b[1m\u001b[32m0.15671\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 979 | loss: 0.15671 - acc: 0.9999 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2938  | total loss: \u001b[1m\u001b[32m0.15678\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 980 | loss: 0.15678 - acc: 0.9999 -- iter: 08/22\n",
            "Training Step: 2939  | total loss: \u001b[1m\u001b[32m0.14904\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 980 | loss: 0.14904 - acc: 0.9999 -- iter: 16/22\n",
            "Training Step: 2940  | total loss: \u001b[1m\u001b[32m0.15303\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 980 | loss: 0.15303 - acc: 0.9999 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2941  | total loss: \u001b[1m\u001b[32m0.14743\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 981 | loss: 0.14743 - acc: 0.9999 -- iter: 08/22\n",
            "Training Step: 2942  | total loss: \u001b[1m\u001b[32m0.14833\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 981 | loss: 0.14833 - acc: 0.9999 -- iter: 16/22\n",
            "Training Step: 2943  | total loss: \u001b[1m\u001b[32m0.14812\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 981 | loss: 0.14812 - acc: 0.9999 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2944  | total loss: \u001b[1m\u001b[32m0.14297\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 982 | loss: 0.14297 - acc: 0.9999 -- iter: 08/22\n",
            "Training Step: 2945  | total loss: \u001b[1m\u001b[32m0.14088\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 982 | loss: 0.14088 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2946  | total loss: \u001b[1m\u001b[32m0.14520\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 982 | loss: 0.14520 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2947  | total loss: \u001b[1m\u001b[32m0.14051\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 983 | loss: 0.14051 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2948  | total loss: \u001b[1m\u001b[32m0.13817\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 983 | loss: 0.13817 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2949  | total loss: \u001b[1m\u001b[32m0.13795\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 983 | loss: 0.13795 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2950  | total loss: \u001b[1m\u001b[32m0.14135\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 984 | loss: 0.14135 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2951  | total loss: \u001b[1m\u001b[32m0.13670\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 984 | loss: 0.13670 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2952  | total loss: \u001b[1m\u001b[32m0.13651\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 984 | loss: 0.13651 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2953  | total loss: \u001b[1m\u001b[32m0.14660\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 985 | loss: 0.14660 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2954  | total loss: \u001b[1m\u001b[32m0.14033\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 985 | loss: 0.14033 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2955  | total loss: \u001b[1m\u001b[32m0.13673\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 985 | loss: 0.13673 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2956  | total loss: \u001b[1m\u001b[32m0.14673\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 986 | loss: 0.14673 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2957  | total loss: \u001b[1m\u001b[32m0.14059\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 986 | loss: 0.14059 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2958  | total loss: \u001b[1m\u001b[32m0.14028\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 986 | loss: 0.14028 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2959  | total loss: \u001b[1m\u001b[32m0.14240\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 987 | loss: 0.14240 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2960  | total loss: \u001b[1m\u001b[32m0.13661\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 987 | loss: 0.13661 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2961  | total loss: \u001b[1m\u001b[32m0.14233\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 987 | loss: 0.14233 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2962  | total loss: \u001b[1m\u001b[32m0.13865\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 988 | loss: 0.13865 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2963  | total loss: \u001b[1m\u001b[32m0.13571\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 988 | loss: 0.13571 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2964  | total loss: \u001b[1m\u001b[32m0.14140\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 988 | loss: 0.14140 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2965  | total loss: \u001b[1m\u001b[32m0.14795\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 989 | loss: 0.14795 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2966  | total loss: \u001b[1m\u001b[32m0.14661\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 989 | loss: 0.14661 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2967  | total loss: \u001b[1m\u001b[32m0.13872\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 989 | loss: 0.13872 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2968  | total loss: \u001b[1m\u001b[32m0.14535\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 990 | loss: 0.14535 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2969  | total loss: \u001b[1m\u001b[32m0.14245\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 990 | loss: 0.14245 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2970  | total loss: \u001b[1m\u001b[32m0.14412\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 990 | loss: 0.14412 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2971  | total loss: \u001b[1m\u001b[32m0.14048\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 991 | loss: 0.14048 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2972  | total loss: \u001b[1m\u001b[32m0.13495\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 991 | loss: 0.13495 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2973  | total loss: \u001b[1m\u001b[32m0.13495\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 991 | loss: 0.13495 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2974  | total loss: \u001b[1m\u001b[32m0.13818\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 992 | loss: 0.13818 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2975  | total loss: \u001b[1m\u001b[32m0.13468\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 992 | loss: 0.13468 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2976  | total loss: \u001b[1m\u001b[32m0.13183\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 992 | loss: 0.13183 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2977  | total loss: \u001b[1m\u001b[32m0.13624\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 993 | loss: 0.13624 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2978  | total loss: \u001b[1m\u001b[32m0.13877\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 993 | loss: 0.13877 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2979  | total loss: \u001b[1m\u001b[32m0.13045\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 993 | loss: 0.13045 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2980  | total loss: \u001b[1m\u001b[32m0.13477\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 994 | loss: 0.13477 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2981  | total loss: \u001b[1m\u001b[32m0.14357\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 994 | loss: 0.14357 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2982  | total loss: \u001b[1m\u001b[32m0.14199\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 994 | loss: 0.14199 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2983  | total loss: \u001b[1m\u001b[32m0.13288\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 995 | loss: 0.13288 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2984  | total loss: \u001b[1m\u001b[32m0.14173\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 995 | loss: 0.14173 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2985  | total loss: \u001b[1m\u001b[32m0.14134\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 995 | loss: 0.14134 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2986  | total loss: \u001b[1m\u001b[32m0.14031\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 996 | loss: 0.14031 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2987  | total loss: \u001b[1m\u001b[32m0.13710\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 996 | loss: 0.13710 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2988  | total loss: \u001b[1m\u001b[32m0.13709\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 996 | loss: 0.13709 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2989  | total loss: \u001b[1m\u001b[32m0.13351\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 997 | loss: 0.13351 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2990  | total loss: \u001b[1m\u001b[32m0.12723\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 997 | loss: 0.12723 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2991  | total loss: \u001b[1m\u001b[32m0.13380\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 997 | loss: 0.13380 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2992  | total loss: \u001b[1m\u001b[32m0.13049\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 998 | loss: 0.13049 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2993  | total loss: \u001b[1m\u001b[32m0.12662\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 998 | loss: 0.12662 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2994  | total loss: \u001b[1m\u001b[32m0.13162\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 998 | loss: 0.13162 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2995  | total loss: \u001b[1m\u001b[32m0.12765\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 999 | loss: 0.12765 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2996  | total loss: \u001b[1m\u001b[32m0.12396\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 999 | loss: 0.12396 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 2997  | total loss: \u001b[1m\u001b[32m0.12962\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 999 | loss: 0.12962 - acc: 1.0000 -- iter: 22/22\n",
            "--\n",
            "Training Step: 2998  | total loss: \u001b[1m\u001b[32m0.12394\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1000 | loss: 0.12394 - acc: 1.0000 -- iter: 08/22\n",
            "Training Step: 2999  | total loss: \u001b[1m\u001b[32m0.12418\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 1000 | loss: 0.12418 - acc: 1.0000 -- iter: 16/22\n",
            "Training Step: 3000  | total loss: \u001b[1m\u001b[32m0.12973\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 1000 | loss: 0.12973 - acc: 1.0000 -- iter: 22/22\n",
            "--\n"
          ]
        }
      ],
      "source": [
        "tensorflow.compat.v1.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net, checkpoint_path='model.tfl.ckpt')\n",
        "\n",
        "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save(\"model.tflearn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z__aD-0D-MiM"
      },
      "outputs": [],
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    \n",
        "    return sentence_words\n",
        "\n",
        "def bag_of_words(s, words, show_details = False):\n",
        "\n",
        "    sentence_words = clean_up_sentence(s)\n",
        "\n",
        "    bag = [0]*len(words)\n",
        "\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "\n",
        "    return numpy.array(bag)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KNhLSqKIBWaD"
      },
      "outputs": [],
      "source": [
        "context = {}\n",
        "\n",
        "ERROR_THRESHOLD = 0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "10YipyYTBVNN"
      },
      "outputs": [],
      "source": [
        "def classify(sentence):\n",
        "    # generate probabilities from the model\n",
        "    results = model.predict([bag_of_words(sentence, words)])[0]\n",
        "    # filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((labels[r[0]], r[1]))\n",
        "    # return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "def response(sentence, show_details=False):\n",
        "    userID = input('Enter your ID: ')\n",
        "    results = classify(sentence)\n",
        "    # if we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        # loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intent_data['intents']:\n",
        "                # find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # set context for this intent if necessary\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print ('context:', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "\n",
        "                    # check if this intent is contextual and applies to this user's conversation\n",
        "                    if not 'context_filter' in i or \\\n",
        "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details: print ('tag:', i['tag'])\n",
        "                        # a random response from the intent\n",
        "                        return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)\n",
        "\n",
        "def chat(show_details=False):\n",
        "    userID = input('Enter your ID: ')\n",
        "    print(\"Start talking with the bot (type quit to stop)!\")\n",
        "    # responses = ''\n",
        "    while True:\n",
        "        inp = input(\"You: \")\n",
        "        # responses = responses + inp + \" \"\n",
        "        if inp.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        results = classify(inp)\n",
        "        if results:\n",
        "        # loop as long as there are matches to process\n",
        "          while results:\n",
        "              for i in intent_data['intents']:\n",
        "                  # find a tag matching the first result\n",
        "                  if i['tag'] == results[0][0]:\n",
        "                      # set context for this intent if necessary\n",
        "                      if 'context_set' in i:\n",
        "                          if show_details: print ('context:', i['context_set'])\n",
        "                          context[userID] = i['context_set']\n",
        "\n",
        "                      # check if this intent is contextual and applies to this user's conversation\n",
        "                      if not 'context_filter' in i or \\\n",
        "                          (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                          if show_details: print ('tag:', i['tag'])\n",
        "                          # a random response from the intent\n",
        "                          print(random.choice(i['responses']))\n",
        "\n",
        "              results.pop(0)\n",
        " \n",
        "#    return responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaBerDC_02A9"
      },
      "outputs": [],
      "source": [
        "chat()\n",
        "#print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxrVK7f4CkC-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}